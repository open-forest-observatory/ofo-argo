{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"OFO Argo Workflows Documentation","text":"<p>Welcome to the Open Forest Observatory (OFO) Argo Workflows documentation. This repository specifies workflows for processing drone data at scale using Argo Workflows on a Kubernetes cluster. It also contains cluster setup resources.</p>"},{"location":"#overview","title":"Overview","text":"<p>The OFO Argo system enables parallel processing of drone missions using the automate-metashape pipeline across multiple virtual machines on Jetstream2 Cloud. This scaling capability allows OFO to process many drone missions simultaneously with a single run command.</p>"},{"location":"#architecture","title":"Architecture","text":"<p>The system uses Argo Workflows running on a Kubernetes cluster, which orchestrates containers, scales processing across multiple VMs, and balances the load between worker nodes. The current setup includes:</p> <ul> <li>Controller node: Manages the Kubernetes cluster and Argo workflows</li> <li>Worker nodes: Handle compute workloads, such as processing drone missions, in parallel</li> <li>Manila shared storage: Provides working data storage to the nodes</li> <li>S3 storage: Stores the inputs/outputs of each step</li> </ul>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>User guides: Guides for accessing and managing the cluster to run workflows</li> <li>Administrator guides: Guides for setting up and configuring the cluster infrastructure</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>OFO Argo GitHub Repository</li> <li>Automate-Metashape Pipeline</li> <li>Argo Workflows Documentation</li> <li>Jetstream2 Cloud</li> </ul>"},{"location":"admin/","title":"Administrator guides","text":"<p>This section contains guides for administrators who need to set up and configure the OFO Argo cluster infrastructure.  For day-to-day cluster usage instructions, including cluster resizing and workflow submission, see the User guides section.</p>"},{"location":"admin/argo-installation-on-cluster/","title":"Argo installation on cluster","text":"<p>This guide covers the installation of Argo Workflows, including the CLI on your local machine (required for all users) and the Kubernetes extension on the cluster (one-time admin installation).</p>"},{"location":"admin/argo-installation-on-cluster/#prerequisites","title":"Prerequisites","text":"<p>This guide assumes you already have:</p> <ul> <li>A Kubernetes cluster created (see Cluster creation and resizing)</li> <li>Manila share PV and PVC configured (see Manila share mounting)</li> <li><code>kubectl</code> configured to connect to the cluster</li> </ul>"},{"location":"admin/argo-installation-on-cluster/#clone-or-update-the-ofo-argo-repository","title":"Clone or update the ofo-argo repository","text":"<p>This repository contains files needed for deploying and testing Argo.</p> <pre><code># Clone the repository (first time)\ncd ~/repos\ngit clone https://github.com/open-forest-observatory/ofo-argo\ncd ofo-argo\n</code></pre> <p>Or, if you already have the repository cloned:</p> <pre><code># Update existing repository\ncd ~/repos/ofo-argo\ngit pull\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#install-the-argo-cli-locally-one-time","title":"Install the Argo CLI locally (one-time)","text":"<p>The Argo CLI is a wrapper around <code>kubectl</code> that simplifies communication with Argo on the cluster.</p> <pre><code>ARGO_WORKFLOWS_VERSION=\"v3.7.6\"\nARGO_OS=\"linux\"\n\n# Download the binary\nwget \"https://github.com/argoproj/argo-workflows/releases/download/${ARGO_WORKFLOWS_VERSION}/argo-${ARGO_OS}-amd64.gz\"\n\n# Unzip\ngunzip \"argo-${ARGO_OS}-amd64.gz\"\n\n# Make binary executable\nchmod +x \"argo-${ARGO_OS}-amd64\"\n\n# Move binary to path\nsudo mv \"./argo-${ARGO_OS}-amd64\" /usr/local/bin/argo\n\n# Test installation\nargo version\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#install-argo-workflow-manager-and-server-on-the-cluster","title":"Install Argo workflow manager and server on the cluster","text":"<p>Create the Argo namespace and install Argo components:</p> <pre><code># Create namespace\nkubectl create namespace argo\n\n# Install Argo workflows on cluster\nkubectl apply -n argo -f https://github.com/argoproj/argo-workflows/releases/download/${ARGO_WORKFLOWS_VERSION}/install.yaml\n</code></pre> <p>Optionally, check that the pods are running:</p> <pre><code>kubectl get pods -n argo\nkubectl describe pod -n argo &lt;pod-name&gt;\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#configure-workflow-permissions","title":"Configure workflow permissions","text":"<p>The standard <code>install.yaml</code> configures permissions for the workflow controller, but does not configure permissions for the service accounts that workflow pods run as. Without these permissions, workflows will fail with an error like:</p> <p>workflowtaskresults.argoproj.io is forbidden: User \"system:serviceaccount:argo:argo\" cannot create resource \"workflowtaskresults\"</p> <p>This step grants the <code>argo</code> service account (used by all our workflows via <code>serviceAccountName: argo</code>) the minimal permissions needed to create and update workflowtaskresults.</p> <pre><code># Apply role and role binding\nkubectl apply -f setup/argo/workflow-executor-rbac.yaml\n\n# Confirm the necessary permission was granted (should return: yes)\nkubectl auth can-i create workflowtaskresults.argoproj.io -n argo --as=system:serviceaccount:argo:argo\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#configure-workflow-controller-delete-completed-pods-and-use-s3-for-artifact-and-log-storage","title":"Configure workflow controller: Delete completed pods, and use S3 for artifact and log storage","text":"<p>Argo Workflows can store workflow logs and artifacts in S3-compatible object storage. Also, it can be configured to delete pods once they are done running (whether successful or unsuccessful). Given we are storing pod logs in S3 (we don't require the pod to be in existence in order for Arto to access its logs through kubernetes), there is no reason to keep pods around after they finish running. This section configures Argo to use JS2 (Jetstream2) object storage for logs and to remove workflow pods that have completed running. All of this is in support of configuring workflows to preferentially schedule pods on nodes that have other running pods on them (to minimize the chances of being evicted from an underutilized node that is being deleted by the autoscaler).</p>"},{"location":"admin/argo-installation-on-cluster/#prerequisites_1","title":"Prerequisites","text":"<p>The S3 credentials secret must exist in the <code>argo</code> namespace. This secret should contain: - <code>access_key</code>: Your S3 access key - <code>secret_key</code>: Your S3 secret key</p> <p>To verify the secret exists:</p> <pre><code>kubectl get secret s3-credentials -n argo\n</code></pre> <p>If it doesn't exist, create it (replacing placeholders with your actual credentials):</p> <pre><code>kubectl create secret generic s3-credentials -n argo \\\n  --from-literal=access_key='YOUR_ACCESS_KEY' \\\n  --from-literal=secret_key='YOUR_SECRET_KEY' \\\n  --from-literal=endpoint='https://js2.jetstream-cloud.org:8001' \\\n  --from-literal=provider='Other'\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#apply-the-workflow-controller-configuration","title":"Apply the workflow controller configuration","text":"<p>The workflow controller configmap tells Argo where to store artifacts and logs. Apply it:</p> <pre><code>kubectl apply -f setup/argo/workflow-controller-configmap.yaml\n</code></pre> <p>This configures: - Bucket: <code>ofo-internal</code> - Path prefix: <code>argo-logs-artifacts/</code> - Log archiving: Enabled (workflow logs are stored in S3) - Key format: <code>argo-logs-artifacts/{workflow-name}/{pod-name}</code></p>"},{"location":"admin/argo-installation-on-cluster/#restart-the-workflow-controller","title":"Restart the workflow controller","text":"<p>The workflow controller needs to be restarted to pick up the new configuration:</p> <pre><code>kubectl rollout restart deployment workflow-controller -n argo\nkubectl rollout status deployment workflow-controller -n argo\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#verify-the-configuration","title":"Verify the configuration","text":"<p>Check that the configmap was applied correctly:</p> <pre><code>kubectl get configmap workflow-controller-configmap -n argo -o yaml\n</code></pre> <p>You should see the <code>artifactRepository</code> section with the S3 configuration.</p>"},{"location":"admin/argo-installation-on-cluster/#test-the-installation","title":"Test the installation","text":"<p>Run a test workflow to verify everything is working:</p> <pre><code>argo submit -n argo test-workflows/dag-diamond.yaml --watch\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#set-up-argo-server-with-https-access","title":"Set up Argo server with HTTPS access","text":"<p>The following steps configure secure external access to the Argo UI.</p>"},{"location":"admin/argo-installation-on-cluster/#verify-argo-server-is-clusterip","title":"Verify Argo server is ClusterIP","text":"<p>Ensure <code>argo-server</code> is set to ClusterIP, which means it's only accessible from within the cluster's internal network. We'll configure a secure gateway to the internet next.</p> <pre><code># Check current type\nkubectl get svc argo-server -n argo\n\n# If it's LoadBalancer, change it back to ClusterIP\nkubectl patch svc argo-server -n argo -p '{\"spec\":{\"type\":\"ClusterIP\"}}'\n\n# Verify (should show: TYPE = ClusterIP)\nkubectl get svc argo-server -n argo\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#install-cert-manager","title":"Install cert-manager","text":"<pre><code># Install cert-manager\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.yaml\n\n# Wait for cert-manager to be ready (takes 5-60 seconds)\nkubectl wait --for=condition=available --timeout=300s deployment/cert-manager -n cert-manager\nkubectl wait --for=condition=available --timeout=300s deployment/cert-manager-webhook -n cert-manager\nkubectl wait --for=condition=available --timeout=300s deployment/cert-manager-cainjector -n cert-manager\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#install-nginx-ingress-controller","title":"Install nginx ingress controller","text":"<pre><code># Install nginx ingress controller\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml\n\n# Wait for LoadBalancer IP to be assigned (may take 1-3 minutes)\nkubectl get svc -n ingress-nginx ingress-nginx-controller -w\n\n# Press Ctrl+C once you see an EXTERNAL-IP appear. Save this IP - you'll need it for DNS.\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#create-dns-a-record","title":"Create DNS A record","text":"<p>Go to your DNS provider (GoDaddy, Cloudflare, Route53, etc.) and create:</p> <ul> <li>Type: A</li> <li>Name: argo (or your preferred subdomain) (in Netlify DNS, replace the <code>@</code> with <code>argo</code>)</li> <li>Value: <code>&lt;IP from previous step&gt;</code></li> <li>TTL: 300 (or auto/default)</li> </ul>"},{"location":"admin/argo-installation-on-cluster/#create-lets-encrypt-cluster-issuer","title":"Create Let's Encrypt cluster issuer","text":"<pre><code>kubectl apply -f setup/argo/clusterissuer-letsencrypt.yaml\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#create-ingress-resource-for-argo-server","title":"Create ingress resource for Argo server","text":"<pre><code>kubectl apply -f setup/argo/ingress-argo.yaml\n</code></pre> <p>Wait 1-10 minutes for DNS records to propagate, then verify:</p> <pre><code>nslookup argo.focal-lab.org\n</code></pre> <p>This should return the ingress controller IP. A \"non-authoritative answer\" from DNS queries is OK.</p>"},{"location":"admin/argo-installation-on-cluster/#request-and-wait-for-certificate","title":"Request and wait for certificate","text":"<pre><code># Watch certificate being issued (usually 1-3 minutes)\nkubectl get certificate -n argo -w\n\n# Wait until you see READY show True:\n# NAME              READY   SECRET           AGE\n# argo-server-tls   True    argo-server-tls  2m\n\n# Press Ctrl+C when READY shows True\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#verify-argo-ui-access","title":"Verify Argo UI access","text":"<p>The Argo UI should now be accessible at https://argo.focal-lab.org</p> <p>Verification commands:</p> <pre><code># Check all components are ready\nkubectl get pods -n cert-manager\nkubectl get pods -n ingress-nginx\nkubectl get pods -n argo\n\n# Check ingress created\nkubectl get ingress -n argo\n\n# Check certificate issued\nkubectl get certificate -n argo\n\n# Check DNS resolves\nnslookup argo.focal-lab.org\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#create-argo-ui-server-token","title":"Create Argo UI server token","text":"<p>Create a token that lasts one year. This token will need to be re-created annually.</p> <p>We're creating this for the <code>default</code> service account. In the future, we may want to create a dedicated service account for argo-server tokens, or separate accounts for each user to allow individual permission management.</p> <pre><code># Create token (valid for 1 year)\nkubectl create token argo-server -n argo --duration=8760h\n</code></pre> <p>Copy the token, preface it with <code>Bearer</code>, and add/update it in Vaultwarden.</p>"},{"location":"admin/argo-installation-on-cluster/#token-rotation-future","title":"Token rotation (future)","text":"<p>To rotate the token in the future:</p> <pre><code># Delete all tokens for the service account\nkubectl delete secret -n argo -l kubernetes.io/service-account.name=argo-server\n</code></pre> <p>Then recreate it with the command above.</p>"},{"location":"admin/cluster-creation-and-resizing/","title":"Cluster creation and resizing","text":"<p>This guide is for the cluster administrator (currently Derek). Since we only need one cluster and Derek is taking care of creating it, this guide is not necessary for the whole team. There is a separate guide on cluster management that is for the whole team.</p> <p>Key resource referenced in creating this guide: Beginner's Guide to Magnum on Jetstream2</p>"},{"location":"admin/cluster-creation-and-resizing/#one-time-local-machine-software-setup","title":"One-time local machine software setup","text":"<p>These instructions will set up your local (Linux, Mac, or WSL) machine to control the cluster through the command line.</p>"},{"location":"admin/cluster-creation-and-resizing/#install-python-and-create-virtual-environment","title":"Install Python and create virtual environment","text":"<p>Make sure you have a recent Python interpreter and the venv utility, then create a virtual environment for OpenStack management:</p> <pre><code>sudo apt update\nsudo apt install -y python3-full python3-venv\npython3 -m venv ~/venv/openstack\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#install-openstack-command-line-tools","title":"Install OpenStack command line tools","text":"<pre><code># Activate environment\nsource ~/venv/openstack/bin/activate\n\n# Install OpenStack utilities\npip install -U python-openstackclient python-magnumclient python-designateclient\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#install-kubectl","title":"Install kubectl","text":"<p>Install the Kubernetes control utility <code>kubectl</code> (from the official Kubernetes documentation):</p> <pre><code># Install prerequisites\nsudo apt install -y apt-transport-https ca-certificates curl gnupg\n\n# Add Kubernetes apt repository\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list\n\n# Install kubectl\nsudo apt update\nsudo apt install -y kubectl\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#create-application-credential","title":"Create application credential","text":"<p>In Horizon, go to Identity &gt; Application Credentials. Click Create:</p> <ul> <li>Do not change the roles</li> <li>Do not set a secret (one will be generated)</li> <li>Do set an expiration date</li> <li>Do check \"unrestricted\" (required because Magnum creates additional app credentials for the cluster)</li> </ul> <p>Download the openrc file and store it in the OFO Vaultwarden organization where OFO members can access it.</p> <p>Copy the application credential onto your local computer (do not put it on a JS2 machine), ideally into <code>~/.ofocluster/app-cred-ofocluster-openrc.sh</code> (which is where we will assume it is in these docs).</p> <p>Source the application credential (which sets relevant environment variables for the OpenStack command line tools):</p> <pre><code>source ~/.ofocluster/app-cred-ofocluster-openrc.sh\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#create-openstack-keypair","title":"Create OpenStack keypair","text":"<p>Create a keypair for cluster node access. If you want, you can save the private key that is displayed when you run this command in order to SSH into the cluster nodes later. However, they won't have public IP addresses, so this is mainly to satisfy Magnum's requirements.</p> <pre><code># Create a new keypair (displays private key - save if needed)\nopenstack keypair create my-openstack-keypair-name\n</code></pre> <p>Alternatively, specify an existing public key you normally use, in this example <code>~/.ssh/id_rsa.pub</code>:</p> <pre><code># Use existing public key\nopenstack keypair create my-openstack-keypair-name --public-key ~/.ssh/id_rsa.pub\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#enable-shell-completion","title":"Enable shell completion","text":"<pre><code># Create directory for completion scripts\nmkdir -p ~/.bash_completion.d\n\n# Generate completion scripts\nopenstack complete &gt; ~/.ofocluster/openstack-completion.bash\nkubectl completion bash &gt; ~/.ofocluster/kubectl-completion.bash\n\n# Add to ~/.bashrc\necho 'source ~/.ofocluster/openstack-completion.bash' &gt;&gt; ~/.bashrc\necho 'source ~/.ofocluster/kubectl-completion.bash' &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#cluster-creation","title":"Cluster creation","text":"<p>Assuming you're in a fresh shell session, enter your JS2 venv and source the application credential:</p> <pre><code>source ~/venv/openstack/bin/activate\nsource ~/.ofocluster/app-cred-ofocluster-openrc.sh\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#view-available-cluster-templates","title":"View available cluster templates","text":"<pre><code>openstack coe cluster template list\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#deploy-the-cluster","title":"Deploy the cluster","text":"<p>Specify the deployment parameters and create the cluster. Choose the most recent Kubernetes version (highest number in the template list). The master node can be <code>m3.small</code>. We'll deploy a cluster with a single worker node that is also <code>m3.small</code>. When we need to scale up, we'll add nodegroups. This initial spec is just the base setup for when we're not running Argo workloads on it. We need to enable audo-scaling now, even though we don't want it for the default worker nodegroup, because these settings apply to child nodegroups and it appears the max node count cannot be overridden.</p> <pre><code># Set deployment parameters\nTEMPLATE=\"kubernetes-1-33-jammy\"\nKEYPAIR=my-openstack-keypair-name # what you created above\n\n# Network configuration\nNETWORK_ID=$(openstack network show --format value -c id auto_allocated_network)\nSUBNET_ID=$(openstack subnet show --format value -c id auto_allocated_subnet_v4)\n\nopenstack coe cluster create \\\n    --cluster-template $TEMPLATE \\\n    --master-count 1 --node-count 1 \\\n    --master-flavor m3.small --flavor m3.small \\\n    --merge-labels \\\n    --labels auto_scaling_enabled=true,min_node_count=1,boot_volume_size=80 \\\n    --keypair $KEYPAIR \\\n    --fixed-network \"${NETWORK_ID}\" \\\n    --fixed-subnet \"${SUBNET_ID}\" \\\n    \"ofocluster2\"\n\n\n\n### Check cluster status (optional)\n\n```bash\nopenstack coe cluster list\nopenstack coe cluster show ofocluster\nopenstack coe nodegroup list ofocluster\n</code></pre> <p>Or with formatting that makes it easier to copy the cluster UUID:</p> <pre><code>openstack coe cluster list --format value -c uuid -c name\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#set-up-kubectl-to-control-kubernetes","title":"Set up <code>kubectl</code> to control Kubernetes","text":"<p>This is required the first time you interact with Kubernetes on the cluster. <code>kubectl</code> is a tool to control Kubernetes (the cluster's software, not its compute nodes/VMs) from your local command line.</p> <p>Once the <code>openstack coe cluster list</code> status (command above) changes to <code>CREATE_COMPLETE</code>, get the Kubernetes configuration file (<code>kubeconfig</code>) and configure your environment:</p> <pre><code># Get cluster configuration\nopenstack coe cluster config \"ofocluster2\" --force\n\n# Set permissions and move to appropriate location\nchmod 600 config\nmkdir -p ~/.ofocluster\nmv -i config ~/.ofocluster/ofocluster.kubeconfig\n\n# Set KUBECONFIG environment variable\nexport KUBECONFIG=~/.ofocluster/ofocluster.kubeconfig\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#create-the-argo-namespace","title":"Create the Argo namespace","text":"<p>We will install various resources into this namespace in this guide and subsequent ones.</p> <pre><code>kubectl create namespace argo\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#create-kubernetes-secrets","title":"Create Kubernetes secrets","text":"<p>The Argo workflows require two Kubernetes secrets to be created:</p>"},{"location":"admin/cluster-creation-and-resizing/#s3-credentials-secret","title":"S3 credentials secret","text":"<p>The Argo workflows upload to and download from Jetstream2's S3-compatible buckets. You need to create a secret to store the S3 Access ID, Secret Key, provider type, and endpoint URL. Obtain the access key ID and secret access key from the OFO Vaultwarden organization. The credentials were originally created by Derek following JS2 docs and particularly <code>openstack ec2 credentials create</code>.</p> <pre><code>kubectl create secret generic s3-credentials \\\n  --from-literal=provider='Other' \\\n  --from-literal=endpoint='https://js2.jetstream-cloud.org:8001' \\\n  --from-literal=access_key='&lt;YOUR_ACCESS_KEY_ID&gt;' \\\n  --from-literal=secret_key='&lt;YOUR_SECRET_ACCESS_KEY&gt;' \\\n  -n argo\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#agisoft-metashape-license-secret","title":"Agisoft Metashape license secret","text":"<p>The photogrammetry workflow requires access to an Agisoft Metashape floating license server. Create a secret to store the license server address. Obtain the license server IP address from the OFO Vaultwarden organization.</p> <pre><code>kubectl create secret generic agisoft-license \\\n  --from-literal=license_server='&lt;LICENSE_SERVER_IP&gt;:5842' \\\n  -n argo\n</code></pre> <p>Replace <code>&lt;LICENSE_SERVER_IP&gt;</code> with the actual IP address from the credentials document.</p> <p>These secrets only need to be created once per cluster.</p>"},{"location":"admin/cluster-creation-and-resizing/#configure-cpu-node-labeling","title":"Configure CPU node labeling","text":"<p>To prevent CPU workloads from being scheduled on expensive GPU nodes, CPU nodes are labeled based on their nodegroup naming pattern. CPU-only workflow templates use <code>nodeSelector</code> to explicitly target these labeled nodes, while GPU pods use resource requests that naturally constrain them to GPU nodes.</p>"},{"location":"admin/cluster-creation-and-resizing/#apply-cpu-node-labels","title":"Apply CPU node labels","text":"<p>Label CPU nodes automatically based on their nodegroup naming pattern:</p> <pre><code>kubectl apply -f setup/k8s/cpu-nodegroup-labels.yaml\n</code></pre> <p>This creates a NodeFeatureRule that automatically labels any node with <code>cpu</code> in its name with <code>feature.node.kubernetes.io/workload-type: cpu</code>. The label is applied by Node Feature Discovery (NFD) when the node joins the cluster.</p> <p>Nodegroup naming requirement</p> <p>When creating CPU nodegroups, ensure the nodegroup name contains <code>cpu</code> (e.g., <code>cpu-group</code>, <code>cpu-m3xl</code>) so nodes are automatically labeled. See nodegroup creation for details.</p>"},{"location":"admin/cluster-creation-and-resizing/#verify-cpu-node-labels","title":"Verify CPU node labels","text":"<pre><code>kubectl get nodes -L feature.node.kubernetes.io/workload-type\n</code></pre> <p>All nodes with <code>cpu</code> in their name should show <code>feature.node.kubernetes.io/workload-type=cpu</code>.</p>"},{"location":"admin/cluster-creation-and-resizing/#how-it-works","title":"How it works","text":"<ul> <li>CPU pods: Use <code>nodeSelector: feature.node.kubernetes.io/workload-type: cpu</code> to explicitly target CPU nodes</li> <li>GPU pods: Request GPU resources (e.g., <code>nvidia.com/mig-1g.5gb</code>), which naturally constrains them to nodes advertising those resources</li> <li>System pods: DaemonSets run on all nodes as needed</li> </ul>"},{"location":"admin/cluster-creation-and-resizing/#enable-mixed-mig-strategy","title":"Enable mixed MIG strategy","text":"<p>The GPU Operator defaults to \"single\" MIG strategy, which exposes MIG slices as generic <code>nvidia.com/gpu</code> resources. For MIG nodegroups to expose specific (and mixed) resources like <code>nvidia.com/mig-2g.10gb</code> (optionally along with other MIG nodes or full nodes like <code>nvidia.com/gpu</code>), enable \"mixed\" strategy:</p> <pre><code># Add NVIDIA helm repo (if not already added)\nhelm repo add nvidia https://helm.ngc.nvidia.com/nvidia\nhelm repo update nvidia\n\n# Check current GPU Operator version\nhelm list -n gpu-operator\n\n# Enable mixed MIG strategy (use the same version as currently installed)\nhelm upgrade nvidia-gpu-operator nvidia/gpu-operator \\\n  -n gpu-operator \\\n  --version &lt;CURRENT_VERSION&gt; \\\n  --reuse-values \\\n  --set mig.strategy=mixed\n</code></pre> <p>To check whether it took (or in the future to make sure it remained set), run the following and look for <code>strategy: mixed</code>.</p> <pre><code>helm get values nvidia-gpu-operator -n gpu-operator | grep -A1 \"mig:\"\n</code></pre> <p>Cluster upgrades</p> <p>This setting may be reset if the cluster template is upgraded and Magnum redeploys the GPU Operator. Re-run this command after cluster upgrades if MIG resources stop appearing.</p>"},{"location":"admin/cluster-creation-and-resizing/#configure-mig-multi-instance-gpu","title":"Configure MIG (Multi-Instance GPU)","text":"<p>MIG partitions A100 GPUs into isolated slices, allowing multiple pods to share one physical GPU with hardware-level isolation. This is optional - standard GPU nodegroups work without MIG.</p>"},{"location":"admin/cluster-creation-and-resizing/#mig-profiles","title":"MIG profiles","text":"Nodegroup pattern MIG profile Pods/GPU VRAM each GPU compute each <code>mig1-*</code> <code>all-1g.5gb</code> 7 10GB 1/7 <code>mig2-*</code> <code>all-2g.10gb</code> 3 10GB 2/7 <code>mig3-*</code> <code>all-3g.20gb</code> 2 20GB 3/7"},{"location":"admin/cluster-creation-and-resizing/#apply-mig-configuration-rule","title":"Apply MIG configuration rule","text":"<pre><code>kubectl apply -f setup/k8s/mig-nodegroup-labels.yaml\n</code></pre> <p>This creates a NodeFeatureRule that automatically labels GPU nodes based on their nodegroup name. The NVIDIA MIG manager watches for these labels and configures the GPU accordingly.</p>"},{"location":"admin/cluster-creation-and-resizing/#verify-mig-is-working","title":"Verify MIG is working","text":"<p>After creating a MIG nodegroup (see MIG nodegroups):</p> <pre><code># Check node MIG config label\nkubectl get nodes -l nvidia.com/mig.config -o custom-columns='NAME:.metadata.name,MIG_CONFIG:.metadata.labels.nvidia\\.com/mig\\.config'\n\n# Check MIG resources are available\nkubectl get nodes -o custom-columns='NAME:.metadata.name,MIG-1G:.status.allocatable.nvidia\\.com/mig-1g\\.10gb,MIG-2G:.status.allocatable.nvidia\\.com/mig-2g\\.10gb,MIG-3G:.status.allocatable.nvidia\\.com/mig-3g\\.20gb'\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#how-it-works_1","title":"How it works","text":"<ol> <li>User creates nodegroup with MIG naming (e.g., <code>mig1-group</code>)</li> <li>Node joins cluster with name containing <code>-mig1-</code></li> <li>NFD applies label <code>nvidia.com/mig.config=all-1g.5gb</code></li> <li>MIG manager detects label, configures GPU into 3 partitions</li> <li>Device plugin exposes <code>nvidia.com/mig-1g.5gb: 3</code> as allocatable resources</li> <li>Pods requesting <code>nvidia.com/mig-1g.5gb: 1</code> get one partition</li> </ol>"},{"location":"admin/cluster-creation-and-resizing/#kubernetes-management","title":"Kubernetes management","text":"<p>If you are resuming cluster management after a reboot, you will need to re-set environment variables and source the application credential:</p> <pre><code>source ~/venv/openstack/bin/activate\nexport KUBECONFIG=~/.ofocluster/ofocluster.kubeconfig\nsource ~/.ofocluster/app-cred-ofocluster-openrc.sh\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#view-cluster-nodes","title":"View cluster nodes","text":"<pre><code>kubectl get nodes\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#access-shell-on-cluster-nodes","title":"Access shell on cluster nodes","text":"<p>Run commands on a node with:</p> <pre><code># Start a debug session on a specific node\nkubectl debug node/&lt;node-name&gt; -it --image=ubuntu\n\n# Once inside, you have host access via /host\n# Check kernel modules\nchroot /host modprobe ceph\nchroot /host lsmod | grep ceph\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#check-disk-usage","title":"Check disk usage","text":"<p>Run a one-off command to check disk usage:</p> <pre><code>kubectl debug node/&lt;node-name&gt; -it --image=busybox -- df -h\n</code></pre> <p>Look for the <code>/dev/vda1</code> volume. Then delete the debugging pods:</p> <pre><code>kubectl get pods -o name | grep node-debugger | xargs kubectl delete\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#rotating-secrets","title":"Rotating secrets","text":"<p>If you accidentally expose a secret, or for periodic rotation, delete, then re-create. Example for S3 (assuming your S3 is via JS2 Swift):</p> <p>List creds to get the ID of the cred you want to swap out: <code>openstack ec2 credentials list</code> Delete it: <code>openstack ec2 credentials delete &lt;your-access-key-id&gt;</code> Create a new one: <code>openstack ec2 credentials create</code> Update it in Vaultwarden. Delete the k8s secret: <code>kubectl delete secret -n argo s3-credentials</code> Re-create k8s secret following the instructions above. If you have already installed Argo on the cluster, restart the workflow controller so it picks up the new creds: <code>kubectl rollout restart deployment workflow-controller -n argo</code></p>"},{"location":"admin/cluster-creation-and-resizing/#cluster-resizing","title":"Cluster resizing","text":"<p>These instructions are for managing which nodes are in the cluster, not what software is running on them.</p>"},{"location":"admin/cluster-creation-and-resizing/#resize-the-default-worker-group","title":"Resize the default worker group","text":"<p>Resize the cluster by adding or removing nodes from the original worker group (not a later-added nodegroup). We will likely not do this, relying instead on nodegroups for specific runs.</p> <pre><code>openstack coe cluster resize \"ofocluster\" 4\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#add-resize-or-delete-nodegroups","title":"Add, resize, or delete nodegroups","text":"<p>For nodegroup management, see the corresponding user guide.</p>"},{"location":"admin/cluster-creation-and-resizing/#delete-the-cluster","title":"Delete the cluster","text":"<pre><code>openstack coe cluster delete \"ofocluster\"\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#monitoring-dashboards","title":"Monitoring dashboards","text":"<p>Incomplete notes in development.</p>"},{"location":"admin/cluster-creation-and-resizing/#grafana-dashboard","title":"Grafana dashboard","text":"<pre><code># Port-forward Grafana to your local machine\nkubectl port-forward -n monitoring-system svc/kube-prometheus-stack-grafana 3000:80\n</code></pre> <p>Then open http://localhost:3000 in your browser.</p>"},{"location":"admin/cluster-creation-and-resizing/#kubernetes-dashboard","title":"Kubernetes dashboard","text":"<pre><code># Create service account\nkubectl create serviceaccount dashboard-admin -n kubernetes-dashboard\n\n# Create cluster role binding\nkubectl create clusterrolebinding dashboard-admin \\\n    --clusterrole=cluster-admin \\\n    --serviceaccount=kubernetes-dashboard:dashboard-admin\n\n# Create token (24 hour duration)\nkubectl create token dashboard-admin -n kubernetes-dashboard --duration=24h\n\n# Port-forward (if not already running)\nkubectl port-forward -n kubernetes-dashboard svc/kubernetes-dashboard 8443:443\n</code></pre> <p>Then open https://localhost:8443 in your browser and use the token to log in.</p>"},{"location":"admin/cluster-creation-and-resizing/#notes-from-testing-and-experimentation-attempting-to-set-up-autoscaling-and-fixed-nodegroups","title":"Notes from testing and experimentation attempting to set up autoscaling and fixed nodegroups","text":"<p>It seems impossible to set new (or override existing) labels when adding nodegroups. Labels only seem to be intended/used for overall cluster creation. Also if we deploy one nodegroup that violates the requirement for min_node_count to be specified, cannot deploy any others (they all fail), even if they would have succeeded otherwise.</p> <p>By not specifying a label <code>max_node_count</code> upon cluster creation, the default-worker nodegroup will not autoscale. But still we need to set the label <code>auto_scaling_enabled</code> to <code>true</code> upon cluster creation because cluster labels apparently cannot be overridden by nodegroups. This means that all nodegroups will autoscale, and we are required to specify <code>--min-nodes</code>, or nodegroup clreation will fail. If you don't specify <code>--max-nodes</code> when creating a nodegroup, it treats the <code>--node-count</code> as the max and may scale down to the min.</p> <p>I tried creating a cluster with no scaling (max nodes 1 and auto_scaling_enabled=false) and then overriding it at the nodegroup level with values that should enable scaling, but it didn't scale (apparently these values get overridden). Also tried not specifying auto_scale_enabled label at all, but then specifying it for nodegroups, but these nodegroups did not scale. Learned that <code>--node-count</code> needs to be within the range of the min and max (if omitted, it is assumed to be 1).</p>"},{"location":"admin/cluster-creation-and-resizing/#testingmonitoring-autoscaling-behavior","title":"Testing/monitoring autoscaling behavior","text":"<p>Deploy a bunch of pods that will need to get scheduled: <pre><code>kubectl create deployment scale-test --image=nginx --replicas=20 -- sleep infinity &amp;&amp; kubectl set resources deployment scale-test --requests=cpu=500m,memory=512Mi\n</code></pre></p> <p>Make sure some become pending (which should trigger a scale up): <pre><code>kubectl get pods\n</code></pre></p> <p>Monitor the cluster autoscaler status to see if it is planning any scaling up or down: <pre><code>kubectl get configmap cluster-autoscaler-status -n kube-system -o yaml\n</code></pre></p> <p>When finished, delete the test deployment: <pre><code>kubectl delete deployment scale-test\n</code></pre></p>"},{"location":"admin/manila-share-mounting/","title":"Manila share mounting","text":"<p>This guide covers mounting a Manila CephFS share to your Kubernetes cluster for persistent storage.</p>"},{"location":"admin/manila-share-mounting/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Kubernetes cluster created with Magnum (see Cluster creation and resizing)</li> <li><code>kubectl</code> configured to connect to the cluster</li> <li>OpenStack application credentials</li> <li>The Manila share already created in OpenStack</li> </ul>"},{"location":"admin/manila-share-mounting/#one-time-local-machine-software-setup","title":"One-time local machine software setup","text":"<p>Ensure your local system has the necessary tools. First, activate the OpenStack virtual environment created in the cluster creation guide.</p> <p>Additionally, we need OpenStack app credentials to look up the parameters of our Manila share based on its name, which in turn requires an additional OpenStack command-line tool for interacting with Manila. An alternative is to look these up from some other existing source (e.g. Horizon UI) and provide them manually, in which case the OpenStack system tools are not necessary.</p> <pre><code>source ~/venv/openstack/bin/activate\nsource ~/.ofocluster/app-cred-ofocluster-openrc.sh\n\n# Install Manila client and jq\npip install -U python-manilaclient\nsudo apt install -y jq\n\n# Install Helm\nsudo snap install helm --classic\n</code></pre>"},{"location":"admin/manila-share-mounting/#install-the-ceph-csi-driver-on-the-cluster","title":"Install the Ceph CSI driver on the cluster","text":"<p>If you are resuming cluster management after a reboot, you will need to re-set environment variables and source the application credential:</p> <pre><code>source ~/venv/openstack/bin/activate\nexport KUBECONFIG=~/.ofocluster/ofocluster.kubeconfig\nsource ~/.ofocluster/app-cred-ofocluster-openrc.sh\n</code></pre> <p>The Ceph CSI (Container Storage Interface) driver enables Kubernetes to mount CephFS shares.</p> <pre><code># Add Helm repository\nhelm repo add ceph-csi https://ceph.github.io/csi-charts\nhelm repo update\n\n# Create namespace for Ceph CSI\nkubectl create namespace ceph-csi-cephfs\n\n# Install Ceph CSI driver\n# - provisioner.replicaCount=2: Reduced from default of 3 so autoscaler doesn't keep nodegroups scaled up with 2 nodes\nhelm install --namespace \"ceph-csi-cephfs\" \"ceph-csi-cephfs\" ceph-csi/ceph-csi-cephfs \\\n  --set provisioner.replicaCount=2\n\n# Check installation status\nhelm status --namespace \"ceph-csi-cephfs\" \"ceph-csi-cephfs\"\n</code></pre>"},{"location":"admin/manila-share-mounting/#look-up-manila-share-parameters","title":"Look up Manila share parameters","text":"<p>Based on the share name and access rule name, query OpenStack to look up the necessary identifiers. The Kubernetes Manila config requires these values.</p> <pre><code># Set your Manila share and access rule names\nMANILA_SHARE_NAME=ofo-share-02\nexport MANILA_ACCESS_RULE_NAME=ofo-share-02-rw\n\n# Extract Manila monitors (json-formatted list)\nexport MANILA_MONITORS_JSON=$(openstack share export location list \"$MANILA_SHARE_NAME\" -f json | jq -r '.[0].Path | split(\":/\")[0] | split(\",\") | map(\"\\\"\" + . + \"\\\"\") | join(\",\")')\n\n# Extract the root path to the Manila share\nexport MANILA_ROOT_PATH=$(openstack share export location list $MANILA_SHARE_NAME -f json | jq -r '.[0].Path' | awk -F':/' '{print \"/\"$2}')\n\n# Get the access rule ID\nACCESS_RULE_ID=$(openstack share access list \"$MANILA_SHARE_NAME\" -f json | jq -r \".[] | select(.\\\"Access To\\\" == \\\"$MANILA_ACCESS_RULE_NAME\\\") | .ID\")\n\n# Extract the secret key for the access rule\nexport MANILA_ACCESS_KEY=$(openstack share access list \"$MANILA_SHARE_NAME\" -f json | jq -r \".[] | select(.\\\"Access To\\\" == \\\"$MANILA_ACCESS_RULE_NAME\\\") | .\\\"Access Key\\\"\")\n\n# Confirm we extracted the expected attributes\necho $MANILA_MONITORS_JSON\necho $MANILA_ROOT_PATH\necho $MANILA_ACCESS_RULE_NAME\necho $MANILA_ACCESS_KEY\n</code></pre> <p>As an alternative, you can look these parameters up in Horizon.</p>"},{"location":"admin/manila-share-mounting/#clone-or-update-the-ofo-argo-repository","title":"Clone or update the ofo-argo repository","text":"<p>The repository contains Kubernetes configuration templates for Manila share mounting.</p> <pre><code># Clone the repository (first time)\ncd ~/repos\ngit clone https://github.com/open-forest-observatory/ofo-argo\ncd ofo-argo\n</code></pre> <p>Or, if you already have the repository cloned:</p> <pre><code># Update existing repository\ncd ~/repos/ofo-argo\ngit pull\n</code></pre>"},{"location":"admin/manila-share-mounting/#apply-the-share-configuration-to-the-cluster","title":"Apply the share configuration to the cluster","text":"<p>There is a template Kubernetes config file that contains variables such as <code>${MANILA_ACCESS_RULE_NAME}</code>. These variables will be substituted with the environment variables we prepared in the previous step. The following command substitutes the environment variables into the config file and applies it to the cluster. It's done in one step so we don't save this file (which contains secrets) to disk.</p> <p>Note that the namespaces of the various resources are defined within the yaml, so <code>-n</code> does not have to be used here. If namespaces ever need to change, update the config yaml.</p> <pre><code># Substitute variables and apply configuration\nenvsubst &lt; setup/k8s/manila-cephfs-csi-config2.yaml | kubectl apply -f -\n\n# Verify resources were created\nkubectl describe secret -n ceph-csi-cephfs manila-share-secret\nkubectl describe persistentvolume ceph-share-rw-pv\nkubectl describe -n argo persistentvolumeclaim ceph-share-rw-pvc\n</code></pre>"},{"location":"admin/manila-share-mounting/#test-the-pvc-mount","title":"Test the PVC mount","text":"<p>Deploy a test pod to verify that the PVC mount works correctly.</p> <p>Note: During development, we sometimes encountered the error <code>MountVolume.MountDevice failed for volume \"ceph-share-rw-pv\" : rpc error: code = Internal desc = rpc error: code = Internal desc = failed to fetch monitor list using clusterID (12345): missing configuration for cluster ID \"12345\"</code>. If this occurs, try deleting all deployed resources except the configmap and re-applying them. It may also be resolved by simply deleting and re-applying the test pod.</p> <pre><code># Deploy test pod\nkubectl apply -f setup/k8s/manila-test-pod2.yaml\n\n# Check pod status\nkubectl get pod -n argo manila-test-pod2\nkubectl describe pod -n argo manila-test-pod2\n\n# Once running, exec into the pod\nkubectl exec -n argo -it manila-test-pod2 -- /bin/sh\n\n# Inside the pod, check the mount\nls -la /mnt/cephfs\ndf -h /mnt/cephfs\n\n# Test write access\necho \"test\" &gt; /mnt/cephfs/test-file.txt\ncat /mnt/cephfs/test-file.txt\n\n# Exit the pod\nexit\n</code></pre>"},{"location":"admin/manila-share-mounting/#clean-up-test-resources","title":"Clean up test resources","text":"<p>After verifying the mount works, delete the test pod:</p> <pre><code>kubectl delete -n argo pod manila-test-pod2\n</code></pre>"},{"location":"admin/manila-share-mounting/#delete-all-resources-if-needed","title":"Delete all resources (if needed)","text":"<p>If you need to completely remove the Manila share mounting configuration:</p> <pre><code>kubectl delete -n argo pod manila-test-pod2\nkubectl delete -n argo pvc ceph-share-rw-pvc2\nkubectl delete pv ceph-share-rw-pv2\nkubectl delete -n ceph-csi-cephfs secret manila-share-secret2\nkubectl delete configmap -n ceph-csi-cephfs ceph-csi-config\n</code></pre>"},{"location":"plans/s3-imagery-download-feature/","title":"Implementation Plan: S3 Imagery Zip Download Feature","text":""},{"location":"plans/s3-imagery-download-feature/#context","title":"Context","text":"<p>The OFO step-based photogrammetry workflow currently requires imagery to already exist on the shared PVC at <code>/data/</code>. Users must manually upload imagery before running workflows, which is time-consuming and requires separate coordination.</p> <p>The workflow processes multiple projects in parallel using Argo's <code>withParam</code> mechanism. Each project has its own configuration file that specifies, among other things, the <code>photo_path</code> where imagery is located.</p>"},{"location":"plans/s3-imagery-download-feature/#problem-statement","title":"Problem Statement","text":"<p>Users need the ability to have the workflow automatically download imagery from S3 (via rclone) at runtime, rather than requiring imagery to be pre-staged on the PVC. This is especially useful for: - Running workflows on imagery stored in cloud object storage - Avoiding manual pre-upload steps - Processing imagery that doesn't need to persist after the workflow</p>"},{"location":"plans/s3-imagery-download-feature/#goals","title":"Goals","text":"<ol> <li>Allow users to specify S3 zip file(s) to download in the <code>argo:</code> section of their config</li> <li>Automatically download and unzip imagery before photogrammetry begins</li> <li>Provide a simple path prefix (<code>__DOWNLOADED__</code>) for users to reference downloaded imagery</li> <li>Isolate downloads per-project to prevent collisions in parallel execution</li> <li>Clean up downloaded imagery after workflow completion (configurable)</li> <li>Fail gracefully: individual project failures should not stop other projects</li> </ol>"},{"location":"plans/s3-imagery-download-feature/#non-goals-future-possibilities","title":"Non-Goals (Future Possibilities)","text":"<ul> <li>Sharing downloaded imagery between projects (would save bandwidth but adds complexity with timing and storage management)</li> <li>Downloading non-zip formats (tarballs, raw folders)</li> <li>Incremental/resumable downloads</li> <li>Per-project folder reorganization (see Future Enhancements section)</li> </ul>"},{"location":"plans/s3-imagery-download-feature/#user-experience","title":"User Experience","text":""},{"location":"plans/s3-imagery-download-feature/#config-file-example","title":"Config File Example","text":"<pre><code>argo:\n  # New: List of S3 zip files to download (can also be a single string)\n  # Format: bucket/path/file.zip (no remote prefix needed)\n  # S3 credentials come from the cluster's s3-credentials Kubernetes secret\n  s3_imagery_zip_download:\n    - ofo-public/drone/missions_01/000558/images/000558_images.zip\n    - ofo-public/drone/missions_01/000559/images/000559_images.zip\n\n  # New: Whether to delete downloaded imagery after workflow (default: true)\n  cleanup_downloaded_imagery: true\n\n  # Existing settings...\n  match_photos:\n    gpu_enabled: true\n\nproject:\n  project_name: my_forest_plot\n  # Use __DOWNLOADED__ prefix to reference downloaded imagery\n  photo_path:\n    - __DOWNLOADED__/000558_images/000558-01\n    - __DOWNLOADED__/000558_images/000558-02\n    - __DOWNLOADED__/000559_images/000559-01\n</code></pre>"},{"location":"plans/s3-imagery-download-feature/#expected-behavior","title":"Expected Behavior","text":"<ol> <li>Workflow downloads <code>000558_images.zip</code> and <code>000559_images.zip</code></li> <li>Each zip is extracted to a folder named after the zip (sans <code>.zip</code> extension)</li> <li>The <code>__DOWNLOADED__</code> prefix is replaced with the actual download path</li> <li>Photogrammetry runs with the resolved paths</li> <li>After photogrammetry completes, downloaded imagery is deleted (if cleanup enabled)</li> </ol>"},{"location":"plans/s3-imagery-download-feature/#architecture-overview","title":"Architecture Overview","text":""},{"location":"plans/s3-imagery-download-feature/#download-location","title":"Download Location","text":"<pre><code>{TEMP_WORKING_DIR}/downloaded_imagery/{iteration_id}/\n\u251c\u2500\u2500 000558_images/\n\u2502   \u251c\u2500\u2500 000558-01/\n\u2502   \u2502   \u2514\u2500\u2500 *.jpg\n\u2502   \u2514\u2500\u2500 000558-02/\n\u2502       \u2514\u2500\u2500 *.jpg\n\u2514\u2500\u2500 000559_images/\n    \u2514\u2500\u2500 000559-01/\n        \u2514\u2500\u2500 *.jpg\n</code></pre> <ul> <li><code>TEMP_WORKING_DIR</code> = <code>/ofo-share/argo-working/{workflow-name}</code> (already exists)</li> <li><code>iteration_id</code> = <code>{index}_{project_name_sanitized}</code> with 3-digit zero-padded index</li> <li>Examples: <code>000_mission_001</code>, <code>001_mission_001</code>, <code>002_other_project</code></li> <li>Includes project name for easy identification during manual intervention</li> <li>Index ensures uniqueness even with duplicate project names in the same config list</li> </ul>"},{"location":"plans/s3-imagery-download-feature/#workflow-dag-changes","title":"Workflow DAG Changes","text":"<p>Current flow: <pre><code>setup-photogrammetry \u2192 match-photos \u2192 align-cameras \u2192 ...\n</code></pre></p> <p>New flow: <pre><code>download-imagery (conditional) \u2192 setup-photogrammetry \u2192 ... \u2192 cleanup-imagery (conditional)\n</code></pre></p>"},{"location":"plans/s3-imagery-download-feature/#implementation-steps","title":"Implementation Steps","text":""},{"location":"plans/s3-imagery-download-feature/#step-1-update-config-parsing-in-determine_datasetspy","title":"Step 1: Update Config Parsing in <code>determine_datasets.py</code>","text":"<p>File: <code>docker-workflow-utils/determine_datasets.py</code></p> <p>Changes needed:</p> <ol> <li> <p>Add iteration ID generation: When building the list of mission parameters, generate a unique <code>iteration_id</code> combining the zero-padded index (3 digits) and the sanitized project name, separated by underscore.</p> </li> <li> <p>Extract new argo attributes: Parse <code>s3_imagery_zip_download</code> and <code>cleanup_downloaded_imagery</code> from the <code>argo:</code> section.</p> </li> <li> <p>Normalize to list: Handle both single string and list inputs for <code>s3_imagery_zip_download</code>.</p> </li> <li> <p>Add new output parameters: Include in the JSON output for each mission:</p> </li> <li><code>iteration_id</code>: Format <code>{index:03d}_{project_name_sanitized}</code> (e.g., <code>000_mission_001</code>)</li> <li><code>imagery_zip_downloads</code>: JSON array of S3 URLs (empty array if not specified)</li> <li><code>imagery_download_enabled</code>: Boolean flag for conditional step execution</li> <li><code>cleanup_downloaded_imagery</code>: Boolean (default <code>true</code>)</li> </ol> <p>Example of parameter extraction logic:</p> <pre><code># In process_config_file() function, after loading argo_config:\n# Note: index is the enumeration index passed to this function\n\n# Generate unique iteration ID: 3-digit zero-padded index + underscore + sanitized project name\n# Example: \"000_mission_001\", \"001_mission_001\" (for duplicates), \"002_other_project\"\niteration_id = f\"{index:03d}_{project_name_sanitized}\"\n\n# Extract imagery download settings\nimagery_downloads = argo_config.get('s3_imagery_zip_download', [])\nif isinstance(imagery_downloads, str):\n    imagery_downloads = [imagery_downloads] if imagery_downloads.strip() else []\n\ncleanup_imagery = argo_config.get('cleanup_downloaded_imagery', True)\n</code></pre> <p>Add to mission_params dict: <pre><code>mission_params['iteration_id'] = iteration_id\nmission_params['imagery_zip_downloads'] = json.dumps(imagery_downloads)\nmission_params['imagery_download_enabled'] = str(len(imagery_downloads) &gt; 0).lower()\nmission_params['cleanup_downloaded_imagery'] = str(cleanup_imagery).lower()\n</code></pre></p> <p>Add comment for future enhancement: <pre><code># FUTURE: Could implement download sharing between projects to save bandwidth.\n# Would require: (1) download coordination/locking, (2) reference counting for cleanup,\n# (3) handling projects that start days apart. Current approach downloads per-project\n# to avoid these complexities and prevent storage issues from long-running workflows.\n</code></pre></p>"},{"location":"plans/s3-imagery-download-feature/#step-2-create-download-script","title":"Step 2: Create Download Script","text":"<p>File: <code>docker-workflow-utils/download_imagery.py</code> (new file)</p> <p>Purpose: Download and extract zip files from S3, preparing imagery for photogrammetry.</p> <p>Inputs (via environment variables or arguments): - <code>IMAGERY_ZIP_URLS</code>: JSON array of S3 paths (format: <code>bucket/path/file.zip</code>, no remote prefix) - <code>DOWNLOAD_BASE_DIR</code>: Base directory for downloads (e.g., <code>{TEMP_WORKING_DIR}/downloaded_imagery</code>) - <code>ITERATION_ID</code>: Unique ID for this project iteration - S3 credentials: <code>S3_PROVIDER</code>, <code>S3_ENDPOINT</code>, <code>S3_ACCESS_KEY</code>, <code>S3_SECRET_KEY</code></p> <p>Logic:</p> <ol> <li>Parse the JSON array of URLs</li> <li>Create project-specific download directory: <code>{DOWNLOAD_BASE_DIR}/{ITERATION_ID}/</code></li> <li>For each URL:    a. Extract filename from URL (e.g., <code>000558_images.zip</code>)    b. Download using rclone: <code>rclone copyto {url} {download_dir}/{filename}</code>    c. Determine extraction folder name (filename sans <code>.zip</code>)    d. Create extraction directory    e. Unzip: <code>unzip {download_dir}/{filename} -d {download_dir}/{folder_name}</code>    f. Delete zip file to save space</li> <li>Output the download path for use by subsequent steps</li> </ol> <p>Error handling: - If any download or extraction fails, exit with non-zero status - Log clear error messages identifying which URL failed - Do not attempt partial cleanup on failure (let workflow handle)</p> <p>Example rclone command construction:</p> <p>Use the existing <code>get_s3_flags()</code> pattern from <code>docker-photogrammetry-postprocessing/entrypoint.py</code> for consistency:</p> <pre><code>def get_s3_flags():\n    \"\"\"Build common S3 authentication flags for rclone commands.\"\"\"\n    return [\n        \"--s3-provider\", os.environ.get(\"S3_PROVIDER\"),\n        \"--s3-endpoint\", os.environ.get(\"S3_ENDPOINT\"),\n        \"--s3-access-key-id\", os.environ.get(\"S3_ACCESS_KEY\"),\n        \"--s3-secret-access-key\", os.environ.get(\"S3_SECRET_KEY\"),\n    ]\n\n# Download command using rclone's on-the-fly backend syntax (:s3:)\n# This avoids needing a pre-configured remote - credentials come from flags\n# User specifies just \"bucket/path/file.zip\", we prepend \":s3:\"\nrclone_url = f\":s3:{s3_path}\"\nrclone_cmd = [\n    \"rclone\", \"copyto\",\n    rclone_url,\n    f\"{download_dir}/{filename}\",\n    \"--progress\",\n    \"--transfers\", \"8\",\n    \"--checkers\", \"8\",\n    \"--retries\", \"5\",\n    \"--retries-sleep\", \"15s\",\n    \"--stats\", \"30s\",\n] + get_s3_flags()\n</code></pre>"},{"location":"plans/s3-imagery-download-feature/#step-3-create-config-transform-script","title":"Step 3: Create Config Transform Script","text":"<p>File: <code>docker-workflow-utils/transform_config.py</code> (new file)</p> <p>Purpose: Replace <code>__DOWNLOADED__</code> prefix in config's <code>photo_path</code> with actual download path.</p> <p>Inputs: - <code>CONFIG_FILE</code>: Path to original config file - <code>OUTPUT_CONFIG_FILE</code>: Path to write transformed config - <code>DOWNLOADED_IMAGERY_PATH</code>: Actual path to downloaded imagery</p> <p>Logic:</p> <ol> <li>Load YAML config file</li> <li>Get <code>photo_path</code> from <code>project</code> section</li> <li>If <code>photo_path</code> is a string, convert to single-item list for uniform handling</li> <li>Replace <code>__DOWNLOADED__</code> prefix with actual path in each photo_path entry</li> <li>Write modified config to output path</li> <li>Preserve all other config settings unchanged</li> </ol> <p>Example transformation: <pre><code># Input photo_path: __DOWNLOADED__/000558_images/000558-01\n# DOWNLOADED_IMAGERY_PATH: /ofo-share/argo-working/wf-abc123/downloaded_imagery/000_my_project\n# Output photo_path: /ofo-share/argo-working/wf-abc123/downloaded_imagery/000_my_project/000558_images/000558-01\n</code></pre></p> <p>Validation (exit non-zero on failure): - FAIL if <code>__DOWNLOADED__</code> prefix is used in <code>photo_path</code> but no downloads were specified in config - FAIL if downloads were specified but no <code>__DOWNLOADED__</code> paths found in <code>photo_path</code> - These are configuration errors that should be caught early rather than causing cryptic failures later</p>"},{"location":"plans/s3-imagery-download-feature/#step-4-create-cleanup-script","title":"Step 4: Create Cleanup Script","text":"<p>File: <code>docker-workflow-utils/cleanup_imagery.py</code> (new file)</p> <p>Purpose: Delete downloaded imagery after photogrammetry completes.</p> <p>Inputs: - <code>DOWNLOAD_DIR</code>: Directory to delete (e.g., <code>{TEMP_WORKING_DIR}/downloaded_imagery/{ITERATION_ID}</code>)</p> <p>Logic:</p> <ol> <li>Verify directory exists and is within expected base path (safety check)</li> <li>Remove directory recursively</li> <li>Log success/failure</li> </ol> <p>Safety checks (exit non-zero on failure): - FAIL if path doesn't contain expected components (<code>downloaded_imagery</code>, <code>argo-working</code>) - FAIL if path appears to be outside the expected temp working directory structure - This prevents accidental deletion of important data if paths are misconfigured - A failed safety check indicates a bug or misconfiguration that needs investigation</p>"},{"location":"plans/s3-imagery-download-feature/#step-5-update-workflow-yaml","title":"Step 5: Update Workflow YAML","text":"<p>File: <code>photogrammetry-workflow-stepbased.yaml</code></p> <p>Changes needed:</p>"},{"location":"plans/s3-imagery-download-feature/#51-add-new-parameters-to-workflow","title":"5.1: Add New Parameters to Workflow","text":"<p>In the <code>process-project-workflow</code> template's <code>inputs.parameters</code> section, add:</p> <pre><code>- name: iteration-id\n- name: imagery-zip-downloads\n  default: \"[]\"\n- name: imagery-download-enabled\n  default: \"false\"\n- name: cleanup-downloaded-imagery\n  default: \"true\"\n</code></pre>"},{"location":"plans/s3-imagery-download-feature/#52-add-download-step-template","title":"5.2: Add Download Step Template","text":"<p>Create new template <code>download-imagery</code>:</p> <pre><code>- name: download-imagery\n  inputs:\n    parameters:\n      - name: imagery-zip-downloads\n      - name: iteration-id\n  container:\n    image: ghcr.io/open-forest-observatory/docker-workflow-utils:latest\n    command: [\"python3\", \"/app/download_imagery.py\"]\n    env:\n      - name: IMAGERY_ZIP_URLS\n        value: \"{{inputs.parameters.imagery-zip-downloads}}\"\n      - name: DOWNLOAD_BASE_DIR\n        value: \"{{workflow.parameters.TEMP_WORKING_DIR}}/downloaded_imagery\"\n      - name: ITERATION_ID\n        value: \"{{inputs.parameters.iteration-id}}\"\n      # S3 credentials from existing secret references\n</code></pre>"},{"location":"plans/s3-imagery-download-feature/#53-add-config-transform-step-template","title":"5.3: Add Config Transform Step Template","text":"<p>Create new template <code>transform-config</code>:</p> <pre><code>- name: transform-config\n  inputs:\n    parameters:\n      - name: config-file\n      - name: iteration-id\n      - name: project-name\n  container:\n    image: ghcr.io/open-forest-observatory/docker-workflow-utils:latest\n    command: [\"python3\", \"/app/transform_config.py\"]\n    env:\n      - name: CONFIG_FILE\n        value: \"{{inputs.parameters.config-file}}\"\n      - name: OUTPUT_CONFIG_FILE\n        value: \"{{workflow.parameters.TEMP_WORKING_DIR}}/configs/{{inputs.parameters.project-name}}-transformed.yml\"\n      - name: DOWNLOADED_IMAGERY_PATH\n        value: \"{{workflow.parameters.TEMP_WORKING_DIR}}/downloaded_imagery/{{inputs.parameters.iteration-id}}\"\n  outputs:\n    parameters:\n      - name: transformed-config-path\n        value: \"{{workflow.parameters.TEMP_WORKING_DIR}}/configs/{{inputs.parameters.project-name}}-transformed.yml\"\n</code></pre>"},{"location":"plans/s3-imagery-download-feature/#54-add-cleanup-step-template","title":"5.4: Add Cleanup Step Template","text":"<p>Create new template <code>cleanup-imagery</code>:</p> <pre><code>- name: cleanup-imagery\n  inputs:\n    parameters:\n      - name: iteration-id\n  container:\n    image: ghcr.io/open-forest-observatory/docker-workflow-utils:latest\n    command: [\"python3\", \"/app/cleanup_imagery.py\"]\n    env:\n      - name: DOWNLOAD_DIR\n        value: \"{{workflow.parameters.TEMP_WORKING_DIR}}/downloaded_imagery/{{inputs.parameters.iteration-id}}\"\n</code></pre>"},{"location":"plans/s3-imagery-download-feature/#55-update-dag-structure","title":"5.5: Update DAG Structure","text":"<p>Modify the <code>process-project-workflow</code> DAG to include new steps:</p> <p>Add download step (conditional): <pre><code>- name: download-imagery\n  template: download-imagery\n  when: \"{{inputs.parameters.imagery-download-enabled}} == 'true'\"\n  arguments:\n    parameters:\n      - name: imagery-zip-downloads\n        value: \"{{inputs.parameters.imagery-zip-downloads}}\"\n      - name: iteration-id\n        value: \"{{inputs.parameters.iteration-id}}\"\n</code></pre></p> <p>Add transform step (conditional, depends on download): <pre><code>- name: transform-config\n  template: transform-config\n  when: \"{{inputs.parameters.imagery-download-enabled}} == 'true'\"\n  depends: \"download-imagery\"\n  arguments:\n    parameters:\n      - name: config-file\n        value: \"{{inputs.parameters.config-file}}\"\n      - name: iteration-id\n        value: \"{{inputs.parameters.iteration-id}}\"\n      - name: project-name\n        value: \"{{inputs.parameters.project-name}}\"\n</code></pre></p> <p>Update setup-photogrammetry dependency: - If downloads enabled: depends on <code>transform-config</code> - Use transformed config path instead of original</p> <p>Add cleanup step at end (conditional): <pre><code>- name: cleanup-imagery\n  template: cleanup-imagery\n  when: \"{{inputs.parameters.imagery-download-enabled}} == 'true' &amp;&amp; {{inputs.parameters.cleanup-downloaded-imagery}} == 'true'\"\n  depends: \"upload-outputs\"  # After all photogrammetry and upload complete\n  arguments:\n    parameters:\n      - name: iteration-id\n        value: \"{{inputs.parameters.iteration-id}}\"\n</code></pre></p>"},{"location":"plans/s3-imagery-download-feature/#56-handle-config-path-switching","title":"5.6: Handle Config Path Switching","text":"<p>The tricky part: subsequent steps need to use either the original config or transformed config.</p> <p>Use Argo's conditional expression syntax to select the appropriate config path:</p> <pre><code>- name: setup-photogrammetry\n  arguments:\n    parameters:\n      - name: config-file\n        # Conditional config path selection:\n        # - If imagery download is enabled, use the transformed config (with __DOWNLOADED__ paths resolved)\n        # - Otherwise, use the original config file path unchanged\n        # This allows the same workflow DAG structure to handle both download and non-download cases\n        value: \"{{=inputs.parameters['imagery-download-enabled'] == 'true' ? steps['transform-config'].outputs.parameters['transformed-config-path'] : inputs.parameters['config-file']}}\"\n</code></pre> <p>This conditional expression uses Argo's expression syntax (<code>{{= ... }}</code>) to evaluate at runtime which config path to pass to downstream steps.</p> <p>Alternative considered (rejected): Always run the transform step and transform the config even if no downloads are specified, outputting to a consistent location. This was rejected because it adds unnecessary processing overhead for the common case where no downloads are needed, and the conditional approach is cleaner.</p>"},{"location":"plans/s3-imagery-download-feature/#step-6-update-docker-image-build","title":"Step 6: Update Docker Image Build","text":"<p>File: <code>docker-workflow-utils/Dockerfile</code></p> <p>Changes: - Ensure <code>rclone</code> is installed (follow pattern from <code>docker-photogrammetry-postprocessing/Dockerfile</code>) - Ensure <code>unzip</code> package is installed - Include new Python scripts: <code>download_imagery.py</code>, <code>transform_config.py</code>, <code>cleanup_imagery.py</code> - Ensure PyYAML is available for config parsing</p>"},{"location":"plans/s3-imagery-download-feature/#step-7-update-parameter-passing-in-main-template","title":"Step 7: Update Parameter Passing in Main Template","text":"<p>File: <code>photogrammetry-workflow-stepbased.yaml</code></p> <p>In the <code>main</code> template where <code>withParam</code> iterates, add the new parameters to the arguments:</p> <pre><code>- name: process-projects\n  template: process-project-workflow\n  arguments:\n    parameters:\n      # Existing parameters...\n      - name: iteration-id\n        value: \"{{item.iteration_id}}\"\n      - name: imagery-zip-downloads\n        value: \"{{item.imagery_zip_downloads}}\"\n      - name: imagery-download-enabled\n        value: \"{{item.imagery_download_enabled}}\"\n      - name: cleanup-downloaded-imagery\n        value: \"{{item.cleanup_downloaded_imagery}}\"\n</code></pre>"},{"location":"plans/s3-imagery-download-feature/#step-8-update-documentation","title":"Step 8: Update Documentation","text":"<p>File: <code>docs/usage/stepbased-workflow.md</code></p> <p>Add new section documenting:</p> <ol> <li>When to use S3 imagery download - Use cases and benefits</li> <li>Configuration options - Document <code>s3_imagery_zip_download</code> and <code>cleanup_downloaded_imagery</code></li> <li>Path syntax - Explain <code>__DOWNLOADED__</code> prefix usage</li> <li>Zip file structure requirements - Explain that zip filename becomes folder name</li> <li>Example configurations - Show complete config examples</li> <li>Troubleshooting - Common issues (wrong paths, download failures)</li> </ol>"},{"location":"plans/s3-imagery-download-feature/#testing-plan","title":"Testing Plan","text":""},{"location":"plans/s3-imagery-download-feature/#unit-tests","title":"Unit Tests","text":"<ol> <li>Test <code>determine_datasets.py</code> correctly parses new attributes</li> <li>Test normalization of single string to list</li> <li>Test default values when attributes not present</li> <li>Test iteration_id format is correct (<code>{index:03d}_{project_name_sanitized}</code>)</li> </ol>"},{"location":"plans/s3-imagery-download-feature/#integration-tests","title":"Integration Tests","text":"<ol> <li>Happy path: Config with downloads specified, verify:</li> <li>Download step runs</li> <li>Files are extracted correctly</li> <li>Config is transformed</li> <li>Photogrammetry receives correct paths</li> <li> <p>Cleanup removes files</p> </li> <li> <p>No downloads: Config without <code>s3_imagery_zip_download</code>, verify:</p> </li> <li>Download step is skipped</li> <li> <p>Config is passed through unchanged</p> </li> <li> <p>Cleanup disabled: Set <code>cleanup_downloaded_imagery: false</code>, verify files persist</p> </li> <li> <p>Download failure: Use invalid S3 path, verify:</p> </li> <li>That specific project fails</li> <li>Other projects continue</li> <li> <p>Workflow reports partial failure</p> </li> <li> <p>Parallel projects: Two projects downloading same-named zips simultaneously, verify:</p> </li> <li>No collisions due to iteration_id isolation</li> <li> <p>Both complete successfully</p> </li> <li> <p>Validation failures: Verify workflow fails early when:</p> </li> <li><code>__DOWNLOADED__</code> prefix used but no downloads specified</li> <li>Downloads specified but no <code>__DOWNLOADED__</code> paths in config</li> </ol>"},{"location":"plans/s3-imagery-download-feature/#rollback-plan","title":"Rollback Plan","text":"<p>If issues arise after deployment:</p> <ol> <li>Users can simply not use <code>s3_imagery_zip_download</code> attribute (feature is additive)</li> <li>Revert workflow YAML to previous version</li> <li>Revert docker-workflow-utils image to previous tag</li> </ol>"},{"location":"plans/s3-imagery-download-feature/#future-enhancements","title":"Future Enhancements","text":"<p>Document these as comments in relevant code files:</p> <ol> <li> <p>Download sharing between projects - Would save bandwidth when multiple projects need the same imagery. Would require: download coordination/locking, reference counting for cleanup, handling projects that start days apart. Current approach downloads per-project to avoid these complexities and prevent storage issues from long-running workflows.</p> </li> <li> <p>Per-project folder reorganization - Currently, workflow intermediates are organized as:    <pre><code>{TEMP_WORKING_DIR}/\n\u251c\u2500\u2500 photogrammetry/{project-name}/\n\u251c\u2500\u2500 postprocessing/\n\u2514\u2500\u2500 downloaded_imagery/{iteration_id}/\n</code></pre>    A future enhancement could reorganize to per-project folders:    <pre><code>{TEMP_WORKING_DIR}/\n\u2514\u2500\u2500 {iteration_id}/\n    \u251c\u2500\u2500 photogrammetry/\n    \u251c\u2500\u2500 postprocessing/\n    \u2514\u2500\u2500 downloaded_imagery/\n</code></pre>    This would make per-project cleanup trivial (delete one folder) and simplify debugging failed iterations. However, this is a larger refactor affecting existing workflow code and should be implemented separately.</p> </li> <li> <p>Support for tarballs - Different extraction command (<code>tar -xzf</code>)</p> </li> <li> <p>Direct folder download - <code>rclone copy</code> instead of <code>copyto</code> + unzip for uncompressed imagery</p> </li> <li> <p>Parallel zip downloads within a project - Currently downloads are sequential within a project</p> </li> <li> <p>Download progress reporting - Surface download progress to Argo UI</p> </li> </ol>"},{"location":"usage/","title":"User guides","text":"<p>This section contains guides for users who need to access and manage the OFO Argo cluster for running workflows. For initial cluster setup, configuration, and maintenance, see the Admin guides section.</p>"},{"location":"usage/argo-usage/","title":"Using Argo on the OFO cluster","text":"<p>This guide describes how to submit workflows to Argo and monitor them. This is a generic guide for any Argo workflow.</p> <p>For specific instructions on running the photogrammetry (automate-metashape) workflow, see the Photogrammetry workflow guide.</p>"},{"location":"usage/argo-usage/#prerequisites","title":"Prerequisites","text":"<p>This guide assumes you already:</p> <ul> <li>Have access to the <code>ofocluster</code> kubernetes cluster</li> <li>Have resized the cluster to the apprpriate size for your workflow</li> <li>Have <code>kubectl</code> configured to connect to the cluster</li> </ul> <p>For guidance on all of this setup, see Cluster access and resizing.</p>"},{"location":"usage/argo-usage/#install-the-argo-cli-locally-one-time","title":"Install the Argo CLI locally (one-time)","text":"<p>The Argo CLI is a wrapper around <code>kubectl</code> that simplifies communication with Argo on the cluster. You should install it on your local machine since that is where you will have set up and authenticated <code>kubectl</code> following the Cluster access guide.</p> <pre><code># Specify the CLI version to install\nARGO_WORKFLOWS_VERSION=\"v3.7.2\"\nARGO_OS=\"linux\"\n\n# Download the binary\nwget \"https://github.com/argoproj/argo-workflows/releases/download/${ARGO_WORKFLOWS_VERSION}/argo-${ARGO_OS}-amd64.gz\"\n\n# Unzip\ngunzip \"argo-${ARGO_OS}-amd64.gz\"\n\n# Make binary executable\nchmod +x \"argo-${ARGO_OS}-amd64\"\n\n# Move binary to path\nsudo mv \"./argo-${ARGO_OS}-amd64\" /usr/local/bin/argo\n\n# Test installation\nargo version\n</code></pre>"},{"location":"usage/argo-usage/#authenticate-with-the-cluster","title":"Authenticate with the cluster","text":"<p>Once after every reboot, you will need to re-set your <code>KUBECONFIG</code> environment variable so that <code>argo</code> (and the underlying <code>kubectl</code>) CLI tools can authenticate with the cluster:</p> <pre><code>source ~/venv/openstack/bin/activate\nexport KUBECONFIG=~/.ofocluster/ofocluster.kubeconfig\n</code></pre>"},{"location":"usage/argo-usage/#submit-a-workflow","title":"Submit a workflow","text":"<p>Simply run <code>argo submit -n argo /path/to/your/workflow.yaml</code>, optionally adding parameters  by appending text in the format <code>-p PARAMETER_NAME=parameter_value</code>. The <code>parameter_value</code> can be  an environment variable. For example:</p> <pre><code>argo submit -n argo photogrammetry-workflow.yaml \\\n-p PARAMETER_NAME=parameter_value \\\n</code></pre>"},{"location":"usage/argo-usage/#observe-and-manage-workflows","title":"Observe and manage workflows","text":""},{"location":"usage/argo-usage/#argo-web-ui","title":"Argo web UI","text":"<p>Access the Argo UI at argo.focal-lab.org. When prompted to log in, supply the client authentication token. You can find the token string in Vaultwarden under the record \"Argo UI token\".</p>"},{"location":"usage/argo-usage/#command-line","title":"Command line","text":"<p>Argo provides a CLI to inspect and monitor workflows. But sometimes pure Kubernetes works well too. For example, here is a snippet to show all nodes and which Argo pods are running on each.</p> <pre><code># Get all nodes first\nkubectl get nodes -o jsonpath='{.items[*].metadata.name}' | tr ' ' '\\n' | grep -v '^$' &gt; /tmp/nodes.txt\n\n# Get all non-DaemonSet pods in argo namespace\nkubectl get pods -n argo -o custom-columns=NODE:.spec.nodeName,NAMESPACE:.metadata.namespace,NAME:.metadata.name,STATUS:.status.phase,OWNER:.metadata.ownerReferences[0].kind --no-headers 2&gt;/dev/null | \\\n  grep -v DaemonSet &gt; /tmp/pods.txt\n\n# Display results\nwhile read node; do\n  echo \"\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\"\n  echo \"NODE: $node\"\n  echo \"\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\"\n\n  # Get pods for this node\n  grep \"^$node \" /tmp/pods.txt 2&gt;/dev/null | awk '{printf \"  %-40s %-20s %s\\n\", $3, $2, $4}'\n\n  # Check if node has no pods\n  if ! grep -q \"^$node \" /tmp/pods.txt 2&gt;/dev/null; then\n    echo \"  (no argo pods)\"\n  fi\n\n  echo \"\"\ndone &lt; /tmp/nodes.txt\n\n# Cleanup\nrm /tmp/nodes.txt /tmp/pods.txt\n</code></pre>"},{"location":"usage/argo-usage/#autoscaler-considerations","title":"Autoscaler considerations","text":"<p>Because our cluster is set up to autoscale (add/remove nodes to match demand), there is the possibility that workflow pods that are running on underutilized nodes will get evicted (killed) and rescheduled on a different node, in the autoscaler's effort to free up and remove the underutilized node. To minimize the probability of this, we have implemented several measures:</p> <ol> <li> <p>Pod packing via affinity rules: Workflow pods prefer to schedule on nodes that already have    other running workflow pods. This keeps pods consolidated on fewer nodes, reducing the chance    that a node becomes \"underutilized\" while still running a workflow pod. This is configured via    <code>podAffinity</code> rules that prefer nodes with pods labeled <code>workflows.argoproj.io/completed: \"false\"</code>.</p> </li> <li> <p>Automatic pod cleanup: Completed pods are automatically deleted via <code>podGC: OnPodCompletion</code>.    This prevents finished pods from lingering and confusing the scheduler's affinity decisions.</p> </li> <li> <p>S3 log archiving: Workflow logs are archived to S3, so we don't need completed pods to stick    around for log access.</p> </li> <li> <p>Eviction protection: All workflow pods are annotated with    <code>cluster-autoscaler.kubernetes.io/safe-to-evict: \"false\"</code>. This tells the cluster autoscaler to    never evict running workflow pods, even if the node appears underutilized. Once pods complete,    they are deleted by podGC, which allows the node to scale down normally.</p> </li> </ol> <p>These defaults are configured in the workflow controller configmap (see Argo installation).</p>"},{"location":"usage/argo-usage/#gpu-node-scheduling","title":"GPU node scheduling","text":"<p>To prevent expensive GPU resources from being consumed by CPU-only workloads, the cluster uses explicit scheduling rules:</p> <p>How it works:</p> <ul> <li>CPU nodes are labeled with <code>workload-type: cpu</code> based on their nodegroup naming pattern (any node with <code>cpu</code> in the name)</li> <li>CPU pods use <code>nodeSelector: workload-type: cpu</code> to explicitly target CPU nodes</li> <li>GPU pods request GPU resources (e.g., <code>nvidia.com/gpu</code> or MIG resources), which naturally constrains them to nodes advertising those resources</li> <li>All pods still inherit <code>podAffinity</code> from the workflow controller configmap to prefer scheduling on nodes with other running pods (to support autoscaling in removing empty nodes)</li> </ul> <p>This approach ensures CPU pods cannot schedule on GPU nodes, even during the brief period when new GPU nodes join the cluster before NFD labels them.</p> <p>For admin setup of CPU node labeling, see CPU node labeling.</p>"},{"location":"usage/argo-usage/#mig-multi-instance-gpu","title":"MIG (Multi-Instance GPU)","text":"<p>For workloads with low GPU utilization, MIG nodegroups partition each A100 into multiple isolated slices. To use MIG, create a MIG nodegroup (see MIG nodegroups).</p> <p>For custom workflows, update the workflow GPU template to request MIG resources:</p> <pre><code>resources:\n  requests:\n    nvidia.com/mig-2g.10gb: 1  # Instead of nvidia.com/gpu: 1\n    cpu: \"10\"\n    memory: \"38Gi\"\n</code></pre> <p>For the photogrammetry workflow, configure MIG resources (along with CPU/memory) in your config file's <code>argo</code> section instead of editing the workflow YAML. See Step-based workflow resource configuration for details.</p> <p>Available MIG resource types:</p> <ul> <li><code>nvidia.com/mig-1g.5gb</code> - 1/7 compute, 5GB VRAM (7 per GPU)</li> <li><code>nvidia.com/mig-2g.10gb</code> - 2/7 compute, 10GB VRAM (3 per GPU)</li> <li><code>nvidia.com/mig-3g.20gb</code> - 3/7 compute, 20GB VRAM (2 per GPU)</li> </ul> <p>GPU resource requests work for both MIG and non-MIG GPU nodes.</p>"},{"location":"usage/cluster-access-and-resizing/","title":"Cluster access and resizing","text":"<p>This guide assumes a cluster has already been created, any necessary persistent volumes (e.g. Manila share) have already been configured, and Argo has already been installed. It describes how to access and resize the cluster as a user. For details on initial cluster deployment and setup, including installation of Argo and configuration of persistent volumes, see the Admin guides.</p> <p>The OFO cluster is named <code>ofocluster</code>. The nodes that comprise it can be seen in Exosphere starting with the string <code>ofocluster-</code>. They appear as created by <code>dyoung@access-ci.org</code>. The nodes should not be modified via Exosphere or Horizon, only through the command line tools described below.</p>"},{"location":"usage/cluster-access-and-resizing/#ofo-cluster-management-principles","title":"OFO cluster management principles","text":"<p>When the cluster is not in use, we will downsize it to its minimum size: one m3.small control node and one m3.small worker node. Just before running a compute load (e.g. Argo data processing run) on the cluster, we will manually add one or more 'nodegroups', which contain a specified number of nodes of a specified flavor. We can add CPU and/or GPU nodegroups. Soon after the workflow run is complete, we will manually delete the nodegroup(s) so that the nodes do not consume our compute credits.</p>"},{"location":"usage/cluster-access-and-resizing/#one-time-local-machine-software-setup","title":"One-time local machine software setup","text":"<p>These instructions will set up your local (Linux, Mac, or WSL) machine to control the cluster through the command line.</p>"},{"location":"usage/cluster-access-and-resizing/#install-python-and-create-virtual-environment","title":"Install Python and create virtual environment","text":"<p>Make sure you have a recent Python interpreter and the venv utility, then create a Python virtual environment for OpenStack management. OpenStack is the platform Jetstream2 uses to allow users to create and manage cloud resources.</p> <pre><code>sudo apt update\nsudo apt install -y python3-full python3-venv\npython3 -m venv ~/venv/openstack\nsource ~/venv/openstack/bin/activate\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#install-openstack-command-line-tools","title":"Install OpenStack command line tools","text":"<pre><code>pip install -U python-openstackclient python-magnumclient python-designateclient\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#install-kubectl","title":"Install kubectl","text":"<p>Install the Kubernetes control utility <code>kubectl</code> (from the official Kubernetes documentation):</p> <pre><code># Install prerequisites\nsudo apt install -y apt-transport-https ca-certificates curl gnupg\n\n# Add Kubernetes apt repository\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list\n\n# Install kubectl\nsudo apt update\nsudo apt install -y kubectl\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#download-openstack-application-credential","title":"Download OpenStack application credential","text":"<p>This is a credential (and associated secret key) that allows you to authenticate with OpenStack in order to manage cloud resources in our project. It will be rotated occasionally, so if yours doesn't appear to work, check whether you have the latest version. In the OFO Vaultwarden, find the entry <code>OpenStack application credential</code>. Download the attached file <code>app-cred-ofocluster-openrc.sh</code> onto your local computer (do not put it on a JS2 machine), ideally into <code>~/.ofocluster/app-cred-ofocluster-openrc.sh</code> (which is where we will assume it is in these docs).</p> <p>Source the application credential (which sets relevant environment variables to authenticate the OpenStack command line tools):</p> <pre><code>source ~/.ofocluster/app-cred-ofocluster-openrc.sh\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#enable-shell-completion","title":"Enable shell completion","text":"<p>This will allow you to use <code>tab</code> to autocomplete OpenStack and Kubectl commands in your shell.</p> <pre><code># Create directory for completion scripts\nmkdir -p ~/.bash_completion.d\n\n# Generate completion scripts\nopenstack complete &gt; ~/.ofocluster/openstack-completion.bash\nkubectl completion bash &gt; ~/.ofocluster/kubectl-completion.bash\n\n# Add to ~/.bashrc\necho 'source ~/.ofocluster/openstack-completion.bash' &gt;&gt; ~/.bashrc\necho 'source ~/.ofocluster/kubectl-completion.bash' &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#set-up-kubectl-to-control-kubernetes","title":"Set up <code>kubectl</code> to control Kubernetes","text":"<p>This is required the first time you interact with Kubernetes on the cluster. <code>kubectl</code> is a tool to control Kubernetes (the cluster's software, not its compute nodes/VMs) from your local command line.</p> <p>Get the Kubernetes configuration file (<code>kubeconfig</code>) and configure your environment:</p> <pre><code># Get cluster configuration\nopenstack coe cluster config \"ofocluster2\" --force\n\n# Set permissions and move to appropriate location\nchmod 600 config\nmkdir -p ~/.ofocluster\nmv -i config ~/.ofocluster/ofocluster.kubeconfig\n\n# Set KUBECONFIG environment variable\nexport KUBECONFIG=~/.ofocluster/ofocluster.kubeconfig\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#cluster-resizing","title":"Cluster resizing","text":"<p>These instructions are for managing which nodes are in the cluster, not what software is running on them.</p> <p>If you are resuming cluster management after a reboot, you will need to re-set environment variables and source the application credential:</p> <pre><code>source ~/venv/openstack/bin/activate\nexport KUBECONFIG=~/.ofocluster/ofocluster.kubeconfig\nsource ~/.ofocluster/app-cred-ofocluster-openrc.sh\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#add-a-new-nodegroup","title":"Add a new nodegroup","text":"<p>Use OpenStack to create new nodegroups.</p>"},{"location":"usage/cluster-access-and-resizing/#cpu-nodegroups","title":"CPU nodegroups","text":"<p>CPU nodegroups must include <code>cpu</code> in the nodegroup name for automatic labeling to work:</p> <pre><code>openstack coe nodegroup create ofocluster2 cpu-group --min-nodes 1 --max-nodes 8 --flavor m3.large\n</code></pre> <p>The <code>cpu</code> substring triggers automatic labeling (<code>workload-type: cpu</code>) via NodeFeatureRule, which is part of ensuring CPU-only workflow pods schedule on these nodes (the other part is the NodeSelector attribute specified for the CPU task template in the Argo workflow YAML).</p>"},{"location":"usage/cluster-access-and-resizing/#gpu-nodegroups","title":"GPU nodegroups","text":"<p>GPU nodegroups consist of full-GPU nodes, and on JS2 must be an <code>xl</code> flavor. For automate-metashape runs via Argo, we now prefer MIG (split GPU) nodegroups (see below). Standard full-GPU nodegroups can use any name, as GPU resource requests handle scheduling:</p> <pre><code>openstack coe nodegroup create ofocluster2 gpu-group --min-nodes 1 --max-nodes 8 --flavor g3.xl\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#mig-nodegroups","title":"MIG nodegroups","text":"<p>MIG (Multi-Instance GPU) partitions full (e.g. JS2 g3.xl A100) GPUs into smaller isolated slices, allowing multiple pods per GPU. Use MIG when your GPU workloads have low utilization (&lt;50%) and don't need full GPU memory. This is the preferred way to provide GPU resources to automate-metashape runs via the OFO Argo step-based workflow, as it provides much greater compute efficiency.</p> <p>Create MIG nodegroups by including <code>mig1-</code>, <code>mig2-</code>, or <code>mig3-</code> in the nodegroup name. Our benchmarking has shown that automate-metashape is most efficient with <code>mig1</code> nodes 7 slices (and you can provide more than one slice to a step).</p> <pre><code>openstack coe nodegroup create ofocluster2 mig1-group --min-nodes 1 --max-nodes 4 --flavor g3.xl\n</code></pre> <p>Resource limits for MIG</p> <p>When using MIG, reduce CPU/memory requests to fit multiple pods per node:</p> Profile MIG resource request Max pods/node CPU each RAM each mig1 (7 slices) <code>nvidia.com/mig-1g.5gb</code> 7 4 16GB mig2 (3 slices) <code>nvidia.com/mig-2g.10gb</code> 3 10 38GB mig3 (2 slices) <code>nvidia.com/mig-3g.20gb</code> 2 15 55GB <p>Workflow compatibility</p> <p>MIG nodegroups are only used by workflows that request MIG resources (e.g., <code>nvidia.com/mig-1g.5gb: 1</code>) instead of full GPUs (<code>nvidia.com/gpu: 1</code>). See MIG workflow configuration.</p> <p>GPU Node Scheduling</p> <p>CPU-only workflow pods use <code>nodeSelector</code> to target CPU nodes explicitly, preventing them from scheduling on expensive GPU nodes. GPU pods request GPU resources, which naturally constrains them to GPU nodes. See GPU node scheduling for details.</p>"},{"location":"usage/cluster-access-and-resizing/#autoscaling","title":"Autoscaling","text":"<p>Due to OpenStack limitations, all nodegroups in the cluster are autoscaling. The cluster adds/removes nodes to schedule all pending pods while keeping nodes near full utilization. Set <code>--min-nodes</code> and <code>--max-nodes</code> to the same value for a fixed-size nodegroup. (But note, if you delete nodes manually or via <code>openstack coe nodegroup delete</code>, the autoscaler will try to replace the deleted nodes with new ones until the nodegroup is fully deleted -- see below for workarounds.) The <code>--node-count</code> parameter is essentially irrelevant and defaults to 1 if omitted.</p> <p>By default, the autoscaler may delete undrerutilized nodes with running pods, in an effort to consolidate pods and minimize running nodes. While often acceptable in many k8s applications, this is unacceptable for automate-metashape because pods may take hours and cannot resume from where they were killed. Therefore, we have configured the Argo workflow controller to label pods as not evictable so this doesn't happen. In addition, we have configured it to prefer scheduling new pods on nodes that already have running pods (the opposite of default Kubernetes behavior) to increase the chances of empty nodes that can be scaled down.</p>"},{"location":"usage/cluster-access-and-resizing/#check-if-the-autoscaler-is-planning-any-upsizing-or-downsizing","title":"Check if the autoscaler is planning any upsizing or downsizing","text":"<pre><code>kubectl get configmap cluster-autoscaler-status -n kube-system -o yaml\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#update-a-nodegroups-minmax-node-count-bounds","title":"Update a nodegroup's min/max node count bounds","text":"<p>The autoscaler should take care of resizing to meet demand (within the node count bounds you specify). Downscaling has a delay of at least 10 min from triggering before being implemented in an effort to prevent cycling. You can change the min and max bounds on the number of nodes the autoscaler will request:</p> <pre><code>openstack coe nodegroup update ofocluster2 cpu-group replace min_node_count=1 max_node_count=1\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#draincordon-nodes-before-downsizing-or-deleting","title":"Drain/cordon nodes before downsizing or deleting","text":"<p>WORK IN PROGRESS</p> <p>It appears that draining will actually kill running pods. Still need to find a way to simply prevent scheduling of new pods (possibly \"cordoning\"), and confirm that nodes are empty, before deleting.</p> <p>When decreasing the number of nodes in a nodegroup, it is best practice to drain the Kubernetes pods from them first. Given that we don't know which nodes OpenStack will delete when reducing the size, we have to drain the whole nodegroup. This is also what you'd do when deleting a nodegroup entirely.</p> <pre><code>NODEGROUP_NAME=cpu-group\nkubectl get nodes -l capi.stackhpc.com/node-group=$NODEGROUP_NAME -o name | xargs -I {} kubectl drain {} --ignore-daemonsets --delete-emptydir-data\n</code></pre> <p>To drain only a specific number of nodes (prioritizing the least utilitzed), sort by utilization and add <code>head</code> to limit the selection:</p> <p><pre><code>NODEGROUP_NAME=cpu-group\nNUM_TO_DRAIN=2\nkubectl get nodes -l capi.stackhpc.com/node-group=$NODEGROUP_NAME \\\n  --sort-by='.status.allocatable.cpu' -o name | \\\n  head -n $NUM_TO_DRAIN | \\\n  xargs -I {} kubectl drain {} --ignore-daemonsets --delete-emptydir-data\n</code></pre> It will print the IDs of the nodes, which you can then explicitly delete (see below)</p>"},{"location":"usage/cluster-access-and-resizing/#delete-a-nodegroup","title":"Delete a nodegroup","text":"<p>If you attempt to delete a nodegroup with many nodes and lots of pending compute jobs, the delete command can compete with the autoscaler, which will try to add nodes to restore the target node count. To prevent this, first reduce the max size of the nodegroup to 1 (the lowest allowed value), then delete.</p> <pre><code>openstack coe nodegroup update ofocluster2 $NODEGROUP_NAME min_node_count=1 max_node_count=1\nopenstack coe nodegroup delete ofocluster2 $NODEGROUP_NAME\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#cluster-inspection-commands","title":"Cluster inspection commands","text":"<p>If you are resuming cluster management after a reboot, you will need to re-set environment variables and source the application credential:</p> <pre><code>source ~/venv/openstack/bin/activate\nexport KUBECONFIG=~/.ofocluster/ofocluster.kubeconfig\nsource ~/.ofocluster/app-cred-ofocluster-openrc.sh\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#check-cluster-vm-status","title":"Check cluster VM status","text":"<p>This looks at the status of the nodes (e.g. size and quantity) from the perspective of OpenStack. It does not examine the state of the software on the nodes (e.g. Kubernetes, Argo).</p> <pre><code>openstack coe cluster list\nopenstack coe cluster show ofocluster\nopenstack coe nodegroup list ofocluster\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#view-kubernetes-nodes","title":"View Kubernetes nodes","text":"<p>Display Kubernetes node names, including which 'nodegroup' each belongs to.</p> <pre><code>kubectl get nodes\n</code></pre> <p>Display the utilization of CPU and memory on the nodes.</p> <pre><code>kubectl top nodes\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#access-the-shell-on-cluster-nodes","title":"Access the shell on cluster nodes","text":"<p>This can be useful for debugging. Run commands on a node with:</p> <pre><code>kubectl debug node/&lt;node-name&gt; -it --image=ubuntu\n</code></pre> <p>Once inside, you have access to the host VM's filesystem via /host. You could use this, for example, to check kernel modules:</p> <pre><code>chroot /host modprobe ceph\nchroot /host lsmod | grep ceph\n</code></pre> <p>Or to check disk usage:</p> <pre><code>kubectl debug node/&lt;node-name&gt; -it --image=busybox -- df -h\n</code></pre> <p>... then look for the <code>/dev/vda1</code> volume.</p> <p>When done, delete the debugging pods:</p> <pre><code>kubectl get pods -o name | grep node-debugger | xargs kubectl delete\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#kubernetes-monitoring-dashboards","title":"Kubernetes monitoring dashboards","text":"<p>Incomplete notes in development.</p>"},{"location":"usage/cluster-access-and-resizing/#grafana-dashboard","title":"Grafana dashboard","text":"<pre><code># Port-forward Grafana to your local machine\nkubectl port-forward -n monitoring-system svc/kube-prometheus-stack-grafana 3000:80\n</code></pre> <p>Then open http://localhost:3000 in your browser.</p>"},{"location":"usage/cluster-access-and-resizing/#kubernetes-dashboard","title":"Kubernetes dashboard","text":"<pre><code># Create service account\nkubectl create serviceaccount dashboard-admin -n kubernetes-dashboard\n\n# Create cluster role binding\nkubectl create clusterrolebinding dashboard-admin \\\n    --clusterrole=cluster-admin \\\n    --serviceaccount=kubernetes-dashboard:dashboard-admin\n\n# Create token (24 hour duration)\nkubectl create token dashboard-admin -n kubernetes-dashboard --duration=24h\n\n# Port-forward (if not already running)\nkubectl port-forward -n kubernetes-dashboard svc/kubernetes-dashboard 8443:443\n</code></pre> <p>Then open https://localhost:8443 in your browser and use the token to log in.</p>"},{"location":"usage/photogrammetry-workflow/","title":"Running the photogrammetry workflow (original monolithic version)","text":"<p>Step-Based Workflow Recommended</p> <p>A new step-based workflow with optimized resource allocation is now available and recommended for all new processing.</p> <p>Benefits of step-based workflow:</p> <ul> <li>\ud83d\udcb0 60-80% reduction in GPU costs</li> <li>\ud83d\udcca Individual step monitoring and debugging</li> <li>\ud83d\udd27 Configurable GPU vs CPU scheduling</li> <li>\u26a1 Disabled steps are completely skipped (no resource allocation)</li> </ul> <p>This page documents the original monolithic workflow for reference only.</p> <p>This guide describes how to run the original OFO photogrammetry workflow, which processes drone imagery using automate-metashape in a single monolithic container and performs post-processing steps.</p>"},{"location":"usage/photogrammetry-workflow/#prerequisites","title":"Prerequisites","text":"<p>Before running the workflow, ensure you have:</p> <ol> <li>Installed and set up the <code>openstack</code> and <code>kubectl</code> utilities</li> <li>Installed the Argo CLI</li> <li>Added the appropriate type and number of nodes to the cluster (cluster-access-and-resizing.md#cluster-resizing)</li> <li>Set up your <code>kubectl</code> authentication env var (part of instructions for adding nodes). Quick reference:</li> </ol> <pre><code>source ~/venv/openstack/bin/activate\nsource ~/.ofocluster/app-cred-ofocluster-openrc.sh\nexport KUBECONFIG=~/.ofocluster/ofocluster.kubeconfig\n</code></pre>"},{"location":"usage/photogrammetry-workflow/#workflow-overview","title":"Workflow overview","text":"<p>The workflow performs the following steps:</p> <ol> <li>Pulls raw drone imagery from <code>/ofo-share-2</code> onto the Kubernetes VM cluster</li> <li>Processes the imagery with Metashape</li> <li>Writes the imagery products to <code>/ofo-share-2</code></li> <li>Uploads the imagery products to <code>S3:ofo-internal</code> and deletes them from <code>/ofo-share</code></li> <li>Downloads the imagery products from S3 back to the cluster and performs postprocessing (CHMs, clipping, COGs, thumbnails)</li> <li>Uploads the final products to <code>S3:ofo-public</code></li> </ol>"},{"location":"usage/photogrammetry-workflow/#setup","title":"Setup","text":""},{"location":"usage/photogrammetry-workflow/#1-prepare-inputs","title":"1. Prepare inputs","text":"<p>Before running the workflow, you need to prepare three types of inputs on the cluster's shared storage:</p> <ol> <li>Drone imagery datasets (JPEG images)</li> <li>Metashape configuration files</li> <li>A config list file specifying which configs to process</li> </ol> <p>All inputs must be placed in <code>/ofo-share-2/argo-data/</code>.</p>"},{"location":"usage/photogrammetry-workflow/#directory-structure","title":"Directory structure","text":"<p>Here is a schematic of the <code>/ofo-share-2/argo-data</code> directory:</p> <pre><code>/ofo-share-2/argo-data/\n\u251c\u2500\u2500 argo-input/\n   \u251c\u2500\u2500 datasets/\n   \u2502   \u251c\u2500\u2500dataset_1/\n   \u2502   \u2502   \u251c\u2500\u2500 image_01.jpg\n   \u2502   \u2502   \u2514\u2500\u2500 image_02.jpg\n   \u2502   \u2514\u2500\u2500dataset_2/\n   \u2502       \u251c\u2500\u2500 image_01.jpg\n   \u2502       \u2514\u2500\u2500 image_02.jpg\n   \u251c\u2500\u2500 configs/\n   \u2502   \u251c\u2500\u2500config_dataset_1.yml\n   \u2502   \u2514\u2500\u2500config_dataset_2.yml\n   \u2514\u2500\u2500 config_list.txt\n</code></pre>"},{"location":"usage/photogrammetry-workflow/#add-drone-imagery-datasets","title":"Add drone imagery datasets","text":"<p>To add new drone imagery datasets to be processed using Argo, transfer files from your local machine (or the cloud) to the <code>/ofo-share-2</code> volume. Put the drone imagery projects to be processed in their own directory in <code>/ofo-share-2/argo-data/argo-input/datasets</code>.</p> <p>One data transfer method is the <code>scp</code> command-line tool:</p> <pre><code>scp -r &lt;local/directory/drone_image_dataset/&gt; exouser@&lt;vm.ip.address&gt;:/ofo-share-2/argo-data/argo-input/datasets\n</code></pre> <p>Replace <code>&lt;vm.ip.address&gt;</code> with the IP address of a cluster node that has the share mounted.</p>"},{"location":"usage/photogrammetry-workflow/#specify-metashape-parameters","title":"Specify Metashape parameters","text":"<p>Metashape processing parameters are specified in configuration YAML files which need to be located at <code>/ofo-share-2/argo-data/argo-input/configs/</code>.</p> <p>Every dataset to be processed needs to have its own standalone configuration file.</p> <p>Naming convention: Config files should be named to match the naming convention <code>&lt;config_id&gt;_&lt;datasetname&gt;.yml</code>. For example:</p> <ul> <li><code>01_benchmarking-greasewood.yml</code></li> <li><code>02_benchmarking-greasewood.yml</code></li> </ul> <p>Setting the <code>photo_path</code>: Within each metashape config.yml file, you must specify <code>photo_path</code> which is the location of the drone imagery dataset to be processed. When running via Argo workflows, this path refers to the location of the images inside the docker container.</p> <p>For example, if your drone images were uploaded to <code>/ofo-share-2/argo-data/argo-input/datasets/dataset_1</code>, then the <code>photo_path</code> should be written as:</p> <pre><code>photo_path: /data/argo-input/datasets/dataset_1\n</code></pre> <p>Parameters handled by Argo: The <code>output_path</code>, <code>project_path</code>, and <code>run_name</code> configuration parameters are handled automatically by the Argo workflow:</p> <ul> <li><code>output_path</code> and <code>project_path</code> are determined via the arguments passed to the automate-metashape container, which in turn are derived from the <code>RUN_FOLDER</code> workflow parameter passed when invoking <code>argo submit</code></li> <li><code>run_name</code> is pulled from the name of the config file (minus the extension) by the Argo workflow</li> </ul> <p>Any values specified for these parameters in the config.yml will be ignored.</p>"},{"location":"usage/photogrammetry-workflow/#create-a-config-list-file","title":"Create a config list file","text":"<p>We use a text file, for example <code>config_list.txt</code>, to tell the Argo workflow which config files should be processed in the current run. This text file should list the paths to each config.yml file you want to process (relative to <code>/ofo-share-2/argo-data</code>), one config file path per line.</p> <p>For example:</p> <pre><code>argo-input/configs/01_benchmarking-greasewood.yml\nargo-input/configs/02_benchmarking-greasewood.yml\nargo-input/configs/01_benchmarking-emerald-subset.yml\nargo-input/configs/02_benchmarking-emerald-subset.yml\n</code></pre> <p>This allows you to organize your config files in subdirectories or different locations. The dataset name will be automatically derived from the config filename (e.g., <code>argo-input/configs/dataset-name.yml</code> becomes dataset <code>dataset-name</code>).</p> <p>You can create your own config list file and name it whatever you want, placing it anywhere within <code>/ofo-share-2/argo-data/</code>. Then specify the path to it (relative to <code>/ofo-share-2/argo-data</code>) using the <code>CONFIG_LIST</code> parameter when submitting the workflow.</p>"},{"location":"usage/photogrammetry-workflow/#submit-the-workflow","title":"Submit the workflow","text":"<p>Once your cluster authentication is set up and your inputs are prepared, run:</p> <pre><code>argo submit -n argo photogrammetry-workflow.yaml \\\n-p CONFIG_LIST=argo-input/config-lists/config_list.txt \\\n-p RUN_FOLDER=gillan_june27 \\\n-p PHOTOGRAMMETRY_CONFIG_ID=01 \\\n-p S3_BUCKET_INTERNAL=ofo-internal \\\n-p S3_BUCKET_PUBLIC=ofo-public \\\n-p OUTPUT_DIRECTORY=jgillan_test \\\n-p S3_BOUNDARY_DIR=jgillan_test \\\n-p WORKING_DIR=/argo-output/temp-working-dir \\\n-p OFO_ARGO_IMAGES_TAG=latest\n</code></pre> <p>Database parameters (not currently functional): <pre><code>-p DB_PASSWORD=&lt;password&gt; \\\n-p DB_HOST=&lt;vm_ip_address&gt; \\\n-p DB_NAME=&lt;db_name&gt; \\\n-p DB_USER=&lt;user_name&gt;\n</code></pre></p>"},{"location":"usage/photogrammetry-workflow/#workflow-parameters","title":"Workflow parameters","text":"Parameter Description <code>CONFIG_LIST</code> Path to text file listing paths to metashape config files (all paths relative to <code>/ofo-share-2/argo-data</code>) <code>RUN_FOLDER</code> Name for the parent directory of the Metashape outputs (locally under <code>argo-data/argo-outputs</code> and at the top level of the S3 bucket). Example: <code>photogrammetry-outputs</code>. <code>PHOTOGRAMMETRY_CONFIG_ID</code> Two-digit configuration ID (e.g., <code>01</code>, <code>02</code>) used to organize outputs into <code>photogrammetry_NN</code> subdirectories in S3 for both raw and postprocessed products. If not specified, both raw and postprocessed products are stored directly in <code>RUN_FOLDER</code> (no <code>photogrammetry_NN</code> subfolder). <code>S3_BUCKET_INTERNAL</code> S3 bucket for internal/intermediate outputs where raw Metashape products (orthomosaics, point clouds, DEMs) are uploaded (typically <code>ofo-internal</code>). When <code>PHOTOGRAMMETRY_CONFIG_ID</code> is set, products are uploaded to <code>{bucket}/{RUN_FOLDER}/photogrammetry_{PHOTOGRAMMETRY_CONFIG_ID}/</code>. When not set, products go to <code>{bucket}/{RUN_FOLDER}/</code>. <code>S3_BUCKET_PUBLIC</code> S3 bucket for public/final outputs (postprocessed, clipped products ready for distribution) and where boundary files are stored (typically <code>ofo-public</code>). <code>OUTPUT_DIRECTORY</code> Name of parent folder where postprocessed products are uploaded. When <code>PHOTOGRAMMETRY_CONFIG_ID</code> is set, products are organized as <code>{OUTPUT_DIRECTORY}/{mission_name}/photogrammetry_{PHOTOGRAMMETRY_CONFIG_ID}/</code>. When not set, products go to <code>{OUTPUT_DIRECTORY}/{mission_name}/</code>. <code>S3_BOUNDARY_DIR</code> Parent directory in <code>S3_BUCKET_PUBLIC</code> where mission boundary polygons reside (used to clip imagery). Expected structure: <code>&lt;S3_BOUNDARY_DIR&gt;/&lt;mission_name&gt;/metadata-mission/&lt;mission_name&gt;_mission-metadata.gpkg</code>. <code>WORKING_DIR</code> Directory within container for downloading and postprocessing (typically <code>/tmp/processing</code> which downloads data to the processing computer; can be changed to a persistent volume) <code>OFO_ARGO_IMAGES_TAG</code> Docker image tag for OFO Argo containers (postprocessing) (default: <code>latest</code>). Use a specific branch name or tag to test development versions (e.g., <code>dy-manila</code>) <code>DB_*</code> Database parameters for logging Argo status (not currently functional; credentials in OFO credentials document) <p>Secrets configuration: - S3 credentials: S3 access credentials, provider type, and endpoint URL are configured via the <code>s3-credentials</code> Kubernetes secret - Agisoft license: Metashape floating license server address is configured via the   <code>agisoft-license</code> Kubernetes secret</p> <p>These secrets should have been created (within the <code>argo</code> namespace) during cluster creation.</p>"},{"location":"usage/photogrammetry-workflow/#monitor-the-workflow","title":"Monitor the workflow","text":""},{"location":"usage/photogrammetry-workflow/#using-the-argo-ui","title":"Using the Argo UI","text":"<p>The Argo UI is great for troubleshooting and checking additional logs. Access it at argo.focal-lab.org, using the credentials from Vaultwarden under the record \"Argo UI token\".</p>"},{"location":"usage/photogrammetry-workflow/#navigating-the-argo-ui","title":"Navigating the Argo UI","text":"<p>The Workflows tab on the left side menu shows all running workflows. Click a current workflow to see a schematic of the jobs spread across multiple instances:</p> <p></p> <p>Click on a specific job to see detailed information including which VM it is running on, the duration of the process, and logs:</p> <p></p> <p>A successful Argo run looks like this:</p> <p></p>"},{"location":"usage/photogrammetry-workflow/#workflow-outputs","title":"Workflow outputs","text":"<p>The final outputs will be written to <code>S3:ofo-public</code> in the following directory structure:</p> <pre><code>/S3:ofo-public/\n\u251c\u2500\u2500 &lt;OUTPUT_DIRECTORY&gt;/\n    \u251c\u2500\u2500 dataset1/\n         \u251c\u2500\u2500 images/\n         \u251c\u2500\u2500 metadata-images/\n         \u251c\u2500\u2500 metadata-mission/\n            \u2514\u2500\u2500 dataset1_mission-metadata.gpkg\n         \u251c\u2500\u2500photogrammetry_01/\n            \u251c\u2500\u2500 full/\n               \u251c\u2500\u2500 dataset1_cameras.xml\n               \u251c\u2500\u2500 dataset1_chm-ptcloud.tif\n               \u251c\u2500\u2500 dataset1_dsm-ptcloud.tif\n               \u251c\u2500\u2500 dataset1_dtm-ptcloud.tif\n               \u251c\u2500\u2500 dataset1_log.txt\n               \u251c\u2500\u2500 dataset1_ortho-dtm-ptcloud.tif\n               \u251c\u2500\u2500 dataset1_points.copc.laz\n               \u2514\u2500\u2500 dataset1_report.pdf\n            \u251c\u2500\u2500 thumbnails/\n               \u251c\u2500\u2500 dataset1_chm-ptcloud.png\n               \u251c\u2500\u2500 dataset1_dsm-ptcloud.png\n               \u251c\u2500\u2500 dataset1_dtm-ptcloud.png\n               \u2514\u2500\u2500 dataset1-ortho-dtm-ptcloud.png\n         \u251c\u2500\u2500photogrammetry_02/\n            \u251c\u2500\u2500 full/\n               \u251c\u2500\u2500 dataset1_cameras.xml\n               \u251c\u2500\u2500 dataset1_chm-ptcloud.tif\n               \u251c\u2500\u2500 dataset1_dsm-ptcloud.tif\n               \u251c\u2500\u2500 dataset1_dtm-ptcloud.tif\n               \u251c\u2500\u2500 dataset1_log.txt\n               \u251c\u2500\u2500 dataset1_ortho-dtm-ptcloud.tif\n               \u251c\u2500\u2500 dataset1_points.copc.laz\n               \u2514\u2500\u2500 dataset1_report.pdf\n            \u251c\u2500\u2500 thumbnails/\n               \u251c\u2500\u2500 dataset1_chm-ptcloud.png\n               \u251c\u2500\u2500 dataset1_dsm-ptcloud.png\n               \u251c\u2500\u2500 dataset1_dtm-ptcloud.png\n               \u2514\u2500\u2500 dataset1-ortho-dtm-ptcloud.png\n    \u251c\u2500\u2500 dataset2/\n</code></pre> <p>This directory structure should already exist prior to running the Argo workflow.</p>"},{"location":"usage/stepbased-quick-reference/","title":"Step-Based Workflow Quick Reference","text":"<p>Quick command reference for common step-based workflow operations. See the complete guide for detailed explanations.</p>"},{"location":"usage/stepbased-quick-reference/#submit-workflow","title":"Submit Workflow","text":""},{"location":"usage/stepbased-quick-reference/#basic-submission","title":"Basic Submission","text":"<pre><code>argo submit photogrammetry-workflow-stepbased.yaml \\\n  --name \"my-run-name\" \\\n  -p CONFIG_LIST=\"argo-input/config-lists/my_missions.txt\" \\\n  -p RUN_FOLDER=\"2024-12-19-run\" \\\n  -p S3_BUCKET_INTERNAL=\"ofo-internal\"\n</code></pre>"},{"location":"usage/stepbased-quick-reference/#full-submission-with-all-parameters","title":"Full Submission with All Parameters","text":"<pre><code>argo submit photogrammetry-workflow-stepbased.yaml \\\n  --name \"production-run-2024-12-19\" \\\n  -p CONFIG_LIST=\"argo-input/config-lists/december_missions.txt\" \\\n  -p RUN_FOLDER=\"2024-12-19-production\" \\\n  -p PHOTOGRAMMETRY_CONFIG_ID=\"v2\" \\\n  -p S3_BUCKET_INTERNAL=\"ofo-internal\" \\\n  -p S3_BUCKET_PUBLIC=\"ofo-public\" \\\n  -p OUTPUT_DIRECTORY=\"processed-outputs/december-2024\" \\\n  -p S3_BOUNDARY_DIR=\"boundaries\" \\\n  -p OFO_ARGO_IMAGES_TAG=\"latest\" \\\n  -p AUTOMATE_METASHAPE_IMAGE_TAG=\"latest\"\n</code></pre>"},{"location":"usage/stepbased-quick-reference/#monitor-workflows","title":"Monitor Workflows","text":""},{"location":"usage/stepbased-quick-reference/#watch-workflow-progress","title":"Watch Workflow Progress","text":"<pre><code># Watch a specific workflow\nargo watch &lt;workflow-name&gt;\n\n# List all running workflows\nargo list\n\n# List all workflows (including completed)\nargo list --all\n\n# Get detailed workflow info\nargo get &lt;workflow-name&gt;\n</code></pre>"},{"location":"usage/stepbased-quick-reference/#view-logs","title":"View Logs","text":"<pre><code># View logs for preprocessing step\nargo logs &lt;workflow-name&gt; -c determine-datasets\n\n# View logs for a specific mission's step\n# Format: process-datasets-&lt;N&gt;.&lt;step-name&gt;\n# where N is the mission index (0-based)\nargo logs &lt;workflow-name&gt; -c process-datasets-0.setup\nargo logs &lt;workflow-name&gt; -c process-datasets-0.match-photos-gpu\nargo logs &lt;workflow-name&gt; -c process-datasets-1.build-depth-maps\n\n# Follow logs in real-time\nargo logs &lt;workflow-name&gt; -c process-datasets-0.setup -f\n</code></pre>"},{"location":"usage/stepbased-quick-reference/#deletestop-workflows","title":"Delete/Stop Workflows","text":"<pre><code># Stop a running workflow\nargo stop &lt;workflow-name&gt;\n\n# Delete a workflow\nargo delete &lt;workflow-name&gt;\n\n# Delete all completed workflows\nargo delete --completed\n</code></pre>"},{"location":"usage/stepbased-quick-reference/#config-file-setup","title":"Config File Setup","text":""},{"location":"usage/stepbased-quick-reference/#minimal-config-example","title":"Minimal Config Example","text":"<pre><code>project:\n  project_name: \"mission_001\"\n  photo_path: \"/data/drone-imagery/mission_001\"\n  project_crs: \"EPSG::32610\"\n\nadd_photos:\n  enabled: true\n\nmatch_photos:\n  enabled: true\n  gpu_enabled: true  # Use GPU node\n\nalign_cameras:\n  enabled: true\n\nbuild_depth_maps:\n  enabled: true\n\nbuild_point_cloud:\n  enabled: true\n\nbuild_dem:\n  enabled: true\n\nbuild_orthomosaic:\n  enabled: true\n</code></pre>"},{"location":"usage/stepbased-quick-reference/#gpu-configuration","title":"GPU Configuration","text":"<pre><code># Use GPU for match_photos\nmatch_photos:\n  enabled: true\n  gpu_enabled: true  # GPU node\n\n# Use CPU for match_photos (cheaper)\nmatch_photos:\n  enabled: true\n  gpu_enabled: false  # CPU node\n\n# Build mesh on GPU (default)\nbuild_mesh:\n  enabled: true\n  gpu_enabled: true  # GPU node\n\n# Build mesh on CPU (cheaper, slower)\nbuild_mesh:\n  enabled: true\n  gpu_enabled: false  # CPU node\n</code></pre>"},{"location":"usage/stepbased-quick-reference/#mig-gpu-partitioning-configuration","title":"MIG (GPU Partitioning) Configuration","text":"<pre><code># Use MIG partition instead of full GPU\nmatch_photos:\n  enabled: true\n  gpu_enabled: true\n  gpu_resource: \"nvidia.com/mig-1g.5gb\"  # 1/7 compute, 5GB VRAM\n  gpu_count: 2  # Request 2 slices for 2/7 compute\n\nbuild_depth_maps:\n  enabled: true\n  gpu_resource: \"nvidia.com/mig-3g.20gb\"  # 3/7 compute, 20GB VRAM\n  # gpu_count: 1 (default)\n\n# Available: nvidia.com/gpu (full, default), mig-1g.5gb, mig-2g.10gb, mig-3g.20gb\n# Use gpu_count to request multiple slices of the same type\n</code></pre>"},{"location":"usage/stepbased-quick-reference/#troubleshooting-commands","title":"Troubleshooting Commands","text":""},{"location":"usage/stepbased-quick-reference/#check-preprocessing-output","title":"Check Preprocessing Output","text":"<pre><code># See which steps are enabled for each mission\nargo logs &lt;workflow-name&gt; -c determine-datasets\n</code></pre>"},{"location":"usage/stepbased-quick-reference/#check-failed-steps","title":"Check Failed Steps","text":"<pre><code># Get workflow status\nargo get &lt;workflow-name&gt;\n\n# View logs of failed step\nargo logs &lt;workflow-name&gt; -c process-datasets-0.&lt;failed-step-name&gt;\n\n# Get node info where step ran\nargo get &lt;workflow-name&gt; -o json | jq '.status.nodes'\n</code></pre>"},{"location":"usage/stepbased-quick-reference/#verify-config-files","title":"Verify Config Files","text":"<pre><code># On the cluster, check config file exists\nls -l /data/argo-input/configs/\n\n# Check config list file\ncat /data/argo-input/config-lists/my_missions.txt\n\n# Validate YAML syntax\npython3 -c \"import yaml; yaml.safe_load(open('/data/argo-input/configs/mission_001.yml'))\"\n</code></pre>"},{"location":"usage/stepbased-quick-reference/#controlling-parallelism","title":"Controlling Parallelism","text":"<p>The max number of concurrent projects is controlled by the <code>parallelism</code> field in the workflow file (around line 79). Edit this value directly before submitting. Default is <code>10</code>.</p> <pre><code># In photogrammetry-workflow-stepbased.yaml\n- name: main\n  parallelism: 10  # Change this value as needed\n  steps:\n    ...\n</code></pre> <p>See the complete guide for details on why this can't be a command-line parameter.</p>"},{"location":"usage/stepbased-quick-reference/#common-workflow-patterns","title":"Common Workflow Patterns","text":""},{"location":"usage/stepbased-quick-reference/#test-with-single-mission","title":"Test with Single Mission","text":"<pre><code># Create test config list\necho \"argo-input/configs/test_mission.yml\" &gt; /data/argo-input/config-lists/test-single.txt\n\n# Submit test run\nargo submit photogrammetry-workflow-stepbased.yaml \\\n  --name \"test-single-mission\" \\\n  -p CONFIG_LIST=\"argo-input/config-lists/test-single.txt\" \\\n  -p RUN_FOLDER=\"test-$(date +%Y%m%d-%H%M%S)\"\n</code></pre>"},{"location":"usage/stepbased-quick-reference/#process-multiple-missions","title":"Process Multiple Missions","text":"<pre><code># Create config list with multiple missions\ncat &gt; /data/argo-input/config-lists/batch.txt &lt;&lt;EOF\nargo-input/configs/mission_001.yml\nargo-input/configs/mission_002.yml\nargo-input/configs/mission_003.yml\nEOF\n\n# Submit batch run\nargo submit photogrammetry-workflow-stepbased.yaml \\\n  --name \"batch-run-$(date +%Y%m%d)\" \\\n  -p CONFIG_LIST=\"argo-input/config-lists/batch.txt\" \\\n  -p RUN_FOLDER=\"batch-$(date +%Y%m%d)\"\n</code></pre>"},{"location":"usage/stepbased-quick-reference/#step-names-reference","title":"Step Names Reference","text":""},{"location":"usage/stepbased-quick-reference/#all-available-steps","title":"All Available Steps","text":"<ol> <li><code>setup</code> - Initialize project, add photos</li> <li><code>match_photos</code> - Generate tie points (GPU/CPU)</li> <li><code>align_cameras</code> - Align cameras, post-processing</li> <li><code>build_depth_maps</code> - Create depth maps (GPU only)</li> <li><code>build_point_cloud</code> - Generate dense point cloud</li> <li><code>build_mesh</code> - Build 3D mesh (GPU/CPU)</li> <li><code>build_dem_orthomosaic</code> - Create DEMs/orthomosaics</li> <li><code>match_photos_secondary</code> - Match secondary photos (GPU/CPU)</li> <li><code>align_cameras_secondary</code> - Align secondary cameras</li> <li><code>finalize</code> - Cleanup and reports</li> </ol>"},{"location":"usage/stepbased-quick-reference/#task-names-in-argo-ui","title":"Task Names in Argo UI","text":"<p>Mission N (0-indexed) tasks appear as: - <code>process-datasets-N.setup</code> - <code>process-datasets-N.match-photos-gpu</code> (or <code>-cpu</code>) - <code>process-datasets-N.align-cameras</code> - <code>process-datasets-N.build-depth-maps</code> - <code>process-datasets-N.build-point-cloud</code> - <code>process-datasets-N.build-mesh-gpu</code> (or <code>-cpu</code>) - <code>process-datasets-N.build-dem-orthomosaic</code> - <code>process-datasets-N.match-photos-secondary-gpu</code> (or <code>-cpu</code>) - <code>process-datasets-N.align-cameras-secondary</code> - <code>process-datasets-N.finalize</code> - <code>process-datasets-N.rclone-upload-task</code> - <code>process-datasets-N.postprocessing-task</code> - <code>process-datasets-N.cleanup-iteration</code></p>"},{"location":"usage/stepbased-quick-reference/#useful-argo-cli-options","title":"Useful Argo CLI Options","text":"<pre><code># Submit and watch immediately\nargo submit &lt;workflow.yaml&gt; --watch\n\n# Submit with custom service account\nargo submit &lt;workflow.yaml&gt; --serviceaccount my-sa\n\n# Get workflow output parameters\nargo get &lt;workflow-name&gt; -o json | jq '.status.outputs.parameters'\n\n# Resubmit a workflow with same parameters\nargo resubmit &lt;workflow-name&gt;\n\n# Retry a failed workflow\nargo retry &lt;workflow-name&gt;\n</code></pre>"},{"location":"usage/stepbased-quick-reference/#performance-tips","title":"Performance Tips","text":""},{"location":"usage/stepbased-quick-reference/#for-small-datasets-100-images","title":"For Small Datasets (&lt;100 images)","text":"<pre><code>match_photos:\n  enabled: true\n  gpu_enabled: false  # CPU is sufficient\n\nbuild_mesh:\n  enabled: false  # Skip if not needed\n</code></pre>"},{"location":"usage/stepbased-quick-reference/#for-large-datasets-500-images","title":"For Large Datasets (&gt;500 images)","text":"<pre><code>match_photos:\n  enabled: true\n  gpu_enabled: true  # GPU recommended\n\nbuild_depth_maps:\n  enabled: true  # Always uses GPU\n\nbuild_mesh:\n  enabled: true\n  gpu_enabled: true  # GPU recommended for large meshes\n</code></pre>"},{"location":"usage/stepbased-quick-reference/#cost-optimization","title":"Cost Optimization","text":"<pre><code># Minimize GPU usage\nmatch_photos:\n  gpu_enabled: false  # Use CPU\n\nbuild_mesh:\n  enabled: false  # Skip mesh generation if not needed\n\n# Remove point cloud after DEM/ortho generation\nbuild_point_cloud:\n  enabled: true\n  remove_after_export: true  # Cleanup in finalize step\n</code></pre>"},{"location":"usage/stepbased-quick-reference/#getting-help","title":"Getting Help","text":"<ul> <li>Full Guide: See MULTINODE-WORKFLOW-GUIDE.md</li> <li>Implementation Details: See <code>implementation-plans/step-workflow-implementation-plan.md</code></li> <li>Argo Docs: https://argoproj.github.io/argo-workflows/</li> <li>automate-metashape: https://github.com/open-forest-observatory/automate-metashape</li> </ul>"},{"location":"usage/stepbased-workflow/","title":"Running the step-based photogrammetry workflow","text":"<p>Recommended Workflow</p> <p>This is the recommended workflow for photogrammetry processing. It provides optimized resource allocation, cost savings, and better monitoring compared to the original monolithic workflow.</p> <p>This guide describes how to run the OFO step-based photogrammetry workflow, which splits Metashape processing into 10 individual steps with optimized CPU/GPU node allocation. The workflow uses automate-metashape and performs post-processing steps.</p>"},{"location":"usage/stepbased-workflow/#key-benefits","title":"Key Benefits","text":"<ul> <li>\ud83c\udfaf GPU steps (match_photos, build_depth_maps, build_mesh) run on expensive GPU nodes only when needed</li> <li>\ud83d\udcbb CPU steps (align_cameras, build_point_cloud, build_dem_orthomosaic, etc.) run on cheaper CPU nodes</li> <li>\u26a1 Disabled steps are completely skipped (no pod creation, no resource allocation)</li> <li>\ud83d\udcca Fine-grained monitoring - Track progress of each individual step in the Argo UI</li> <li>\ud83d\udd27 Flexible GPU usage - Configure whether GPU-capable steps use GPU or CPU nodes</li> <li>\ud83d\udcb0 Cost optimization - Reduce GPU usage by 60-80% compared to monolithic workflow</li> </ul>"},{"location":"usage/stepbased-workflow/#prerequisites","title":"Prerequisites","text":"<p>Before running the workflow, ensure you have:</p> <ol> <li>Installed and set up the <code>openstack</code> and <code>kubectl</code> utilities</li> <li>Installed the Argo CLI</li> <li>Added the appropriate type and number of nodes to the cluster</li> <li>Set up your <code>kubectl</code> authentication env var (part of instructions for adding nodes). Quick reference:</li> </ol> <pre><code>source ~/venv/openstack/bin/activate\nsource ~/.ofocluster/app-cred-ofocluster-openrc.sh\nexport KUBECONFIG=~/.ofocluster/ofocluster.kubeconfig\n</code></pre>"},{"location":"usage/stepbased-workflow/#workflow-overview","title":"Workflow overview","text":"<p>The step-based workflow executes 10 separate Metashape processing steps as individual containerized tasks, followed by upload, post-processing, and cleanup. Each mission processes sequentially through these steps:</p>"},{"location":"usage/stepbased-workflow/#metashape-processing-steps","title":"Metashape Processing Steps","text":"<ol> <li>setup (CPU) - Initialize project, add photos, calibrate reflectance</li> <li>match_photos (GPU/CPU configurable) - Generate tie points for camera alignment</li> <li>align_cameras (CPU) - Align cameras, add GCPs, optimize, filter sparse points</li> <li>build_depth_maps (GPU) - Create depth maps for dense reconstruction</li> <li>build_point_cloud (CPU) - Generate dense point cloud from depth maps</li> <li>build_mesh (GPU/CPU configurable) - Build 3D mesh model</li> <li>build_dem_orthomosaic (CPU) - Create DEMs and orthomosaic products</li> <li>match_photos_secondary (GPU/CPU configurable, optional) - Match secondary photos if provided</li> <li>align_cameras_secondary (CPU, optional) - Align secondary cameras if provided</li> <li>finalize (CPU) - Cleanup, generate reports</li> </ol>"},{"location":"usage/stepbased-workflow/#post-processing-steps","title":"Post-Processing Steps","text":"<ol> <li>rclone-upload-task - Upload Metashape outputs to S3</li> <li>postprocessing-task - Generate CHMs, clip to boundaries, create COGs and thumbnails, upload to S3</li> <li>cleanup-iteration - Remove temporary iteration directories after successful postprocessing</li> </ol> <p>Sequential Execution</p> <p>Steps execute sequentially within each mission to prevent conflicts with shared Metashape project files. However, multiple missions process in parallel, each with its own step sequence.</p> <p>Automatic Cleanup: After postprocessing completes successfully, the workflow automatically removes the temporary iteration directory (<code>{TEMP_WORKING_DIR}/{workflow-name}/{iteration-id}/</code>) to free disk space, keeping only the final products uploaded to S3.</p> <p>Conditional Execution</p> <p>Steps disabled in your config file are completely skipped - no container is created and no resources are allocated. This is more efficient than the original workflow where disabled operations still ran inside a single long-running container.</p>"},{"location":"usage/stepbased-workflow/#setup","title":"Setup","text":""},{"location":"usage/stepbased-workflow/#prepare-inputs","title":"Prepare inputs","text":"<p>Before running the workflow, you need to prepare three types of inputs on the cluster's shared storage:</p> <ol> <li>Drone imagery datasets (JPEG images)</li> <li>Metashape configuration files</li> <li>A config list file specifying which configs to process</li> </ol> <p>All inputs must be placed in <code>/ofo-share/argo-data/</code>.</p>"},{"location":"usage/stepbased-workflow/#add-drone-imagery-datasets","title":"Add drone imagery datasets","text":"<p>To add new drone imagery datasets to be processed using Argo, transfer files from your local machine (or the cloud) to the <code>/ofo-share</code> volume. Put the drone imagery datasets to be processed in their own directory in <code>/ofo-share/argo-data/argo-input/datasets</code> (or another folder within <code>argo-input</code>).</p> <p>One data transfer method is the <code>scp</code> command-line tool:</p> <pre><code>scp -r &lt;local/directory/drone_image_dataset/&gt; exouser@&lt;vm.ip.address&gt;:/ofo-share/argo-data/argo-input/datasets\n</code></pre> <p>Replace <code>&lt;vm.ip.address&gt;</code> with the IP address of a cluster node that has the share mounted.</p>"},{"location":"usage/stepbased-workflow/#specify-metashape-parameters","title":"Specify Metashape parameters","text":"<p>Config Structure Requirement</p> <p>The step-based workflow requires an updated config structure with:</p> <ul> <li>Global settings under <code>project:</code> section</li> <li>Each operation as a top-level config section with <code>enabled</code> flag</li> <li>Separate <code>match_photos</code> and <code>align_cameras</code> sections (not combined <code>alignPhotos</code>)</li> <li>Separate <code>build_dem</code> and <code>build_orthomosaic</code> sections</li> </ul> <p>See the updated config example for the full structure.</p> <p>Metashape processing parameters are specified in configuration YAML files which should be placed somewhere within <code>/ofo-share/argo-data/argo-input</code>.</p> <p>Every project to be processed needs to have its own standalone configuration file.</p> <p>Setting the <code>photo_path</code>: Within the <code>project:</code> section of the config YAML, you must specify <code>photo_path</code> which is the location of the drone imagery dataset. When running via Argo workflows, this path refers to the location inside the docker container. The <code>/ofo-share/argo-data</code> directory gets mounted at <code>/data</code> inside the container, so for example, if your drone images are at <code>/ofo-share/argo-data/argo-input/datasets/dataset_1</code>, then the <code>photo_path</code> should be written as:</p> <pre><code>project:\n  photo_path: /data/argo-input/datasets/dataset_1\n</code></pre>"},{"location":"usage/stepbased-workflow/#downloading-imagery-from-s3-optional","title":"Downloading imagery from S3 (optional)","text":"<p>Instead of pre-staging imagery on the shared PVC, you can have the workflow automatically download and extract imagery zip files from S3 at runtime. This is useful for:</p> <ul> <li>Cloud-native workflows: Process imagery stored in S3 without manual uploads</li> <li>One-time processing: Imagery that doesn't need to persist after the workflow</li> <li>Remote collaboration: Team members can trigger workflows without PVC access</li> </ul>"},{"location":"usage/stepbased-workflow/#when-to-use-s3-imagery-download","title":"When to use S3 imagery download","text":"<p>Use this feature when:</p> <ul> <li>Your imagery is already stored as zip files in S3</li> <li>You want to avoid manual file transfers to the cluster</li> <li>You're processing imagery that won't be reused</li> </ul> <p>Don't use this feature when:</p> <ul> <li>Your imagery is already on the PVC (use direct paths instead)</li> <li>You need to reprocess the same imagery multiple times (pre-staging is more efficient)</li> <li>Your zip files are very large and bandwidth is a concern</li> </ul>"},{"location":"usage/stepbased-workflow/#configuration","title":"Configuration","text":"<p>Add the following to the <code>argo:</code> section of your config file:</p> <pre><code>argo:\n  # List of S3 zip files to download (can also be a single string)\n  s3_imagery_zip_download:\n    - ofo-public/drone/missions_01/000558/images/000558_images.zip\n    - ofo-public/drone/missions_01/000559/images/000559_images.zip\n\n  # Whether to delete downloaded imagery after workflow completes (default: true)\n  cleanup_downloaded_imagery: true\n</code></pre> Parameter Description Default <code>s3_imagery_zip_download</code> S3 path(s) of zip files to download. Can be a single string or a list. Format: <code>bucket/path/file.zip</code>. The S3 endpoint and credentials are configured in the cluster's <code>s3-credentials</code> Kubernetes secret. (none) <code>cleanup_downloaded_imagery</code> If <code>true</code>, downloaded imagery is deleted after photogrammetry completes to free disk space <code>true</code>"},{"location":"usage/stepbased-workflow/#path-syntax-the-__downloaded__-prefix","title":"Path syntax: The <code>__DOWNLOADED__</code> prefix","text":"<p>When using S3 imagery download, reference downloaded files in <code>photo_path</code> using the <code>__DOWNLOADED__</code> prefix:</p> <pre><code>project:\n  project_name: my_forest_plot\n  photo_path:\n    - __DOWNLOADED__/000558_images/000558-01\n    - __DOWNLOADED__/000558_images/000558-02\n    - __DOWNLOADED__/000559_images/000559-01\n</code></pre> <p>The workflow automatically replaces <code>__DOWNLOADED__</code> with the actual download location before photogrammetry begins.</p>"},{"location":"usage/stepbased-workflow/#zip-file-structure-requirements","title":"Zip file structure requirements","text":"<p>The zip filename (without <code>.zip</code> extension) becomes the extraction folder name. Plan your <code>photo_path</code> entries accordingly:</p> <p>Example: Downloading <code>000558_images.zip</code> containing: <pre><code>000558_images.zip\n\u251c\u2500\u2500 000558-01/\n\u2502   \u251c\u2500\u2500 IMG_0001.jpg\n\u2502   \u2514\u2500\u2500 IMG_0002.jpg\n\u2514\u2500\u2500 000558-02/\n    \u251c\u2500\u2500 IMG_0001.jpg\n    \u2514\u2500\u2500 IMG_0002.jpg\n</code></pre></p> <p>Results in this structure after extraction: <pre><code>{download_dir}/\n\u2514\u2500\u2500 000558_images/          \u2190 folder name from zip filename\n    \u251c\u2500\u2500 000558-01/\n    \u2502   \u251c\u2500\u2500 IMG_0001.jpg\n    \u2502   \u2514\u2500\u2500 IMG_0002.jpg\n    \u2514\u2500\u2500 000558-02/\n        \u251c\u2500\u2500 IMG_0001.jpg\n        \u2514\u2500\u2500 IMG_0002.jpg\n</code></pre></p> <p>Reference these paths as: <pre><code>photo_path:\n  - __DOWNLOADED__/000558_images/000558-01\n  - __DOWNLOADED__/000558_images/000558-02\n</code></pre></p>"},{"location":"usage/stepbased-workflow/#complete-example-configuration","title":"Complete example configuration","text":"<pre><code>argo:\n  # S3 imagery download settings\n  s3_imagery_zip_download:\n    - ofo-public/drone/missions_01/000558/images/000558_images.zip\n  cleanup_downloaded_imagery: true\n\n  # Standard workflow settings\n  match_photos:\n    gpu_enabled: true\n    gpu_resource: \"nvidia.com/mig-1g.5gb\"\n    cpu_request: \"4\"\n    memory_request: \"16Gi\"\n\n  build_depth_maps:\n    gpu_resource: \"nvidia.com/mig-2g.10gb\"\n\nproject:\n  project_name: mission_000558\n  # Reference downloaded imagery with __DOWNLOADED__ prefix\n  photo_path:\n    - __DOWNLOADED__/000558_images/000558-01\n    - __DOWNLOADED__/000558_images/000558-02\n\n# ... rest of Metashape config sections ...\nmatch_photos:\n  enabled: true\n  # ...\n</code></pre>"},{"location":"usage/stepbased-workflow/#how-it-works","title":"How it works","text":"<p>When <code>s3_imagery_zip_download</code> is specified, the workflow adds these steps before photogrammetry:</p> <ol> <li>download-imagery: Downloads each zip file from S3 using rclone and extracts it</li> <li>transform-config: Replaces <code>__DOWNLOADED__</code> in <code>photo_path</code> with the actual download location</li> </ol> <p>After all processing completes (including upload and postprocessing):</p> <ol> <li>cleanup-imagery (if enabled): Deletes the downloaded imagery to free disk space</li> </ol> <p>Each project gets its own isolated download directory to prevent collisions when processing multiple projects in parallel.</p>"},{"location":"usage/stepbased-workflow/#troubleshooting-s3-imagery-download","title":"Troubleshooting S3 imagery download","text":""},{"location":"usage/stepbased-workflow/#config-validation-failed-downloaded-prefix-used-but-no-downloads-specified","title":"\"Config validation failed: DOWNLOADED prefix used but no downloads specified\"","text":"<p>Cause: Your <code>photo_path</code> contains <code>__DOWNLOADED__</code> but <code>s3_imagery_zip_download</code> is empty or missing.</p> <p>Solution: Either add <code>s3_imagery_zip_download</code> entries, or change <code>photo_path</code> to use direct paths (e.g., <code>/data/...</code>).</p>"},{"location":"usage/stepbased-workflow/#config-validation-failed-downloads-specified-but-no-downloaded-paths-found","title":"\"Config validation failed: Downloads specified but no DOWNLOADED paths found\"","text":"<p>Cause: You specified <code>s3_imagery_zip_download</code> but your <code>photo_path</code> entries don't use the <code>__DOWNLOADED__</code> prefix.</p> <p>Solution: Update <code>photo_path</code> to use <code>__DOWNLOADED__/...</code> paths that reference your downloaded zip contents.</p>"},{"location":"usage/stepbased-workflow/#download-fails-with-failed-to-copy-or-timeout-errors","title":"Download fails with \"Failed to copy\" or timeout errors","text":"<p>Possible causes:</p> <ul> <li>Incorrect S3 path format (should be <code>bucket/path/file.zip</code> without a remote prefix)</li> <li>S3 credentials not configured in the cluster's <code>s3-credentials</code> secret</li> <li>Network issues or S3 endpoint unavailable</li> <li>Zip file doesn't exist at the specified path</li> </ul> <p>Debug steps:</p> <ol> <li>Check the <code>download-imagery</code> step logs in Argo UI</li> <li>Verify the S3 path is correct by listing files (requires rclone configured with the same credentials):    <pre><code>rclone ls :s3:ofo-public/drone/missions_01/000558/images/ --s3-provider=Ceph --s3-endpoint=&lt;endpoint&gt;\n</code></pre></li> </ol>"},{"location":"usage/stepbased-workflow/#photo-path-not-found-errors-in-setup-step","title":"\"Photo path not found\" errors in setup step","text":"<p>Cause: The extracted zip structure doesn't match your <code>photo_path</code> entries.</p> <p>Solution:</p> <ol> <li>Check what's actually inside your zip file</li> <li>Ensure <code>photo_path</code> matches the extracted folder structure</li> <li>Remember: zip filename (minus <code>.zip</code>) becomes the top-level folder</li> </ol>"},{"location":"usage/stepbased-workflow/#disk-space-issues","title":"Disk space issues","text":"<p>Cause: Downloaded imagery fills up the shared storage.</p> <p>Solutions:</p> <ul> <li>Ensure <code>cleanup_downloaded_imagery: true</code> (default) to auto-delete after completion</li> <li>Process fewer projects in parallel to reduce concurrent disk usage</li> <li>Monitor disk usage during workflow execution</li> </ul>"},{"location":"usage/stepbased-workflow/#resource-request-configuration","title":"Resource request configuration","text":"<p>All Argo workflow resource requests (GPU, CPU, memory) are configured in the top-level <code>argo</code> section of your automate-metashape config file. The defaults assume one or more JS2 <code>m3.large</code> CPU nodes and one or more <code>mig1</code> (7-slice MIG <code>g3.xl</code>) GPU nodes (see cluster access and resizing).</p> <p>Importantly, using well-selected resource requests may allow more than one workflow step to schedule simultaneously on the same compute node, without substantially extending the compute time of either, thus greatly increasing compute efficiency by requiring fewer compute nodes. The example config YAML includes suggested resource requests we have developed through extensive benchmarking.</p>"},{"location":"usage/stepbased-workflow/#gpu-scheduling","title":"GPU scheduling","text":"<p>Three steps support configurable GPU usage via <code>argo.&lt;step&gt;.gpu_enabled</code> parameters:</p> <ul> <li><code>argo.match_photos.gpu_enabled</code> - If <code>true</code>, runs on GPU node; if <code>false</code>, runs on CPU node (default: <code>true</code>)</li> <li><code>argo.build_mesh.gpu_enabled</code> - If <code>true</code>, runs on GPU node; if <code>false</code>, runs on CPU node (default: <code>true</code>)</li> <li><code>argo.match_photos_secondary.gpu_enabled</code> - Inherits from <code>match_photos</code> unless explicitly set</li> </ul> <p>The <code>build_depth_maps</code> step always runs on GPU nodes (<code>gpu_enabled</code> cannot be disabled) as it always benefits from GPU acceleration. However, you can configure the GPU resource type and count using <code>gpu_resource</code> and <code>gpu_count</code>.</p>"},{"location":"usage/stepbased-workflow/#gpu-resource-selection-mig-support","title":"GPU resource selection (MIG Support)","text":"<p>For GPU steps, you can specify which GPU resource to request using <code>gpu_resource</code> and <code>gpu_count</code> in the <code>argo</code> section. This allows using MIG (Multi-Instance GPU) partitions instead of full GPUs:</p> <pre><code>argo:\n  match_photos:\n    gpu_enabled: true\n    gpu_resource: \"nvidia.com/mig-1g.5gb\"  # Use smallest MIG partition\n    gpu_count: 2                           # Request 2 MIG slices for more parallelism\n\n  build_depth_maps:\n    gpu_resource: \"nvidia.com/gpu\"         # Explicitly request full GPU (this is the default)\n    # gpu_count defaults to 1 if omitted\n\n  build_mesh:\n    gpu_enabled: true\n    gpu_resource: \"nvidia.com/mig-3g.20gb\" # Larger MIG partition for mesh building\n    gpu_count: 1\n</code></pre> <p>Available GPU resources:</p> Resource Description Pods per GPU <code>nvidia.com/gpu</code> Full GPU (default if <code>gpu_resource</code> omitted) 1 <code>nvidia.com/mig-1g.5gb</code> 1/7 compute, 5GB VRAM 7 <code>nvidia.com/mig-2g.10gb</code> 2/7 compute, 10GB VRAM 3 <code>nvidia.com/mig-3g.20gb</code> 3/7 compute, 20GB VRAM 2 <p>Use <code>gpu_count</code> to request multiple MIG slices (e.g., <code>gpu_count: 2</code> with <code>mig-1g.5gb</code> to get 2/7 compute power).</p> <p>When to use MIG</p> <p>Use MIG partitions when your GPU steps have low utilization. This allows multiple workflow steps to share a single physical GPU, reducing costs. In extensive benchmarking, we have found that we get the greatest efficiency with mig-1g.5gb nodes, potentially providing more than one slice to GPU-intensive pods.</p> <p>Nodegroup requirement</p> <p>MIG resources are only available on MIG-enabled nodegroups. Create a MIG nodegroup with a name containing <code>mig1-</code>, <code>mig2-</code>, or <code>mig3-</code> (see MIG nodegroups).</p>"},{"location":"usage/stepbased-workflow/#cpu-and-memory-configuration","title":"CPU and memory configuration","text":"<p>You can configure CPU and memory requests for all workflow steps (both CPU and GPU steps) using <code>cpu_request</code> and <code>memory_request</code> parameters in the <code>argo</code> section:</p> <pre><code>argo:\n  # Optional: Set global defaults that apply to all steps\n  defaults:\n    cpu_request: \"10\"        # Default CPU cores for all steps\n    memory_request: \"50Gi\"   # Default memory for all steps\n\n  # Override for specific steps\n  match_photos:\n    cpu_request: \"8\"         # Override default CPU request for this step\n    memory_request: \"32Gi\"   # Override default memory request for this step\n\n  build_depth_maps:\n    cpu_request: \"6\"\n    memory_request: \"24Gi\"\n\n  align_cameras:\n    cpu_request: \"15\"        # CPU-heavy step\n    memory_request: \"50Gi\"\n</code></pre> <p>Default values (if not specified) are hard-coded into the workflow YAML under the CPU and GPU step templates.</p> <p>Fallback order:</p> <ol> <li>Step-specific value (e.g., <code>argo.match_photos.cpu_request</code>)</li> <li>User default from <code>argo.defaults</code> (if specified)</li> <li>Hardcoded default (based on step type and GPU mode)</li> </ol> <p>Using defaults as a template</p> <p>You can leave step-level parameters blank/empty to use the defaults, which serves as a visual template:</p> <pre><code>argo:\n  defaults:\n    cpu_request: \"8\"\n    memory_request: \"40Gi\"\n\n  match_photos:\n    cpu_request:      # Blank = uses defaults.cpu_request \u2192 8\n    memory_request:   # Blank = uses defaults.memory_request \u2192 40Gi\n\n  build_depth_maps:\n    cpu_request: \"12\" # Override: uses 12 instead of defaults\n    memory_request:   # Blank = uses defaults.memory_request \u2192 40Gi\n</code></pre>"},{"location":"usage/stepbased-workflow/#secondary-photo-processing","title":"Secondary photo processing","text":"<p>The <code>match_photos_secondary</code> and <code>align_cameras_secondary</code> steps inherit resource configuration from their primary steps unless explicitly overridden:</p> <pre><code>argo:\n  match_photos:\n    gpu_resource: \"nvidia.com/mig-2g.10gb\"\n    cpu_request: \"6\"\n    memory_request: \"24Gi\"\n\n  # match_photos_secondary automatically inherits the above settings\n  # unless you override them:\n  match_photos_secondary:\n    gpu_resource: \"nvidia.com/mig-1g.5gb\"  # Override: use smaller GPU\n    # cpu_request and memory_request still inherited from match_photos\n</code></pre> <p>This 4-level fallback applies: Secondary-specific \u2192 Primary step \u2192 User defaults \u2192 Hardcoded defaults</p> <p>Parameters handled by Argo: The <code>project_path</code>, <code>output_path</code>, and <code>project_name</code> configuration parameters are handled automatically by the Argo workflow:</p> <ul> <li><code>project_path</code> and <code>output_path</code> are determined via CLI arguments passed to the automate-metashape container, derived from the <code>TEMP_WORKING_DIR</code> Argo workflow parameter (passed by the user on the command line when invoking <code>argo submit</code>)</li> <li><code>project_name</code> is extracted from <code>project.project_name</code> in the config file (or from the filename   of the config file if missing in the config) and passed by Argo via CLI to each step to ensure consistent project names per mission</li> </ul> <p>Any values specified for <code>project_path</code> and <code>output_path</code> in the config.yml will be overridden by Argo CLI arguments.</p>"},{"location":"usage/stepbased-workflow/#create-a-config-list-file","title":"Create a config list file","text":"<p>We use a text file, for example <code>config-list.txt</code>, to tell the Argo workflow which config files should be processed in the current run. Place this file in the same directory as your config files, then list just the filenames (not full paths), one per line.</p> <p>Example: If your configs are in <code>/ofo-share/argo-data/argo-input/configs/</code>, create a file at <code>/ofo-share/argo-data/argo-input/configs/config-list.txt</code>:</p> <pre><code># Benchmarking missions\n01_benchmarking-greasewood.yml\n02_benchmarking-greasewood.yml\n\n# Skipping emerald for now\n# 01_benchmarking-emerald-subset.yml\n# 02_benchmarking-emerald-subset.yml\n\n03_production-run.yml  # high priority\n</code></pre> <p>Features:</p> <ul> <li>Filenames only: List just the config filename; the directory is inferred from the config list's location</li> <li>Comments: Lines starting with <code>#</code> (after whitespace) are skipped</li> <li>Inline comments: Text after <code>#</code> on any line is ignored (e.g., <code>config.yml # note</code>)</li> <li>Blank lines: Empty lines are ignored for readability</li> <li>Backward compatibility: Absolute paths (starting with <code>/</code>) still work if needed</li> </ul> <p>The project name will be automatically derived from the config filename (e.g., <code>project-name.yml</code> becomes project <code>project-name</code>), unless explicitly set in the config file at <code>project.project_name</code> (which takes priority).</p> <p>You can create your own config list file and name it whatever you want, placing it anywhere within <code>/ofo-share/argo-data/</code>. Then specify the path to it within the container (using <code>/data/XYZ</code> to refer to <code>/ofo-share/argo-data/XYZ</code>) using the <code>CONFIG_LIST</code> parameter when submitting the workflow.</p>"},{"location":"usage/stepbased-workflow/#determine-the-maximum-number-of-projects-to-process-in-parallel","title":"Determine the maximum number of projects to process in parallel","text":"<p>When tasked with parallelizing across multiple multi-step DAGs, Argo prioritizes breadth first. So when it has a choice, it will start on a new DAG (metashape project) rather than starting the next step of an existing one. This is unfortunately not customizable, and it is undesirable because the workflow involves storing in-process files (including raw imagery, metashape project, outputs) locally during processing. Our shared storage does not have the space to store all files locally at the same time. In addition, we have a limited number of Metashape licenses. So we need to restrict the number of parallel DAGs (metashape projects) it will attempt to run.</p> <p>The workflow controls this via the <code>parallelism</code> field in the <code>main</code> template (line 66 in <code>photogrammetry-workflow-stepbased.yaml</code>). To change the max parallel projects, edit this value directly in the workflow file before submitting. The default is set to <code>10</code>.</p> <p>Why not a command-line parameter?</p> <p>Argo Workflows doesn't support parameter substitution for integer fields like <code>parallelism</code>, so this value must be hardcoded in the workflow file. This is an known issue with Argo and we should look for it to be resovled so we can implement it as a command line parameter.</p>"},{"location":"usage/stepbased-workflow/#adjusting-parallelism-on-a-running-workflow","title":"Adjusting parallelism on a running workflow","text":"<p>If you need to increase or decrease parallelism while a workflow is already running, you can patch the workflow in place. First, find your workflow name:</p> <pre><code>argo list -n argo\n</code></pre> <p>Then patch the <code>main</code> template's parallelism (index 0):</p> <pre><code>kubectl patch workflow &lt;workflow-name&gt; -n argo --type='json' \\\n  -p='[{\"op\": \"replace\", \"path\": \"/spec/templates/0/parallelism\", \"value\": 20}]'\n</code></pre> <p>The change takes effect immediately for any new pods that haven't started yet. Already-running pods are not affected.</p> <p>Note</p> <p>This only affects the running workflow instance. Future submissions will still use the value from the YAML file.</p>"},{"location":"usage/stepbased-workflow/#submit-the-workflow","title":"Submit the workflow","text":"<p>Once your cluster authentication is set up and your inputs are prepared, run:</p> <pre><code>argo submit -n argo photogrammetry-workflow-stepbased.yaml \\\n  --name \"my-run-$(date +%Y%m%d)\" \\\n  -p CONFIG_LIST=/data/argo-input/configs/config-list.txt \\\n  -p TEMP_WORKING_DIR=/data/argo-output/tmp/derek-0202 \\\n  -p S3_BUCKET_INTERNAL=ofo-internal \\\n  -p S3_PHOTOGRAMMETRY_DIR=photogrammetry-outputs_dytest02 \\\n  -p PHOTOGRAMMETRY_CONFIG_ID=03 \\\n  -p S3_BUCKET_PUBLIC=ofo-public \\\n  -p S3_POSTPROCESSED_DIR=drone_dytest02 \\\n  -p S3_BOUNDARY_DIR=drone_dytest02 \\\n  -p OFO_ARGO_IMAGES_TAG=latest \\\n  -p AUTOMATE_METASHAPE_IMAGE_TAG=latest\n</code></pre> <p>Workflow File</p> <p>Note the different workflow file: <code>photogrammetry-workflow-stepbased.yaml</code> instead of <code>photogrammetry-workflow.yaml</code></p> <p>Database parameters (not currently functional): <pre><code>-p DB_PASSWORD=&lt;password&gt; \\\n-p DB_HOST=&lt;vm_ip_address&gt; \\\n-p DB_NAME=&lt;db_name&gt; \\\n-p DB_USER=&lt;user_name&gt;\n</code></pre></p>"},{"location":"usage/stepbased-workflow/#workflow-parameters","title":"Workflow parameters","text":"Parameter Description <code>CONFIG_LIST</code> Absolute path to text file listing metashape config files. Each line should be a config filename (resolved relative to the config list's directory) or an absolute path. Lines starting with <code>#</code> are comments. Example: <code>/data/argo-input/configs/config-list.txt</code> <code>TEMP_WORKING_DIR</code> Absolute path for temporary workflow files (both photogrammetry and postprocessing). Workflow creates <code>{workflow-name}/{iteration-id}/</code> subdirectories automatically for each mission. Iteration directories are automatically deleted after successful postprocessing to free disk space. Example: <code>/data/argo-output/temp-runs/gillan_june27</code> <code>PHOTOGRAMMETRY_CONFIG_ID</code> Two-digit configuration ID (e.g., <code>01</code>, <code>02</code>) used to organize outputs into <code>photogrammetry_NN</code> subdirectories in S3 for both raw and postprocessed products. If not specified or set to <code>NONE</code>, both raw and postprocessed products are stored without the <code>photogrammetry_NN</code> subfolder. <code>S3_BUCKET_INTERNAL</code> S3 bucket for internal/intermediate outputs where raw Metashape products (orthomosaics, point clouds, DEMs) are uploaded (typically <code>ofo-internal</code>). <code>S3_PHOTOGRAMMETRY_DIR</code> S3 directory name for raw Metashape outputs. When <code>PHOTOGRAMMETRY_CONFIG_ID</code> is set, products upload to <code>{S3_BUCKET_INTERNAL}/{S3_PHOTOGRAMMETRY_DIR}/photogrammetry_{PHOTOGRAMMETRY_CONFIG_ID}/</code>. When <code>PHOTOGRAMMETRY_CONFIG_ID</code> is not set, products go to <code>{bucket}/{S3_PHOTOGRAMMETRY_DIR}/</code>. Example: <code>photogrammetry-outputs</code> <code>S3_BUCKET_PUBLIC</code> S3 bucket for public/final outputs (postprocessed, clipped products ready for distribution) and where boundary files are stored (typically <code>ofo-public</code>). <code>S3_POSTPROCESSED_DIR</code> S3 directory name for postprocessed outputs. When <code>PHOTOGRAMMETRY_CONFIG_ID</code> is set, products are organized as <code>{S3_POSTPROCESSED_DIR}/{mission_name}/photogrammetry_{PHOTOGRAMMETRY_CONFIG_ID}/</code>. When not set, products go to <code>{S3_POSTPROCESSED_DIR}/{mission_name}/</code>. Example: <code>drone/missions_03</code> <code>S3_BOUNDARY_DIR</code> Parent directory in <code>S3_BUCKET_PUBLIC</code> where mission boundary polygons reside (used to clip imagery). The structure beneath this directory is assumed to be: <code>&lt;S3_BOUNDARY_DIR&gt;/&lt;mission_name&gt;/metadata-mission/&lt;mission_name&gt;_mission-metadata.gpkg</code>. Example: <code>drone/missions_03</code> <code>OFO_ARGO_IMAGES_TAG</code> Docker image tag for OFO Argo containers (postprocessing and argo-workflow-utils) (default: <code>latest</code>). Use a specific branch name or tag to test development versions (e.g., <code>dy-manila</code>) <code>AUTOMATE_METASHAPE_IMAGE_TAG</code> Docker image tag for the automate-metashape container (default: <code>latest</code>). Use a specific branch name or tag to test development versions <code>DB_*</code> Database parameters for logging Argo status (not currently functional; credentials in OFO credentials document) <p>Secrets configuration:</p> <ul> <li>S3 credentials: S3 access credentials, provider type, and endpoint URL are configured via the <code>s3-credentials</code> Kubernetes secret</li> <li>Agisoft license: Metashape floating license server address is configured via the   <code>agisoft-license</code> Kubernetes secret</li> </ul> <p>These secrets should have been created (within the <code>argo</code> namespace) during cluster creation.</p>"},{"location":"usage/stepbased-workflow/#monitor-the-workflow","title":"Monitor the workflow","text":""},{"location":"usage/stepbased-workflow/#using-the-argo-ui","title":"Using the Argo UI","text":"<p>The Argo UI is great for troubleshooting and checking individual step progress. Access it at argo.focal-lab.org, using the credentials from Vaultwarden under the record \"Argo UI token\".</p>"},{"location":"usage/stepbased-workflow/#navigating-the-argo-ui","title":"Navigating the Argo UI","text":"<p>The Workflows tab on the left side menu shows all running workflows. Click a workflow to see a detailed DAG (directed acyclic graph) showing:</p> <ul> <li>Preprocessing task: The <code>determine-projects</code> step that reads config files</li> <li>Per-mission columns: Each mission shows as a separate column with all its processing steps</li> <li>Individual step status: Each of the 10+ steps shown with color-coded status</li> </ul> <p>Step status colors:</p> <ul> <li>\ud83d\udfe2 Green (Succeeded): Step completed successfully</li> <li>\ud83d\udd35 Blue (Running): Step currently executing</li> <li>\u26aa Gray (Skipped): Step was disabled in config or conditionally skipped</li> <li>\ud83d\udd34 Red (Failed): Step encountered an error</li> <li>\ud83d\udfe1 Yellow (Pending): Step waiting for dependencies</li> </ul> <p>Click on a specific step to see detailed information including:</p> <ul> <li>Which VM/node it's running on (CPU vs GPU node)</li> <li>Duration of the step</li> <li>Real-time logs</li> <li>Resource usage</li> <li>Input/output parameters</li> </ul> <p>Viewing Step Logs</p> <p>To view logs for a specific step:</p> <ol> <li>Click the workflow in Argo UI</li> <li>Click on the individual step node (e.g., <code>match-photos-gpu</code>, <code>build-depth-maps</code>)</li> <li>Click the \"Logs\" tab</li> <li>Logs will stream in real-time if the step is running</li> </ol>"},{"location":"usage/stepbased-workflow/#multi-mission-miew","title":"Multi-mission miew","text":"<p>When processing multiple missions, the Argo UI shows all missions side-by-side. This makes it easy to:</p> <ul> <li>See which missions are at which step</li> <li>Identify if one mission is failing while others succeed</li> <li>Compare processing times across missions</li> <li>Monitor overall workflow progress</li> </ul>"},{"location":"usage/stepbased-workflow/#understanding-step-names","title":"Understanding step names","text":"<p>Task names in the Argo UI follow the pattern <code>process-projects-N.&lt;step-name&gt;</code>:</p> <ul> <li><code>process-projects-0.setup</code> - Setup step for first mission (index 0)</li> <li><code>process-projects-0.match-photos-gpu</code> - Match photos on GPU for first mission</li> <li><code>process-projects-1.build-depth-maps</code> - Build depth maps for second mission (index 1)</li> </ul> <p>Finding Your Mission</p> <p>To identify which mission corresponds to which index:</p> <ol> <li>Check the <code>determine-projects</code> step logs to see the order of missions in the JSON output</li> <li>Click on any task (e.g., <code>process-projects-0.setup</code>) and view the parameters to see the <code>project-name</code> value</li> <li>The project name appears in all file paths, logs, and processing outputs</li> </ol> <p>GPU-capable steps show either <code>-gpu</code> or <code>-cpu</code> suffix depending on config.</p>"},{"location":"usage/stepbased-workflow/#using-the-cli","title":"Using the CLI","text":"<p>View workflow status from the command line:</p> <pre><code># Watch overall workflow progress\nargo watch &lt;workflow-name&gt;\n\n# List all workflows\nargo list\n\n# Get logs for preprocessing step\nargo logs &lt;workflow-name&gt; -c determine-projects\n\n# Get logs for a specific mission's step\n# Format: process-projects-&lt;N&gt;.&lt;step-name&gt;\nargo logs &lt;workflow-name&gt; -c process-projects-0.setup\nargo logs &lt;workflow-name&gt; -c process-projects-0.match-photos-gpu\nargo logs &lt;workflow-name&gt; -c process-projects-1.build-depth-maps\n\n# Follow logs in real-time\nargo logs &lt;workflow-name&gt; -c process-projects-0.setup -f\n</code></pre>"},{"location":"usage/stepbased-workflow/#workflow-outputs","title":"Workflow outputs","text":"<p>The final outputs will be written to <code>S3:ofo-public</code> in the following directory structure:</p> <pre><code>/S3:ofo-public/\n\u251c\u2500\u2500 &lt;OUTPUT_DIRECTORY&gt;/\n    \u251c\u2500\u2500 dataset1/\n         \u251c\u2500\u2500 images/\n         \u251c\u2500\u2500 metadata-images/\n         \u251c\u2500\u2500 metadata-mission/\n            \u2514\u2500\u2500 dataset1_mission-metadata.gpkg\n         \u251c\u2500\u2500photogrammetry_01/\n            \u251c\u2500\u2500 full/\n               \u251c\u2500\u2500 dataset1_cameras.xml\n               \u251c\u2500\u2500 dataset1_chm-ptcloud.tif\n               \u251c\u2500\u2500 dataset1_dsm-ptcloud.tif\n               \u251c\u2500\u2500 dataset1_dtm-ptcloud.tif\n               \u251c\u2500\u2500 dataset1_log.txt\n               \u251c\u2500\u2500 dataset1_ortho-dtm-ptcloud.tif\n               \u251c\u2500\u2500 dataset1_points.copc.laz\n               \u2514\u2500\u2500 dataset1_report.pdf\n            \u251c\u2500\u2500 thumbnails/\n               \u251c\u2500\u2500 dataset1_chm-ptcloud.png\n               \u251c\u2500\u2500 dataset1_dsm-ptcloud.png\n               \u251c\u2500\u2500 dataset1_dtm-ptcloud.png\n               \u2514\u2500\u2500 dataset1-ortho-dtm-ptcloud.png\n         \u251c\u2500\u2500photogrammetry_02/\n            \u251c\u2500\u2500 full/\n               \u251c\u2500\u2500 dataset1_cameras.xml\n               \u251c\u2500\u2500 dataset1_chm-ptcloud.tif\n               \u251c\u2500\u2500 dataset1_dsm-ptcloud.tif\n               \u251c\u2500\u2500 dataset1_dtm-ptcloud.tif\n               \u251c\u2500\u2500 dataset1_log.txt\n               \u251c\u2500\u2500 dataset1_ortho-dtm-ptcloud.tif\n               \u251c\u2500\u2500 dataset1_points.copc.laz\n               \u2514\u2500\u2500 dataset1_report.pdf\n            \u251c\u2500\u2500 thumbnails/\n               \u251c\u2500\u2500 dataset1_chm-ptcloud.png\n               \u251c\u2500\u2500 dataset1_dsm-ptcloud.png\n               \u251c\u2500\u2500 dataset1_dtm-ptcloud.png\n               \u2514\u2500\u2500 dataset1-ortho-dtm-ptcloud.png\n    \u251c\u2500\u2500 dataset2/\n</code></pre> <p>This directory structure should already exist prior to running the Argo workflow.</p>"}]}