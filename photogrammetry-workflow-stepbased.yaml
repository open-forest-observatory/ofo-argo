apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: automate-metashape-workflow-
spec:
  serviceAccountName: argo
  entrypoint: main

  # Set imagePullPolicy for all containers in the workflow
  podSpecPatch: |
    containers:
      - name: main
        imagePullPolicy: Always

  # Pod scheduling:
  # - CPU nodes are labeled (workload-type=cpu) via NodeFeatureRule based on nodegroup name
  # - CPU pods: Use nodeSelector to target CPU nodes explicitly
  # - GPU pods: GPU resource requests ensure scheduling only on nodes with GPU resources
  # - podAffinity (prefer nodes with running pods) inherited from workflow-controller-configmap

  # A list of input parameters available to the workflow at runtime.
  # These parameters can be referenced throughout the workflow templates using {{workflow.parameters.<name>}}
  arguments:
    parameters:
      - name: CONFIG_LIST
        value: "/data/argo-input/config-lists/config_list.txt"
      - name: TEMP_WORKING_DIR
        value: "/data/argo-output/temp-dir"
      - name: S3_PHOTOGRAMMETRY_DIR
        value: "default-run"
      - name: PHOTOGRAMMETRY_CONFIG_ID
        value: "NONE"
      # S3 bucket for internal/intermediate outputs (raw Metashape products like orthomosaics, point clouds, DEMs)
      - name: S3_BUCKET_INTERNAL
        value: ""
      # S3 bucket for public/final outputs (postprocessed, clipped products ready for distribution)
      - name: S3_BUCKET_PUBLIC
        value: "ofo-public"
      - name: S3_POSTPROCESSED_DIR
        value: ""
      # Parent directory in S3_BUCKET_PUBLIC where mission boundary polygons reside.
      # Expected structure: <S3_BOUNDARY_DIR>/<mission_name>/metadata-mission/<mission_name>_mission-metadata.gpkg
      - name: S3_BOUNDARY_DIR
        value: ""
      # - name: OUTPUT_MAX_DIM
      #   value: "800"
      # Docker image tag for OFO Argo containers (postprocessing, argo-workflow-utils)
      - name: OFO_ARGO_IMAGES_TAG
        value: "latest"
      # Docker image tag for automate-metashape container
      - name: AUTOMATE_METASHAPE_IMAGE_TAG
        value: "latest"
      # License retry settings for Metashape license acquisition
      - name: LICENSE_RETRY_INTERVAL
        value: "300"
      - name: LICENSE_MAX_RETRIES
        value: "0"

  # Defining where to read raw drone imagery data and write out imagery products to `/ofo-share`
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: ceph-share-rw-pvc2

  templates:
    # the 'main' template defines the order of high-level steps to be completed in the workflow.
    # the 'process-projects' step has a looping directive (withParam) which goes through each project and processes it.
    - name: main
      # Max concurrent projects to process in parallel. Edit this value directly to change.
      # (Argo doesn't support parameter substitution for integer fields like parallelism)
      parallelism: 18
      steps:
        - - name: compute-photogrammetry-config-subfolder
            template: compute-photogrammetry-config-subfolder
          - name: determine-projects
            template: determine-projects
        # withParam now receives only minimal references (index + project_name) to avoid
        # Argo's parameter size limit. Each project loads its full config from the shared file.
        - - name: process-projects
            template: process-project-workflow
            arguments:
              parameters:
                # Minimal reference - full config loaded from file at runtime
                - name: project-index
                  value: "{{item.index}}"
                - name: project-name
                  value: "{{item.project_name}}"
                # Path to shared configs file
                - name: configs-file
                  value: "{{steps.determine-projects.outputs.parameters.configs-file}}"
                - name: photogrammetry-config-subfolder
                  value: "{{steps.compute-photogrammetry-config-subfolder.outputs.result}}"
            withParam: "{{steps.determine-projects.outputs.result}}"

    ## Here we define what the main steps actually do

    # Compute the photogrammetry config subfolder name based on PHOTOGRAMMETRY_CONFIG_ID
    # If PHOTOGRAMMETRY_CONFIG_ID is non-empty and not "NONE", returns "photogrammetry_<ID>", otherwise returns empty string
    - name: compute-photogrammetry-config-subfolder
      # Ensure CPU pods only schedule on CPU nodes (labeled by NFD based on nodegroup name)
      nodeSelector:
        feature.node.kubernetes.io/workload-type: cpu
      script:
        image: python:3.9
        command: ["python3"]
        source: |
          import sys
          config_id = "{{workflow.parameters.PHOTOGRAMMETRY_CONFIG_ID}}"
          if config_id and config_id != "NONE":
              print(f"photogrammetry_{config_id}", end='')
          else:
              print("", end='')

    # Preprocess step: Read config files and generate mission parameters with enabled flags
    # This reads each mission's config file, extracts the project name, and determines which
    # processing steps are enabled. It also determines GPU vs CPU node scheduling for GPU-capable steps.
    # Uses containerized preprocessing script from argo-workflow-utils.
    #
    # To avoid Argo's parameter size limit (default 256KB), this step writes full configs to a
    # shared file and outputs only minimal references (index + project_name) to stdout.
    # Each project iteration loads its full config from the shared file at runtime.
    - name: determine-projects
      # Ensure CPU pods only schedule on CPU nodes (labeled by NFD based on nodegroup name)
      nodeSelector:
        feature.node.kubernetes.io/workload-type: cpu
      container:
        image: ghcr.io/open-forest-observatory/argo-workflow-utils:{{workflow.parameters.OFO_ARGO_IMAGES_TAG}}
        volumeMounts:
          - name: data
            mountPath: /data
        command: ["python3"]
        args:
          - "/app/determine_datasets.py"
          - "{{workflow.parameters.CONFIG_LIST}}"
          # Write full configs to shared file (artifact mode) to avoid withParam size limits
          - "{{workflow.parameters.TEMP_WORKING_DIR}}/{{workflow.name}}/project-configs.json"
      outputs:
        parameters:
          - name: configs-file
            value: "{{workflow.parameters.TEMP_WORKING_DIR}}/{{workflow.name}}/project-configs.json"

    # Load a single project's configuration from the shared configs file
    # This avoids passing large JSON through Argo parameters (which has size limits)
    - name: load-project-config
      inputs:
        parameters:
          - name: configs-file
          - name: project-index
      nodeSelector:
        feature.node.kubernetes.io/workload-type: cpu
      script:
        image: python:3.9-slim
        volumeMounts:
          - name: data
            mountPath: /data
        command: ["python3"]
        source: |
          import json
          with open("{{inputs.parameters.configs-file}}", "r") as f:
              configs = json.load(f)
          project_config = configs[{{inputs.parameters.project-index}}]
          print(json.dumps(project_config))

    # Multi-step photogrammetry workflow with conditional GPU/CPU execution
    # Config is loaded from shared file at runtime to avoid Argo parameter size limits
    - name: process-project-workflow
      inputs:
        parameters:
          - name: project-index
          - name: project-name
          - name: configs-file
          - name: photogrammetry-config-subfolder
      dag:
        tasks:
          # First task: Load full project config from shared file
          # All subsequent tasks depend on this and use expressions to extract config values
          - name: load-config
            template: load-project-config
            arguments:
              parameters:
                - name: configs-file
                  value: "{{inputs.parameters.configs-file}}"
                - name: project-index
                  value: "{{inputs.parameters.project-index}}"

          # Step 0a: Download imagery from S3 (conditional, runs after config loaded)
          - name: download-imagery
            depends: "load-config.Succeeded"
            template: download-imagery
            when: "{{=sprig.fromJson(tasks['load-config'].outputs.result).imagery_download_enabled == 'true'}}"
            arguments:
              parameters:
                - name: imagery-zip-downloads
                  value: "{{=toJson(sprig.fromJson(tasks['load-config'].outputs.result).imagery_zip_downloads)}}"
                - name: iteration-id
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).iteration_id}}"

          # Step 0b: Transform config to resolve __DOWNLOADED__ paths (conditional, depends on download)
          - name: transform-config
            template: transform-config
            when: "{{=sprig.fromJson(tasks['load-config'].outputs.result).imagery_download_enabled == 'true'}}"
            depends: "download-imagery.Succeeded"
            arguments:
              parameters:
                - name: config-file
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).config}}"
                - name: iteration-id
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).iteration_id}}"
                - name: project-name
                  value: "{{inputs.parameters.project-name}}"

          # Step 1: Setup (always runs, CPU only)
          # Depends on transform-config if downloads enabled, otherwise runs after load-config
          - name: setup
            template: metashape-cpu-step
            depends: "load-config.Succeeded && (transform-config.Succeeded || transform-config.Skipped)"
            arguments:
              parameters:
                - name: project-name
                  value: "{{inputs.parameters.project-name}}"
                - name: config-file
                  # Use transformed config if downloads enabled, otherwise use original config
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).imagery_download_enabled == 'true' ? tasks['transform-config'].outputs.parameters['transformed-config-path'] : sprig.fromJson(tasks['load-config'].outputs.result).config}}"
                - name: step
                  value: "setup"
                - name: iteration-id
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).iteration_id}}"
                - name: cpu-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).setup_cpu_request}}"
                - name: memory-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).setup_memory_request}}"

          # Step 2: Match Photos (conditional, GPU or CPU based on config)
          - name: match-photos-gpu
            depends: "setup.Succeeded"
            when: "{{=sprig.fromJson(tasks['load-config'].outputs.result).match_photos_enabled == true && sprig.fromJson(tasks['load-config'].outputs.result).match_photos_use_gpu == true}}"
            template: metashape-gpu-step
            arguments:
              parameters:
                - name: project-name
                  value: "{{inputs.parameters.project-name}}"
                - name: config-file
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).imagery_download_enabled == 'true' ? tasks['transform-config'].outputs.parameters['transformed-config-path'] : sprig.fromJson(tasks['load-config'].outputs.result).config}}"
                - name: step
                  value: "match_photos"
                - name: iteration-id
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).iteration_id}}"
                - name: gpu-resource
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).match_photos_gpu_resource}}"
                - name: gpu-count
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).match_photos_gpu_count}}"
                - name: cpu-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).match_photos_cpu_request}}"
                - name: memory-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).match_photos_memory_request}}"

          - name: match-photos-cpu
            depends: "setup.Succeeded"
            when: "{{=sprig.fromJson(tasks['load-config'].outputs.result).match_photos_enabled == true && sprig.fromJson(tasks['load-config'].outputs.result).match_photos_use_gpu == false}}"
            template: metashape-cpu-step
            arguments:
              parameters:
                - name: project-name
                  value: "{{inputs.parameters.project-name}}"
                - name: config-file
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).imagery_download_enabled == 'true' ? tasks['transform-config'].outputs.parameters['transformed-config-path'] : sprig.fromJson(tasks['load-config'].outputs.result).config}}"
                - name: step
                  value: "match_photos"
                - name: iteration-id
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).iteration_id}}"
                - name: cpu-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).match_photos_cpu_request}}"
                - name: memory-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).match_photos_memory_request}}"

          # Step 3: Align Cameras (conditional, CPU only)
          - name: align-cameras
            depends: "match-photos-gpu.Succeeded || match-photos-gpu.Skipped || match-photos-cpu.Succeeded || match-photos-cpu.Skipped"
            when: "{{=sprig.fromJson(tasks['load-config'].outputs.result).align_cameras_enabled == true}}"
            template: metashape-cpu-step
            arguments:
              parameters:
                - name: project-name
                  value: "{{inputs.parameters.project-name}}"
                - name: config-file
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).imagery_download_enabled == 'true' ? tasks['transform-config'].outputs.parameters['transformed-config-path'] : sprig.fromJson(tasks['load-config'].outputs.result).config}}"
                - name: step
                  value: "align_cameras"
                - name: iteration-id
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).iteration_id}}"
                - name: cpu-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).align_cameras_cpu_request}}"
                - name: memory-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).align_cameras_memory_request}}"

          # Step 4: Build Depth Maps (conditional, GPU only)
          - name: build-depth-maps
            depends: "align-cameras.Succeeded || align-cameras.Skipped"
            when: "{{=sprig.fromJson(tasks['load-config'].outputs.result).build_depth_maps_enabled == true}}"
            template: metashape-gpu-step
            arguments:
              parameters:
                - name: project-name
                  value: "{{inputs.parameters.project-name}}"
                - name: config-file
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).imagery_download_enabled == 'true' ? tasks['transform-config'].outputs.parameters['transformed-config-path'] : sprig.fromJson(tasks['load-config'].outputs.result).config}}"
                - name: step
                  value: "build_depth_maps"
                - name: iteration-id
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).iteration_id}}"
                - name: gpu-resource
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).build_depth_maps_gpu_resource}}"
                - name: gpu-count
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).build_depth_maps_gpu_count}}"
                - name: cpu-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).build_depth_maps_cpu_request}}"
                - name: memory-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).build_depth_maps_memory_request}}"

          # Step 5: Build Point Cloud (conditional, CPU only)
          - name: build-point-cloud
            depends: "build-depth-maps.Succeeded || build-depth-maps.Skipped"
            when: "{{=sprig.fromJson(tasks['load-config'].outputs.result).build_point_cloud_enabled == true}}"
            template: metashape-cpu-step
            arguments:
              parameters:
                - name: project-name
                  value: "{{inputs.parameters.project-name}}"
                - name: config-file
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).imagery_download_enabled == 'true' ? tasks['transform-config'].outputs.parameters['transformed-config-path'] : sprig.fromJson(tasks['load-config'].outputs.result).config}}"
                - name: step
                  value: "build_point_cloud"
                - name: iteration-id
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).iteration_id}}"
                - name: cpu-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).build_point_cloud_cpu_request}}"
                - name: memory-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).build_point_cloud_memory_request}}"

          # Step 6: Build Mesh (conditional, GPU or CPU based on config)
          - name: build-mesh-gpu
            depends: "build-point-cloud.Succeeded || build-point-cloud.Skipped"
            when: "{{=sprig.fromJson(tasks['load-config'].outputs.result).build_mesh_enabled == true && sprig.fromJson(tasks['load-config'].outputs.result).build_mesh_use_gpu == true}}"
            template: metashape-gpu-step
            arguments:
              parameters:
                - name: project-name
                  value: "{{inputs.parameters.project-name}}"
                - name: config-file
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).imagery_download_enabled == 'true' ? tasks['transform-config'].outputs.parameters['transformed-config-path'] : sprig.fromJson(tasks['load-config'].outputs.result).config}}"
                - name: step
                  value: "build_mesh"
                - name: iteration-id
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).iteration_id}}"
                - name: gpu-resource
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).build_mesh_gpu_resource}}"
                - name: gpu-count
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).build_mesh_gpu_count}}"
                - name: cpu-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).build_mesh_cpu_request}}"
                - name: memory-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).build_mesh_memory_request}}"

          - name: build-mesh-cpu
            depends: "build-point-cloud.Succeeded || build-point-cloud.Skipped"
            when: "{{=sprig.fromJson(tasks['load-config'].outputs.result).build_mesh_enabled == true && sprig.fromJson(tasks['load-config'].outputs.result).build_mesh_use_gpu == false}}"
            template: metashape-cpu-step
            arguments:
              parameters:
                - name: project-name
                  value: "{{inputs.parameters.project-name}}"
                - name: config-file
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).imagery_download_enabled == 'true' ? tasks['transform-config'].outputs.parameters['transformed-config-path'] : sprig.fromJson(tasks['load-config'].outputs.result).config}}"
                - name: step
                  value: "build_mesh"
                - name: iteration-id
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).iteration_id}}"
                - name: cpu-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).build_mesh_cpu_request}}"
                - name: memory-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).build_mesh_memory_request}}"

          # Step 7: Build DEM/Orthomosaic (conditional, CPU only)
          - name: build-dem-orthomosaic
            depends: "(build-point-cloud.Succeeded || build-point-cloud.Skipped) && (build-mesh-gpu.Succeeded || build-mesh-gpu.Skipped || build-mesh-cpu.Succeeded || build-mesh-cpu.Skipped)"
            when: "{{=sprig.fromJson(tasks['load-config'].outputs.result).build_dem_orthomosaic_enabled == true}}"
            template: metashape-cpu-step
            arguments:
              parameters:
                - name: project-name
                  value: "{{inputs.parameters.project-name}}"
                - name: config-file
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).imagery_download_enabled == 'true' ? tasks['transform-config'].outputs.parameters['transformed-config-path'] : sprig.fromJson(tasks['load-config'].outputs.result).config}}"
                - name: step
                  value: "build_dem_orthomosaic"
                - name: iteration-id
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).iteration_id}}"
                - name: cpu-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).build_dem_orthomosaic_cpu_request}}"
                - name: memory-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).build_dem_orthomosaic_memory_request}}"

          # Step 8: Match Photos Secondary (conditional, GPU or CPU based on config)
          - name: match-photos-secondary-gpu
            depends: "build-dem-orthomosaic.Succeeded || build-dem-orthomosaic.Skipped"
            when: "{{=sprig.fromJson(tasks['load-config'].outputs.result).match_photos_secondary_enabled == true && sprig.fromJson(tasks['load-config'].outputs.result).match_photos_secondary_use_gpu == true}}"
            template: metashape-gpu-step
            arguments:
              parameters:
                - name: project-name
                  value: "{{inputs.parameters.project-name}}"
                - name: config-file
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).imagery_download_enabled == 'true' ? tasks['transform-config'].outputs.parameters['transformed-config-path'] : sprig.fromJson(tasks['load-config'].outputs.result).config}}"
                - name: step
                  value: "match_photos_secondary"
                - name: iteration-id
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).iteration_id}}"
                - name: gpu-resource
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).match_photos_secondary_gpu_resource}}"
                - name: gpu-count
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).match_photos_secondary_gpu_count}}"
                - name: cpu-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).match_photos_secondary_cpu_request}}"
                - name: memory-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).match_photos_secondary_memory_request}}"

          - name: match-photos-secondary-cpu
            depends: "build-dem-orthomosaic.Succeeded || build-dem-orthomosaic.Skipped"
            when: "{{=sprig.fromJson(tasks['load-config'].outputs.result).match_photos_secondary_enabled == true && sprig.fromJson(tasks['load-config'].outputs.result).match_photos_secondary_use_gpu == false}}"
            template: metashape-cpu-step
            arguments:
              parameters:
                - name: project-name
                  value: "{{inputs.parameters.project-name}}"
                - name: config-file
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).imagery_download_enabled == 'true' ? tasks['transform-config'].outputs.parameters['transformed-config-path'] : sprig.fromJson(tasks['load-config'].outputs.result).config}}"
                - name: step
                  value: "match_photos_secondary"
                - name: iteration-id
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).iteration_id}}"
                - name: cpu-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).match_photos_secondary_cpu_request}}"
                - name: memory-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).match_photos_secondary_memory_request}}"

          # Step 9: Align Cameras Secondary (conditional, CPU only)
          - name: align-cameras-secondary
            depends: "match-photos-secondary-gpu.Succeeded || match-photos-secondary-gpu.Skipped || match-photos-secondary-cpu.Succeeded || match-photos-secondary-cpu.Skipped"
            when: "{{=sprig.fromJson(tasks['load-config'].outputs.result).align_cameras_secondary_enabled == true}}"
            template: metashape-cpu-step
            arguments:
              parameters:
                - name: project-name
                  value: "{{inputs.parameters.project-name}}"
                - name: config-file
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).imagery_download_enabled == 'true' ? tasks['transform-config'].outputs.parameters['transformed-config-path'] : sprig.fromJson(tasks['load-config'].outputs.result).config}}"
                - name: step
                  value: "align_cameras_secondary"
                - name: iteration-id
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).iteration_id}}"
                - name: cpu-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).align_cameras_secondary_cpu_request}}"
                - name: memory-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).align_cameras_secondary_memory_request}}"

          # Step 10: Finalize (always runs, CPU only)
          - name: finalize
            depends: "(build-dem-orthomosaic.Succeeded || build-dem-orthomosaic.Skipped) && (align-cameras-secondary.Succeeded || align-cameras-secondary.Skipped)"
            template: metashape-cpu-step
            arguments:
              parameters:
                - name: project-name
                  value: "{{inputs.parameters.project-name}}"
                - name: config-file
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).imagery_download_enabled == 'true' ? tasks['transform-config'].outputs.parameters['transformed-config-path'] : sprig.fromJson(tasks['load-config'].outputs.result).config}}"
                - name: step
                  value: "finalize"
                - name: iteration-id
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).iteration_id}}"
                - name: cpu-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).finalize_cpu_request}}"
                - name: memory-request
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).finalize_memory_request}}"

          # Upload task (runs after finalize completes)
          - name: rclone-upload-task
            depends: "finalize.Succeeded"
            template: rclone-upload-template
            arguments:
              parameters:
                - name: project-name
                  value: "{{inputs.parameters.project-name}}"
                - name: iteration-id
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).iteration_id}}"
                - name: photogrammetry-config-subfolder
                  value: "{{inputs.parameters.photogrammetry-config-subfolder}}"

          # Post-processing task (runs after upload completes)
          - name: postprocessing-task
            depends: "rclone-upload-task.Succeeded"
            template: postprocessing-template
            arguments:
              parameters:
                - name: project-name
                  value: "{{inputs.parameters.project-name}}"
                - name: iteration-id
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).iteration_id}}"
                - name: photogrammetry-config-subfolder
                  value: "{{inputs.parameters.photogrammetry-config-subfolder}}"

          # Cleanup task (runs after postprocessing completes)
          - name: cleanup-iteration
            depends: "postprocessing-task.Succeeded"
            template: cleanup-iteration-template
            arguments:
              parameters:
                - name: iteration-id
                  value: "{{=sprig.fromJson(tasks['load-config'].outputs.result).iteration_id}}"

    ## Here we define what each step does in 'process-project-workflow' step

    # CPU step template for Metashape processing
    - name: metashape-cpu-step
      inputs:
        parameters:
          - name: project-name
          - name: config-file
          - name: step
          - name: iteration-id
          - name: cpu-request
            default: "18"  # Default CPU cores for CPU steps
          - name: memory-request
            default: "100Gi"  # Default memory for CPU steps
      # Ensure CPU pods only schedule on CPU nodes (labeled by NFD based on nodegroup name)
      nodeSelector:
        feature.node.kubernetes.io/workload-type: cpu
      podSpecPatch: '{"containers":[{"name":"main","resources":{"requests":{"cpu":"{{inputs.parameters.cpu-request}}","memory":"{{inputs.parameters.memory-request}}"}}}]}'
      script:
        image: ghcr.io/open-forest-observatory/automate-metashape:{{workflow.parameters.AUTOMATE_METASHAPE_IMAGE_TAG}}
        volumeMounts:
          - name: data
            mountPath: /data
        command: ["/bin/sh"]
        source: |
          set -e

          # For setup step, clean up any leftover products from a previous incomplete run
          # Only remove project/ and output/ directories, preserve downloaded-raw-imagery/ which was added prior to this
          if [ "{{inputs.parameters.step}}" = "setup" ]; then
            PHOTOGRAMMETRY_DIR="{{workflow.parameters.TEMP_WORKING_DIR}}/{{workflow.name}}/{{inputs.parameters.iteration-id}}/photogrammetry"
            for subdir in project output; do
              TARGET_DIR="$PHOTOGRAMMETRY_DIR/$subdir"
              if [ -d "$TARGET_DIR" ]; then
                echo "[cleanup] Removing leftover data from previous run: $TARGET_DIR"
                rm -rf "$TARGET_DIR"
                echo "[cleanup] Successfully removed: $TARGET_DIR"
              fi
            done
            echo "[cleanup] Cleanup complete (preserved downloaded-raw-imagery if present)"
          fi

          # Run the Metashape step with license retry wrapper
          python3 /app/python/license_retry_wrapper.py \
            --config-file "{{inputs.parameters.config-file}}" \
            --project-path "{{workflow.parameters.TEMP_WORKING_DIR}}/{{workflow.name}}/{{inputs.parameters.iteration-id}}/photogrammetry/project" \
            --output-path "{{workflow.parameters.TEMP_WORKING_DIR}}/{{workflow.name}}/{{inputs.parameters.iteration-id}}/photogrammetry/output" \
            --project-name "{{inputs.parameters.project-name}}" \
            --step "{{inputs.parameters.step}}"
        env:
          - name: AGISOFT_FLS
            valueFrom:
              secretKeyRef:
                name: agisoft-license
                key: license_server
          - name: LICENSE_RETRY_INTERVAL
            value: "{{workflow.parameters.LICENSE_RETRY_INTERVAL}}"
          - name: LICENSE_MAX_RETRIES
            value: "{{workflow.parameters.LICENSE_MAX_RETRIES}}"

    # GPU step template for Metashape processing
    # Supports dynamic GPU resource selection (full GPU or MIG partitions) and count
    - name: metashape-gpu-step
      inputs:
        parameters:
          - name: project-name
          - name: config-file
          - name: step
          - name: iteration-id
          - name: gpu-resource
            default: "nvidia.com/gpu"  # Full GPU. MIG options: nvidia.com/mig-1g.5gb, mig-2g.10gb, mig-3g.20gb
          - name: gpu-count
            default: "1"  # Number of GPU resources to request (e.g., 2 for two MIG slices)
          - name: cpu-request
            default: "4"  # Default CPU cores for GPU steps
          - name: memory-request
            default: "16Gi"  # Default memory for GPU steps
      # Dynamic GPU resource request via podSpecPatch (parameter substitution happens before YAML parse)
      # GPU resource request ensures pod only schedules on nodes with GPU resources available
      podSpecPatch: '{"containers":[{"name":"main","resources":{"requests":{"cpu":"{{inputs.parameters.cpu-request}}","memory":"{{inputs.parameters.memory-request}}","{{inputs.parameters.gpu-resource}}":"{{inputs.parameters.gpu-count}}"},"limits":{"{{inputs.parameters.gpu-resource}}":"{{inputs.parameters.gpu-count}}"}}}]}'
      script:
        image: ghcr.io/open-forest-observatory/automate-metashape:{{workflow.parameters.AUTOMATE_METASHAPE_IMAGE_TAG}}
        volumeMounts:
          - name: data
            mountPath: /data
        command: ["/bin/sh"]
        source: |
          set -e

          # Run the Metashape step with license retry wrapper
          python3 /app/python/license_retry_wrapper.py \
            --config-file "{{inputs.parameters.config-file}}" \
            --project-path "{{workflow.parameters.TEMP_WORKING_DIR}}/{{workflow.name}}/{{inputs.parameters.iteration-id}}/photogrammetry/project" \
            --output-path "{{workflow.parameters.TEMP_WORKING_DIR}}/{{workflow.name}}/{{inputs.parameters.iteration-id}}/photogrammetry/output" \
            --project-name "{{inputs.parameters.project-name}}" \
            --step "{{inputs.parameters.step}}"
        env:
          - name: AGISOFT_FLS
            valueFrom:
              secretKeyRef:
                name: agisoft-license
                key: license_server
          - name: LICENSE_RETRY_INTERVAL
            value: "{{workflow.parameters.LICENSE_RETRY_INTERVAL}}"
          - name: LICENSE_MAX_RETRIES
            value: "{{workflow.parameters.LICENSE_MAX_RETRIES}}"

    # --------- S3 IMAGERY DOWNLOAD ---------
    # Downloads and extracts zip files containing imagery from S3
    - name: download-imagery
      inputs:
        parameters:
          - name: imagery-zip-downloads
          - name: iteration-id
      # Ensure CPU pods only schedule on CPU nodes (labeled by NFD based on nodegroup name)
      nodeSelector:
        feature.node.kubernetes.io/workload-type: cpu
      container:
        image: ghcr.io/open-forest-observatory/argo-workflow-utils:{{workflow.parameters.OFO_ARGO_IMAGES_TAG}}
        volumeMounts:
          - name: data
            mountPath: /data
        command: ["python3", "/app/download_imagery.py"]
        env:
          - name: IMAGERY_ZIP_URLS
            value: "{{inputs.parameters.imagery-zip-downloads}}"
          - name: DOWNLOAD_DIR
            value: "{{workflow.parameters.TEMP_WORKING_DIR}}/{{workflow.name}}/{{inputs.parameters.iteration-id}}/photogrammetry/downloaded-raw-imagery"
          - name: S3_PROVIDER
            valueFrom:
              secretKeyRef:
                name: s3-credentials
                key: provider
          - name: S3_ENDPOINT
            valueFrom:
              secretKeyRef:
                name: s3-credentials
                key: endpoint
          - name: S3_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: s3-credentials
                key: access_key
          - name: S3_SECRET_KEY
            valueFrom:
              secretKeyRef:
                name: s3-credentials
                key: secret_key
        resources:
          requests:
            cpu: "2"
            memory: "2Gi"

    # --------- TRANSFORM CONFIG ---------
    # Replaces __DOWNLOADED__ prefix in config's photo_path with actual download path
    - name: transform-config
      inputs:
        parameters:
          - name: config-file
          - name: iteration-id
          - name: project-name
      # Ensure CPU pods only schedule on CPU nodes (labeled by NFD based on nodegroup name)
      nodeSelector:
        feature.node.kubernetes.io/workload-type: cpu
      container:
        image: ghcr.io/open-forest-observatory/argo-workflow-utils:{{workflow.parameters.OFO_ARGO_IMAGES_TAG}}
        volumeMounts:
          - name: data
            mountPath: /data
        command: ["python3", "/app/transform_config.py"]
        env:
          - name: CONFIG_FILE
            value: "{{inputs.parameters.config-file}}"
          - name: OUTPUT_CONFIG_FILE
            value: "{{workflow.parameters.TEMP_WORKING_DIR}}/{{workflow.name}}/{{inputs.parameters.iteration-id}}/photogrammetry/config-transformed.yml"
          - name: DOWNLOADED_IMAGERY_PATH
            value: "{{workflow.parameters.TEMP_WORKING_DIR}}/{{workflow.name}}/{{inputs.parameters.iteration-id}}/photogrammetry/downloaded-raw-imagery"
        resources:
          requests:
            cpu: "1"
            memory: "256Mi"
      outputs:
        parameters:
          - name: transformed-config-path
            value: "{{workflow.parameters.TEMP_WORKING_DIR}}/{{workflow.name}}/{{inputs.parameters.iteration-id}}/photogrammetry/config-transformed.yml"

    # --------- RCLONE UPLOAD (Docker image) ---------
    - name: rclone-upload-template
      inputs:
        parameters:
          - name: project-name
          - name: iteration-id
          - name: photogrammetry-config-subfolder
      # Ensure CPU pods only schedule on CPU nodes (labeled by NFD based on nodegroup name)
      nodeSelector:
        feature.node.kubernetes.io/workload-type: cpu
      container:
        image: rclone/rclone:latest
        volumeMounts:
          - name: data
            mountPath: /data
        command: ["/bin/sh", "-lc"]
        args:
          - |
            set -euo pipefail
            SRC="{{workflow.parameters.TEMP_WORKING_DIR}}/{{workflow.name}}/{{inputs.parameters.iteration-id}}/photogrammetry/output/"
            SRC_parent="{{workflow.parameters.TEMP_WORKING_DIR}}/{{workflow.name}}/{{inputs.parameters.iteration-id}}/photogrammetry"

            # Build S3 destination path with optional photogrammetry config subfolder
            if [ -n "{{inputs.parameters.photogrammetry-config-subfolder}}" ]; then
              DST="s3:{{workflow.parameters.S3_BUCKET_INTERNAL}}/{{workflow.parameters.S3_PHOTOGRAMMETRY_DIR}}/{{inputs.parameters.photogrammetry-config-subfolder}}"
            else
              DST="s3:{{workflow.parameters.S3_BUCKET_INTERNAL}}/{{workflow.parameters.S3_PHOTOGRAMMETRY_DIR}}"
            fi

            echo "[rclone] Uploading $SRC -> $DST"

            # Note that the rclone S3 credentials are provided via environment variables set in the
            # workflow spec below. This is an  alternative to using a rclone config file or passing
            # command-line flags to rclone.
            rclone copy "$SRC" "$DST" \
              --transfers 8 --checkers 8 --retries 5 --retries-sleep=15s \
              --s3-upload-cutoff 200Mi --s3-chunk-size 100Mi --s3-upload-concurrency 4 \
              --stats 15s --stats-log-level NOTICE

            # Check if upload was successful
            if [ $? -eq 0 ]; then
              echo "[rclone] Upload successful for {{inputs.parameters.project-name}}"

              # Clean up local files after successful upload
              echo "[cleanup] Removing local files after successful upload..."
              if [ -d "$SRC_parent" ]; then
                rm -rf "$SRC_parent"
                echo "[cleanup] Successfully removed: $SRC_parent"
              else
                echo "[cleanup] Source directory not found: $SRC_parent"
              fi
            else
              echo "[rclone] Upload failed for {{inputs.parameters.project-name}}"
              echo "[cleanup] Keeping local files due to upload failure"
              exit 1
            fi

            echo "[rclone] Upload and cleanup completed for {{inputs.parameters.project-name}}"

        resources:
          requests:
            cpu: 500m
            memory: "256Mi"
          limits:
            memory: "1Gi"
        env:
          # Env vars to tell rclone how to connect to S3
          - name: RCLONE_CONFIG_S3_TYPE
            value: "s3"
          - name: RCLONE_CONFIG_S3_PROVIDER
            valueFrom:
              secretKeyRef:
                name: s3-credentials
                key: provider
          - name: RCLONE_CONFIG_S3_ENDPOINT
            valueFrom:
              secretKeyRef:
                name: s3-credentials
                key: endpoint
          - name: RCLONE_CONFIG_S3_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: s3-credentials
                key: access_key
          - name: RCLONE_CONFIG_S3_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: s3-credentials
                key: secret_key

    #--------- POST-PROCESSING (Python Docker image) ---------
    - name: postprocessing-template
      inputs:
        parameters:
          - name: project-name
          - name: iteration-id
          - name: photogrammetry-config-subfolder
      # Ensure CPU pods only schedule on CPU nodes (labeled by NFD based on nodegroup name)
      nodeSelector:
        feature.node.kubernetes.io/workload-type: cpu
      container:
        image: ghcr.io/open-forest-observatory/photogrammetry-postprocessing:{{workflow.parameters.OFO_ARGO_IMAGES_TAG}}
        volumeMounts:
          - name: data
            mountPath: /data
        env:
          - name: S3_PROVIDER
            valueFrom:
              secretKeyRef:
                name: s3-credentials
                key: provider
          - name: S3_ENDPOINT
            valueFrom:
              secretKeyRef:
                name: s3-credentials
                key: endpoint
          - name: S3_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: s3-credentials
                key: access_key
          - name: S3_SECRET_KEY
            valueFrom:
              secretKeyRef:
                name: s3-credentials
                key: secret_key
          - name: S3_BUCKET_INTERNAL
            value: "{{workflow.parameters.S3_BUCKET_INTERNAL}}"
          - name: S3_PHOTOGRAMMETRY_DIR
            value: "{{workflow.parameters.S3_PHOTOGRAMMETRY_DIR}}"
          - name: PHOTOGRAMMETRY_CONFIG_SUBFOLDER
            value: "{{inputs.parameters.photogrammetry-config-subfolder}}"
          - name: PROJECT_NAME
            value: "{{inputs.parameters.project-name}}"
          - name: S3_BUCKET_INPUT_BOUNDARY
            value: "{{workflow.parameters.S3_BUCKET_PUBLIC}}"
          - name: INPUT_BOUNDARY_DIR
            value: "{{workflow.parameters.S3_BOUNDARY_DIR}}"
          - name: S3_BUCKET_PUBLIC
            value: "{{workflow.parameters.S3_BUCKET_PUBLIC}}"
          - name: S3_POSTPROCESSED_DIR
            value: "{{workflow.parameters.S3_POSTPROCESSED_DIR}}"
          #- name: OUTPUT_MAX_DIM
            #value: "{{workflow.parameters.OUTPUT_MAX_DIM}}"
          - name: TEMP_WORKING_DIR_POSTPROCESSING
            value: "{{workflow.parameters.TEMP_WORKING_DIR}}/{{workflow.name}}/{{inputs.parameters.iteration-id}}/postprocessing"
        resources:
          requests:
            cpu: "12"
            memory: "48Gi"
          limits:
            memory: "56Gi"

    #--------- CLEANUP ITERATION DIRECTORY ---------
    - name: cleanup-iteration-template
      inputs:
        parameters:
          - name: iteration-id
      nodeSelector:
        feature.node.kubernetes.io/workload-type: cpu
      container:
        image: alpine:latest
        volumeMounts:
          - name: data
            mountPath: /data
        command: ["/bin/sh", "-c"]
        args:
          - |
            ITERATION_DIR="{{workflow.parameters.TEMP_WORKING_DIR}}/{{workflow.name}}/{{inputs.parameters.iteration-id}}"
            echo "[cleanup] Removing iteration directory: $ITERATION_DIR"
            rm -rf "$ITERATION_DIR"
            echo "[cleanup] Done"
        resources:
          requests:
            cpu: "100m"
            memory: "64Mi"
