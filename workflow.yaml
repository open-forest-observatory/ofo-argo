apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: automate-metashape-workflow-
spec:
  serviceAccountName: argo
  entrypoint: main

  arguments:
    parameters:
      - name: CONFIG_LIST
        default: "config_list.txt"
      - name: RUN_FOLDER
        default: "default-run"
      - name: AGISOFT_FLS
        default: ""
      - name: DB_HOST
        default: ""
      - name: DB_NAME
        default: ""
      - name: DB_USER
        default: ""
      - name: DB_PASSWORD
        default: ""

  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: ofo-share-nfs-pvc
    - name: results
      persistentVolumeClaim:
        claimName: argo-output-nfs-pvc

  templates:

    - name: main
      steps:
        - - name: determine-datasets
            template: determine-datasets
        - - name: log-datasets-to-db
            template: log-datasets-to-db
            arguments:
              parameters:
                - name: datasets
                  value: "{{steps.determine-datasets.outputs.result}}"
        - - name: process-datasets
            template: process-dataset-workflow
            arguments:
              parameters:
                - name: dataset-name
                  value: "{{item}}"
            withParam: "{{steps.determine-datasets.outputs.result}}"

    - name: determine-datasets
      script:
        image: python:3.9
        volumeMounts:
          - name: data
            mountPath: /input
            subPath: "argo-input/{{workflow.parameters.CONFIG_LIST}}"
        command: ["python3"]
        source: |
          import json, sys
          with open("/input", "r") as f:
              datasets = [line.strip() for line in f if line.strip()]
              json.dump(datasets, sys.stdout)

    - name: log-datasets-to-db
      inputs:
        parameters:
          - name: datasets
      container:
        image: ghcr.io/open-forest-observatory/ofo-argo-utils:latest
        command: ["python", "/app/db_logger.py"]
        args:
          - "log-initial"
          - "--workflow-id"
          - "{{workflow.name}}"
          - "--datasets-json"
          - "{{inputs.parameters.datasets}}"
        env:
          - name: DB_HOST
            value: "{{workflow.parameters.DB_HOST}}"
          - name: DB_NAME
            value: "{{workflow.parameters.DB_NAME}}"
          - name: DB_USER
            value: "{{workflow.parameters.DB_USER}}"
          - name: DB_PASSWORD
            value: "{{workflow.parameters.DB_PASSWORD}}"

    - name: process-dataset-workflow
      inputs:
        parameters:
          - name: dataset-name
      steps:
        - - name: log-start
            template: log-dataset-start
            arguments:
              parameters:
                - name: dataset-name
                  value: "{{inputs.parameters.dataset-name}}"
        - - name: run-processing
            template: run-automate-metashape
            arguments:
              parameters:
                - name: dataset-name
                  value: "{{inputs.parameters.dataset-name}}"
            continueOn:
              failed: true
        - - name: determine-success
            template: evaluate-success
            arguments:
              parameters:
                - name: step-status
                  value: "{{steps.run-processing.status}}"
        - - name: log-completion
            template: log-dataset-completion
            arguments:
              parameters:
                - name: dataset-name
                  value: "{{inputs.parameters.dataset-name}}"
                - name: success
                  value: "{{steps.determine-success.outputs.result}}"

    - name: log-dataset-start
      inputs:
        parameters:
          - name: dataset-name
      container:
        image: ghcr.io/open-forest-observatory/ofo-argo-utils:latest
        command: ["python", "/app/db_logger.py"]
        args:
          - "log-start"
          - "--workflow-id"
          - "{{workflow.name}}"
          - "--dataset"
          - "{{inputs.parameters.dataset-name}}"
        env:
          - name: DB_HOST
            value: "{{workflow.parameters.DB_HOST}}"
          - name: DB_NAME
            value: "{{workflow.parameters.DB_NAME}}"
          - name: DB_USER
            value: "{{workflow.parameters.DB_USER}}"
          - name: DB_PASSWORD
            value: "{{workflow.parameters.DB_PASSWORD}}"

    - name: log-dataset-completion
      inputs:
        parameters:
          - name: dataset-name
          - name: success
      container:
        image: ghcr.io/open-forest-observatory/ofo-argo-utils:latest
        command: ["python", "/app/db_logger.py"]
        args:
          - "log-completion"
          - "--workflow-id"
          - "{{workflow.name}}"
          - "--dataset"
          - "{{inputs.parameters.dataset-name}}"
          - "--success"
          - "{{inputs.parameters.success}}"
        env:
          - name: DB_HOST
            value: "{{workflow.parameters.DB_HOST}}"
          - name: DB_NAME
            value: "{{workflow.parameters.DB_NAME}}"
          - name: DB_USER
            value: "{{workflow.parameters.DB_USER}}"
          - name: DB_PASSWORD
            value: "{{workflow.parameters.DB_PASSWORD}}"

    - name: evaluate-success
      inputs:
        parameters:
          - name: step-status
      script:
        image: python:3.9
        command: ["python3"]
        source: |
          import sys
          status = "{{inputs.parameters.step-status}}"
          sys.stdout.write("true" if status == "Succeeded" else "false")

    - name: run-automate-metashape
      inputs:
        parameters:
          - name: dataset-name
      metadata:
        labels:
          workload-type: metashape-job
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: workload-type
                    operator: In
                    values:
                      - metashape-job
              topologyKey: "kubernetes.io/hostname"
      container:
        image: ghcr.io/open-forest-observatory/automate-metashape
        volumeMounts:
          - name: data
            mountPath: /data
          - name: results
            mountPath: /results
        command: ["python3", "/app/python/metashape_workflow.py"]
        args:
          - "--config_file"
          - "/data/argo-input/configs/{{inputs.parameters.dataset-name}}.yml"
          - "--project-path"
          - "/results/{{workflow.parameters.RUN_FOLDER}}/{{inputs.parameters.dataset-name}}/project"
          - "--output-path"
          - "/results/{{workflow.parameters.RUN_FOLDER}}/{{inputs.parameters.dataset-name}}/output"
          - "--run-name"
          - "{{inputs.parameters.dataset-name}}"
        env:
          - name: AGISOFT_FLS
            value: "{{workflow.parameters.AGISOFT_FLS}}"

         
