apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: automate-metashape-workflow-
spec:
  serviceAccountName: argo
  entrypoint: main

  arguments:
    parameters:
      - name: CONFIG_FILE
        default: "config.yml"
      - name: RUN_FOLDER
        default: "default-run"
      - name: DATASET_LIST
        default: "datasets.txt"
      - name: AGISOFT_FLS
        default: ""
        
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: ofo-share-nfs-pvc
  - name: results
    persistentVolumeClaim:
      claimName: argo-output-nfs-pvc

  templates:
    - name: main
      steps:
        - - name: determine-datasets
            template: determine-datasets
        - - name: run-automate-metashape
            template: run-automate-metashape
            arguments:
              parameters:
                - name: dataset-names
                  value: "{{item}}"
                - name: config-file
                  value: "{{workflow.parameters.CONFIG_FILE}}"
            withParam: "{{steps.determine-datasets.outputs.result}}"
    
    # use subPath argument to mount dataset list argument (in /ofo-share/argo-output/datasets.txt for now)
    
    - name: determine-datasets
      script:
        image: python:3.9
        volumeMounts:
        - name: data
          mountPath: /data/argo-input
          subPath: "{{workflow.parameters.DATASET_LIST}}"
        command: ["python3"]
        source: |
          import json
          import sys
          file_path = "/data/argo-input"
          with open(file_path, "r") as f:
            json.dump([line.strip() for line in f], sys.stdout)

    - name: run-automate-metashape
      inputs:
        parameters:
          - name: dataset-names
          - name: config-file
      #the following 'metadata' and 'affinity' language is about ensuring one metashape project on one VM
      metadata:
        labels:
          workload-type: metashape-job  # arbitrary, but used for pod spreading
      
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: workload-type
                    operator: In
                    values:
                      - metashape-job
              topologyKey: "kubernetes.io/hostname"
      
      container:
        image: ghcr.io/open-forest-observatory/automate-metashape
        volumeMounts:
        - name: data
          mountPath: /data
        - name: results
          mountPath: /results
          #subPath: "{{workflow.parameters.RUN_FOLDER}}"
        command: ["python3", "/app/python/metashape_workflow.py"]
        args: 
          - "--config_file"
          - "/data/argo-input/{{inputs.parameters.config-file}}"
          - "--photo-path"
          - "/data/argo-input/{{inputs.parameters.dataset-names}}"
          - "--project-path"
          - "/results/{{workflow.parameters.RUN_FOLDER}}/{{inputs.parameters.dataset-names}}/project"
          - "--output-path"
          - "/results/{{workflow.parameters.RUN_FOLDER}}/{{inputs.parameters.dataset-names}}/output"
          - "--run-name"
          - "{{inputs.parameters.dataset-names}}"
        env:
          - name: AGISOFT_FLS
            value: "{{workflow.parameters.AGISOFT_FLS}}"

         
