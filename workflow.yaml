apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: automate-metashape-workflow-
spec:
  serviceAccountName: argo
  entrypoint: main

  # A list of input parameters available to the workflow at runtime. 
  # These parameters can be referenced throughout the workflow templates using {{workflow.parameters.<name>}}
  arguments:
    parameters:
      - name: CONFIG_LIST
        default: "config_list.txt"
      - name: RUN_FOLDER
        default: "default-run"
      - name: AGISOFT_FLS
        default: ""
      - name: DB_HOST
        default: ""
      - name: DB_NAME
        default: ""
      - name: DB_USER
        default: ""
      - name: DB_PASSWORD
        default: ""
      # S3 / rclone upload parameters
      - name: S3_BUCKET
        default: ""
      - name: S3_ENDPOINT
        default: ""
      - name: S3_PROVIDER
        default: ""      # e.g., AWS, Minio, Ceph

  # Defining where to read raw drone imagery data and write out imagery products to `/ofo-share`
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: ofo-share-nfs-pvc
    - name: results
      persistentVolumeClaim:
        claimName: argo-output-nfs-pvc

  templates:
    # the 'main' template defines the order of high-level steps to be completed in the workflow. 
    # the 'process-datasets' step has a looping directive (withParam) which goes through each dataset name and processes it.
    - name: main
      steps:
        - - name: determine-datasets
            template: determine-datasets
        - - name: log-datasets-to-db
            template: log-datasets-to-db
            arguments:
              parameters:
                - name: datasets
                  value: "{{steps.determine-datasets.outputs.result}}"
        - - name: process-datasets
            template: process-dataset-workflow
            arguments:
              parameters:
                - name: dataset-name
                  value: "{{item}}"
            withParam: "{{steps.determine-datasets.outputs.result}}"

## Here we define what the main steps actually do 

   # Use containerized python to parse through the list of datasets as specified from runtime parameter 'CONFIG_LIST'
    # outputs a json of dataset names that is passed to the next steps
    - name: determine-datasets
      script:
        image: python:3.9
        volumeMounts:
          - name: data
            mountPath: /input
            subPath: "argo-input/{{workflow.parameters.CONFIG_LIST}}"
        command: ["python3"]
        source: |
          import json, sys, os
          with open("/input", "r") as f:
              datasets = []
              for line in f:
                  name = line.strip()
                  if name:
                      base = os.path.splitext(name)[0]  # removes .yml, .yaml, etc.
                      datasets.append(base)

              # Output as JSON list
              json.dump(datasets, sys.stdout)
              
    # launches a docker container which contains our custom py script to log the dataset names into a postgis database
    # takes the json list of dataset names (from the 'determine datasets' step)
    - name: log-datasets-to-db
      inputs:
        parameters:
          - name: datasets
      container:
        image: ghcr.io/open-forest-observatory/ofo-argo-utils:latest
        command: ["python", "/app/db_logger.py"]
        args:
          - "log-initial"
          - "--workflow-id"
          - "{{workflow.name}}"
          - "--datasets-json"
          - "{{inputs.parameters.datasets}}"
        env:
          - name: DB_HOST
            value: "{{workflow.parameters.DB_HOST}}"
          - name: DB_NAME
            value: "{{workflow.parameters.DB_NAME}}"
          - name: DB_USER
            value: "{{workflow.parameters.DB_USER}}"
          - name: DB_PASSWORD
            value: "{{workflow.parameters.DB_PASSWORD}}"
            
    # High-level order of steps in the 'process-dataset-workflow' step. Each step will be defined later.
    - name: process-dataset-workflow
      inputs:
        parameters:
          - name: dataset-name
      dag:
        tasks:
          - name: log-start
            template: log-dataset-start
            arguments:
              parameters:
                - name: dataset-name
                  value: "{{inputs.parameters.dataset-name}}"
                  
          - name: run-processing
            template: run-automate-metashape
            arguments:
              parameters:
                - name: dataset-name
                  value: "{{inputs.parameters.dataset-name}}"
            continueOn:
              failed: true
              
          # As soon as processing finishes, kick off upload in parallel on any available node
          - name: rclone-upload
            dependencies: [run-processing]
            template: rclone-upload-to-s3
            arguments:
              parameters:
                - name: dataset-name
                  value: "{{inputs.parameters.dataset-name}}"
                  
          - name: log-completion-success
            dependencies: [run-processing]
            template: log-dataset-completion
            arguments:
              parameters:
                - name: dataset-name
                  value: "{{inputs.parameters.dataset-name}}"
                - name: success
                  value: "true"
            when: "{{tasks.run-processing.status}} == Succeeded"

          - name: log-completion-failure
            dependencies: [run-processing]
            template: log-dataset-completion
            arguments:
              parameters:
                - name: dataset-name
                  value: "{{inputs.parameters.dataset-name}}"
                - name: success
                  value: "false"
            when: "{{tasks.run-processing.status}} != Succeeded"
                  
    ## Here we define what each step does in 'process-dataset-workflow' step
    
    # use our custom containerized db_logger.py to log 'processing' in the postgis DB
    - name: log-dataset-start
      inputs:
        parameters:
          - name: dataset-name
      container:
        image: ghcr.io/open-forest-observatory/ofo-argo-utils:latest
        command: ["python", "/app/db_logger.py"]
        args:
          - "log-start"
          - "--workflow-id"
          - "{{workflow.name}}"
          - "--dataset"
          - "{{inputs.parameters.dataset-name}}"
        env:
          - name: DB_HOST
            value: "{{workflow.parameters.DB_HOST}}"
          - name: DB_NAME
            value: "{{workflow.parameters.DB_NAME}}"
          - name: DB_USER
            value: "{{workflow.parameters.DB_USER}}"
          - name: DB_PASSWORD
            value: "{{workflow.parameters.DB_PASSWORD}}"
            
    # use our custom containerized db_logger.py to log 'completed' or 'failed' in the postgis DB
    - name: log-dataset-completion
      inputs:
        parameters:
          - name: dataset-name
          - name: success
      container:
        image: ghcr.io/open-forest-observatory/ofo-argo-utils:latest
        command: ["python", "/app/db_logger.py"]
        args:
          - "log-completion"
          - "--workflow-id"
          - "{{workflow.name}}"
          - "--dataset"
          - "{{inputs.parameters.dataset-name}}"
          - "--success"
          - "{{inputs.parameters.success}}"
        env:
          - name: DB_HOST
            value: "{{workflow.parameters.DB_HOST}}"
          - name: DB_NAME
            value: "{{workflow.parameters.DB_NAME}}"
          - name: DB_USER
            value: "{{workflow.parameters.DB_USER}}"
          - name: DB_PASSWORD
            value: "{{workflow.parameters.DB_PASSWORD}}"
        
          
    # Defining how to process each dataset name
    - name: run-automate-metashape
      inputs:
        parameters:
          - name: dataset-name
      # the following 'metadata' and 'affinity' language is about ensuring one metashape project on one VM
      metadata:
        labels:
          workload-type: metashape-job
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: workload-type
                    operator: In
                    values:
                      - metashape-job
              topologyKey: "kubernetes.io/hostname"
      # Use docker automate-metashape to do photogrammetry
      container:
        image: ghcr.io/open-forest-observatory/automate-metashape
        volumeMounts:
          - name: data
            mountPath: /data
          - name: results
            mountPath: /results
        command: ["python3", "/app/python/metashape_workflow.py"]
        args:
          - "--config_file"
          - "/data/argo-input/configs/{{inputs.parameters.dataset-name}}.yml"
          - "--project-path"
          - "/results/{{workflow.parameters.RUN_FOLDER}}/{{inputs.parameters.dataset-name}}/project"
          - "--output-path"
          - "/results/{{workflow.parameters.RUN_FOLDER}}/{{inputs.parameters.dataset-name}}/output"
          - "--run-name"
          - "{{inputs.parameters.dataset-name}}"
        env:
          - name: AGISOFT_FLS
            value: "{{workflow.parameters.AGISOFT_FLS}}"
            
  # --------- RCLONE UPLOAD (Docker image) ---------
    - name: rclone-upload-to-s3
      inputs:
        parameters:
          - name: dataset-name
      container:
        image: rclone/rclone:latest
        volumeMounts:
          - name: results
            mountPath: /results
        command: ["/bin/sh", "-lc"]
        args:
          - |
            set -euo pipefail
            SRC="/results/{{workflow.parameters.RUN_FOLDER}}/{{inputs.parameters.dataset-name}}"
            DST=":s3:{{workflow.parameters.S3_BUCKET}}/{{workflow.parameters.RUN_FOLDER}}/{{inputs.parameters.dataset-name}}"

            echo "[rclone] Uploading $SRC -> $DST"
            rclone copy "$SRC" "$DST" \
              --s3-provider "{{workflow.parameters.S3_PROVIDER}}" \
              --s3-endpoint "{{workflow.parameters.S3_ENDPOINT}}" \
              --s3-access-key-id "$RCLONE_S3_ACCESS_KEY_ID" \
              --s3-secret-access-key "$RCLONE_S3_SECRET_ACCESS_KEY" \
              --transfers 8 --checkers 8 --retries 5 --retries-sleep=15s \
              --s3-upload-cutoff 200Mi --s3-chunk-size 100Mi --s3-upload-concurrency 4 \
              --stats 15s --stats-log-level NOTICE

            echo "[rclone] Upload complete. Starting verification..."

            # Verify the upload by checking file name and size
            rclone check "$SRC" "$DST" \
              --s3-provider "{{workflow.parameters.S3_PROVIDER}}" \
              --s3-endpoint "{{workflow.parameters.S3_ENDPOINT}}" \
              --s3-access-key-id "$RCLONE_S3_ACCESS_KEY_ID" \
              --s3-secret-access-key "$RCLONE_S3_SECRET_ACCESS_KEY" \
              --one-way \
              --differ /tmp/rclone-check.log 

            if [ $? -eq 0 ]; then
              echo "[rclone] Verification successful - all files match!"
              echo "UPLOAD_VERIFIED=true" > /results/upload_status_{{inputs.parameters.dataset-name}}.txt
            else
              echo "[rclone] Verification failed - some files don't match!"
              if [ -f /tmp/rclone-check.log ]; then
                echo "Files that differ:"
                cat /tmp/rclone-check.log
              fi
              echo "UPLOAD_VERIFIED=false" > /results/upload_status_{{inputs.parameters.dataset-name}}.txt
              exit 1
            fi

        # Light footprint so it can co-locate with Metashape if the node has room.
        resources:
          requests:
            cpu: "100m"
            memory: "256Mi"
          limits:
            cpu: "1"
            memory: "2Gi"
        env:
          - name: RCLONE_S3_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: s3-credentials
                key: access_key
          - name: RCLONE_S3_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: s3-credentials
                key: secret_key
      
