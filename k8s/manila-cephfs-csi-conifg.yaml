# This is a Kubernetes config file that specifies resources for mounting a Manila CephFS share via
# the Ceph CSI driver into a K8s cluster. Once applied, there will be a PVC in the 'argo' namespace
# that pods can use to mount the share. For installation instructions, see OFO infra docs.

# Secret (required for the PV CSI driver, it needs to access creds via a K8s Secret)
apiVersion: v1
kind: Secret
metadata:
  name: manila-share-secret
  namespace: ceph-csi-cephfs
stringData:
  userID: "${MANILA_ACCESS_RULE_NAME}"
  userKey: "${MANILA_ACCESS_KEY}" # The name shown in the 'Access to:' field of the share in OpenStack/Exosphere
# Eventually we could use Sealed Secrets so that we can put encrypted secrets here and make it safe to commit to Git, then we just provide the cluster the decryption key.

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ceph-csi-config
  namespace: ceph-csi-cephfs
data:
  config.json: |-
    [{"clusterID":"12345","monitors":[${MANILA_MONITORS_JSON}]}]
  # $MANILA_MONITORS_JSON should be something like: "149.165.158.38:6789","149.165.158.22:6789","149.165.158.54:6789","149.165.158.70:6789","149.165.158.86:6789"


---

### PV
apiVersion: v1
kind: PersistentVolume
metadata:
  name: ceph-share-rw-pv
spec:
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  capacity:
    storage: 300Gi # For static provisioning with pre-existing volumes, the storage field is primarily a label for binding, not an enforced quota.
  csi:
    driver: cephfs.csi.ceph.com
    volumeHandle: ceph-share-rw-pv # doesn't have to match PV name but recommended for tracking
    # Claude said nodePublishSecretRef is unnecessary:
    nodePublishSecretRef:
      name: manila-share-secret
      namespace: ceph-csi-cephfs
    nodeStageSecretRef:
      name: manila-share-secret
      namespace: ceph-csi-cephfs
    volumeAttributes:
      clusterID: "12345" # Arbitrary but has to match the ConfigMap above
      fsName: "cephfs"
      monitors: "${MANILA_MONITORS}" # Should be something like "149.165.158.38:6789,149.165.158.22:6789,149.165.158.54:6789,149.165.158.70:6789,149.165.158.86:6789"
      rootPath: "${MANILA_ROOT_PATH}" # Should be something like "/volumes/_nogroup/<UUID>/<UUID>"
      staticVolume: "true"
  storageClassName: "" # Tells K8s we don't want dynamic provisioning, it already exists. Must match the same attribute/value in the PV and PVC, though in PV "" is the deafult whereas in PVC it is not, so technically this is required for the PVC but not PV
  persistentVolumeReclaimPolicy: Retain # This is the deafult, but it ensures we don't attempt to delete the Manila share when the PVC is deleted.
# Note that if the PVC is deleted, we will also have to delete and re-create the PV because oherwise it will be stuck in a "Released", not "Available", state.

---

# PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ceph-share-rw-pvc
  namespace: argo
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 300Gi # For static provisioning with pre-existing volumes, the storage field is primarily a label for binding, not an enforced quota. This val just has to be less than or equal the one in the PV
  volumeName: ceph-share-rw-pv
  storageClassName: ""
