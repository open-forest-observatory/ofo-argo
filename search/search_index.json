{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OFO Argo Workflows Documentation","text":"<p>Welcome to the Open Forest Observatory (OFO) Argo Workflows documentation. This repository specifies workflows for processing drone data at scale using Argo Workflows on a Kubernetes cluster. It also contains cluster setup resources.</p>"},{"location":"#overview","title":"Overview","text":"<p>The OFO Argo system enables parallel processing of drone missions using the automate-metashape pipeline across multiple virtual machines on Jetstream2 Cloud. This scaling capability allows OFO to process many drone missions simultaneously with a single run command.</p>"},{"location":"#architecture","title":"Architecture","text":"<p>The system uses Argo Workflows running on a Kubernetes cluster, which orchestrates containers, scales processing across multiple VMs, and balances the load between worker nodes. The current setup includes:</p> <ul> <li>Controller node: Manages the Kubernetes cluster and Argo workflows</li> <li>Worker nodes: Handle compute workloads, such as processing drone missions, in parallel</li> <li>Manila shared storage: Provides working data storage to the nodes</li> <li>S3 storage: Stores the inputs/outputs of each step</li> </ul>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>User guides: Guides for accessing and managing the cluster to run workflows</li> <li>Administrator guides: Guides for setting up and configuring the cluster infrastructure</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>OFO Argo GitHub Repository</li> <li>Automate-Metashape Pipeline</li> <li>Argo Workflows Documentation</li> <li>Jetstream2 Cloud</li> </ul>"},{"location":"admin/","title":"Administrator guides","text":"<p>This section contains guides for administrators who need to set up and configure the OFO Argo cluster infrastructure.  For day-to-day cluster usage instructions, including cluster resizing and workflow submission, see the User guides section.</p>"},{"location":"admin/argo-installation-on-cluster/","title":"Argo installation on cluster","text":"<p>This guide covers the installation of Argo Workflows, including the CLI on your local machine (required for all users) and the Kubernetes extension on the cluster (one-time admin installation).</p>"},{"location":"admin/argo-installation-on-cluster/#prerequisites","title":"Prerequisites","text":"<p>This guide assumes you already have:</p> <ul> <li>A Kubernetes cluster created (see Cluster creation and resizing)</li> <li>Manila share PV and PVC configured (see Manila share mounting)</li> <li><code>kubectl</code> configured to connect to the cluster</li> </ul>"},{"location":"admin/argo-installation-on-cluster/#clone-or-update-the-ofo-argo-repository","title":"Clone or update the ofo-argo repository","text":"<p>This repository contains files needed for deploying and testing Argo.</p> <pre><code># Clone the repository (first time)\ncd ~/repos\ngit clone https://github.com/open-forest-observatory/ofo-argo\ncd ofo-argo\n</code></pre> <p>Or, if you already have the repository cloned:</p> <pre><code># Update existing repository\ncd ~/repos/ofo-argo\ngit pull\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#install-the-argo-cli-locally-one-time","title":"Install the Argo CLI locally (one-time)","text":"<p>The Argo CLI is a wrapper around <code>kubectl</code> that simplifies communication with Argo on the cluster.</p> <pre><code>ARGO_WORKFLOWS_VERSION=\"v3.7.2\"\nARGO_OS=\"linux\"\n\n# Download the binary\nwget \"https://github.com/argoproj/argo-workflows/releases/download/${ARGO_WORKFLOWS_VERSION}/argo-${ARGO_OS}-amd64.gz\"\n\n# Unzip\ngunzip \"argo-${ARGO_OS}-amd64.gz\"\n\n# Make binary executable\nchmod +x \"argo-${ARGO_OS}-amd64\"\n\n# Move binary to path\nsudo mv \"./argo-${ARGO_OS}-amd64\" /usr/local/bin/argo\n\n# Test installation\nargo version\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#install-argo-workflow-manager-and-server-on-the-cluster","title":"Install Argo workflow manager and server on the cluster","text":"<p>Create the Argo namespace and install Argo components:</p> <pre><code># Create namespace\nkubectl create namespace argo\n\n# Install Argo workflows\nkubectl apply -n argo -f https://github.com/argoproj/argo-workflows/releases/download/${ARGO_WORKFLOWS_VERSION}/install.yaml\n</code></pre> <p>Optionally, check that the pods are running:</p> <pre><code>kubectl get pods -n argo\nkubectl describe pod -n argo &lt;pod-name&gt;\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#configure-workflow-permissions","title":"Configure workflow permissions","text":"<p>The standard <code>install.yaml</code> configures permissions for the workflow controller, but does not configure permissions for the service accounts that workflow pods run as. Without these permissions, the executor will fall back to the legacy insecure pod patch method. This step grants the <code>default</code> service account the minimal permissions needed to create and update workflowtaskresults.</p> <p>If you use custom service accounts in your workflows, you'll need to create additional RoleBindings for those accounts.</p> <pre><code># Apply role and role binding\nkubectl apply -f setup/argo/role-rolebinding-default-create.yaml\n\n# Confirm the necessary permission was granted (should return: yes)\nkubectl auth can-i create workflowtaskresults.argoproj.io -n argo --as=system:serviceaccount:argo:default\n\n# Optional: Describe the roles\nkubectl describe role argo-role -n argo\nkubectl describe role executor -n argo\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#test-the-installation","title":"Test the installation","text":"<p>Run a test workflow to verify everything is working:</p> <pre><code>argo submit -n argo test-workflows/dag-diamond.yaml --watch\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#set-up-argo-server-with-https-access","title":"Set up Argo server with HTTPS access","text":"<p>The following steps configure secure external access to the Argo UI.</p>"},{"location":"admin/argo-installation-on-cluster/#verify-argo-server-is-clusterip","title":"Verify Argo server is ClusterIP","text":"<p>Ensure <code>argo-server</code> is set to ClusterIP, which means it's only accessible from within the cluster's internal network. We'll configure a secure gateway to the internet next.</p> <pre><code># Check current type\nkubectl get svc argo-server -n argo\n\n# If it's LoadBalancer, change it back to ClusterIP\nkubectl patch svc argo-server -n argo -p '{\"spec\":{\"type\":\"ClusterIP\"}}'\n\n# Verify (should show: TYPE = ClusterIP)\nkubectl get svc argo-server -n argo\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#install-cert-manager","title":"Install cert-manager","text":"<pre><code># Install cert-manager\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.yaml\n\n# Wait for cert-manager to be ready (takes ~1 minute)\nkubectl wait --for=condition=available --timeout=300s deployment/cert-manager -n cert-manager\nkubectl wait --for=condition=available --timeout=300s deployment/cert-manager-webhook -n cert-manager\nkubectl wait --for=condition=available --timeout=300s deployment/cert-manager-cainjector -n cert-manager\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#install-nginx-ingress-controller","title":"Install nginx ingress controller","text":"<pre><code># Install nginx ingress controller\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml\n\n# Wait for LoadBalancer IP to be assigned (may take 1-3 minutes)\nkubectl get svc -n ingress-nginx ingress-nginx-controller -w\n\n# Press Ctrl+C once you see an EXTERNAL-IP appear. Save this IP - you'll need it for DNS.\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#create-dns-a-record","title":"Create DNS A record","text":"<p>Go to your DNS provider (GoDaddy, Cloudflare, Route53, etc.) and create:</p> <ul> <li>Type: A</li> <li>Name: argo (or your preferred subdomain)</li> <li>Value: <code>&lt;IP from previous step&gt;</code></li> <li>TTL: 300 (or auto/default)</li> </ul> <p>A \"non-authoritative answer\" from DNS queries is OK.</p>"},{"location":"admin/argo-installation-on-cluster/#create-lets-encrypt-cluster-issuer","title":"Create Let's Encrypt cluster issuer","text":"<pre><code>kubectl apply -f setup/argo/clusterissuer-letsencrypt.yaml\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#create-ingress-resource-for-argo-server","title":"Create ingress resource for Argo server","text":"<pre><code>kubectl apply -f setup/argo/ingress-argo.yaml\n</code></pre> <p>Wait 5-10 minutes for DNS records to propagate, then verify:</p> <pre><code>nslookup argo.focal-lab.org\n# Should return the ingress controller IP\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#request-and-wait-for-certificate","title":"Request and wait for certificate","text":"<pre><code># Watch certificate being issued (usually 1-3 minutes)\nkubectl get certificate -n argo -w\n\n# Wait until you see READY show True:\n# NAME              READY   SECRET           AGE\n# argo-server-tls   True    argo-server-tls  2m\n\n# Press Ctrl+C when READY shows True\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#verify-argo-ui-access","title":"Verify Argo UI access","text":"<p>The Argo UI should now be accessible at https://argo.focal-lab.org</p> <p>Verification commands:</p> <pre><code># Check all components are ready\nkubectl get pods -n cert-manager\nkubectl get pods -n ingress-nginx\nkubectl get pods -n argo\n\n# Check ingress created\nkubectl get ingress -n argo\n\n# Check certificate issued\nkubectl get certificate -n argo\n\n# Check DNS resolves\nnslookup argo.focal-lab.org\n</code></pre>"},{"location":"admin/argo-installation-on-cluster/#create-argo-ui-server-token","title":"Create Argo UI server token","text":"<p>Create a token that lasts one year. This token will need to be re-created annually.</p> <p>We're creating this for the <code>default</code> service account. In the future, we may want to create a dedicated service account for argo-server tokens, or separate accounts for each user to allow individual permission management.</p> <pre><code># Create token (valid for 1 year)\nkubectl create token argo-server -n argo --duration=8760h\n</code></pre> <p>Copy the token, preface it with <code>Bearer</code>, and add/update it in Vaultwarden.</p>"},{"location":"admin/argo-installation-on-cluster/#token-rotation-future","title":"Token rotation (future)","text":"<p>To rotate the token in the future:</p> <pre><code># Delete all tokens for the service account\nkubectl delete secret -n argo -l kubernetes.io/service-account.name=argo-server\n</code></pre> <p>Then recreate it with the command above.</p>"},{"location":"admin/cluster-creation-and-resizing/","title":"Cluster creation and resizing","text":"<p>This guide is for the cluster administrator (currently Derek). Since we only need one cluster and Derek is taking care of creating it, this guide is not necessary for the whole team. There is a separate guide on cluster management that is for the whole team.</p> <p>Key resource referenced in creating this guide: Beginner's Guide to Magnum on Jetstream2</p>"},{"location":"admin/cluster-creation-and-resizing/#one-time-local-machine-software-setup","title":"One-time local machine software setup","text":"<p>These instructions will set up your local (Linux, Mac, or WSL) machine to control the cluster through the command line.</p>"},{"location":"admin/cluster-creation-and-resizing/#install-python-and-create-virtual-environment","title":"Install Python and create virtual environment","text":"<p>Make sure you have a recent Python interpreter and the venv utility, then create a virtual environment for OpenStack management:</p> <pre><code>sudo apt update\nsudo apt install -y python3-full python3-venv\npython3 -m venv ~/venv/openstack\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#install-openstack-command-line-tools","title":"Install OpenStack command line tools","text":"<pre><code># Activate environment\nsource ~/venv/openstack/bin/activate\n\n# Install OpenStack utilities\npip install -U python-openstackclient python-magnumclient python-designateclient\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#install-kubectl","title":"Install kubectl","text":"<p>Install the Kubernetes control utility <code>kubectl</code> (from the official Kubernetes documentation):</p> <pre><code># Install prerequisites\nsudo apt install -y apt-transport-https ca-certificates curl gnupg\n\n# Add Kubernetes apt repository\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list\n\n# Install kubectl\nsudo apt update\nsudo apt install -y kubectl\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#create-application-credential","title":"Create application credential","text":"<p>In Horizon, go to Identity &gt; Application Credentials. Click Create:</p> <ul> <li>Do not change the roles</li> <li>Do not set a secret (one will be generated)</li> <li>Do set an expiration date</li> <li>Do check \"unrestricted\" (required because Magnum creates additional app credentials for the cluster)</li> </ul> <p>Download the openrc file and store it in the OFO Vaultwarden organization where OFO members can access it.</p> <p>Copy the application credential onto your local computer (do not put it on a JS2 machine), ideally into <code>~/.ofocluster/app-cred-ofocluster-openrc.sh</code> (which is where we will assume it is in these docs).</p> <p>Source the application credential (which sets relevant environment variables for the OpenStack command line tools):</p> <pre><code>source ~/.ofocluster/app-cred-ofocluster-openrc.sh\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#create-openstack-keypair","title":"Create OpenStack keypair","text":"<p>Create a keypair for cluster node access. If you want, you can save the private key that is displayed when you run this command in order to SSH into the cluster nodes later. However, they won't have public IP addresses, so this is mainly to satisfy Magnum's requirements.</p> <pre><code># Create a new keypair (displays private key - save if needed)\nopenstack keypair create my-openstack-keypair-name\n</code></pre> <p>Alternatively, specify an existing public key you normally use, in this example <code>~/.ssh/id_rsa.pub</code>:</p> <pre><code># Use existing public key\nopenstack keypair create my-openstack-keypair-name --public-key ~/.ssh/id_rsa.pub\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#enable-shell-completion","title":"Enable shell completion","text":"<pre><code># Create directory for completion scripts\nmkdir -p ~/.bash_completion.d\n\n# Generate completion scripts\nopenstack complete &gt; ~/.ofocluster/openstack-completion.bash\nkubectl completion bash &gt; ~/.ofocluster/kubectl-completion.bash\n\n# Add to ~/.bashrc\necho 'source ~/.ofocluster/openstack-completion.bash' &gt;&gt; ~/.bashrc\necho 'source ~/.ofocluster/kubectl-completion.bash' &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#cluster-creation","title":"Cluster creation","text":"<p>Assuming you're in a fresh shell session, enter your JS2 venv and source the application credential:</p> <pre><code>source ~/venv/openstack/bin/activate\nsource ~/.ofocluster/app-cred-ofocluster-openrc.sh\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#view-available-cluster-templates","title":"View available cluster templates","text":"<pre><code>openstack coe cluster template list\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#deploy-the-cluster","title":"Deploy the cluster","text":"<p>Specify the deployment parameters and create the cluster. Choose the most recent Kubernetes version (highest number in the template list). The master node can be <code>m3.small</code>. We'll deploy a cluster with a single worker node that is also <code>m3.small</code>. When we need to scale up, we'll add nodegroups. This initial spec is just the base setup for when we're not running Argo workloads on it.</p> <pre><code># Set deployment parameters\nTEMPLATE=\"kubernetes-1-33-jammy\"\nFLAVOR=\"m3.small\"\nMASTER_FLAVOR=\"m3.small\"\nBOOT_VOLUME_SIZE_GB=80\n\n# Number of instances\nN_MASTER=1  # Needs to be odd\nN_WORKER=1\n\n# Min and max number of worker nodes (if using autoscaling)\nAUTOSCALE=false\nN_WORKER_MIN=1\nN_WORKER_MAX=5\n\n# Network configuration\nNETWORK_ID=$(openstack network show --format value -c id auto_allocated_network)\nSUBNET_ID=$(openstack subnet show --format value -c id auto_allocated_subnet_v4)\nKEYPAIR=my-openstack-keypair-name\n\n# Deploy the cluster\nopenstack coe cluster create \\\n    --cluster-template $TEMPLATE \\\n    --master-count $N_MASTER --node-count $N_WORKER \\\n    --master-flavor $MASTER_FLAVOR --flavor $FLAVOR \\\n    --merge-labels \\\n    --labels auto_scaling_enabled=$AUTOSCALE \\\n    --labels min_node_count=$N_WORKER_MIN \\\n    --labels max_node_count=$N_WORKER_MAX \\\n    --labels boot_volume_size=$BOOT_VOLUME_SIZE_GB \\\n    --keypair $KEYPAIR \\\n    --fixed-network \"${NETWORK_ID}\" \\\n    --fixed-subnet \"${SUBNET_ID}\" \\\n    \"ofocluster\"\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#check-cluster-status-optional","title":"Check cluster status (optional)","text":"<pre><code>openstack coe cluster list\nopenstack coe cluster show ofocluster\nopenstack coe nodegroup list ofocluster\n</code></pre> <p>Or with formatting that makes it easier to copy the cluster UUID:</p> <pre><code>openstack coe cluster list --format value -c uuid -c name\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#set-up-kubectl-to-control-kubernetes","title":"Set up <code>kubectl</code> to control Kubernetes","text":"<p>This is required the first time you interact with Kubernetes on the cluster. <code>kubectl</code> is a tool to control Kubernetes (the cluster's software, not its compute nodes/VMs) from your local command line.</p> <p>Once the <code>openstack coe cluster list</code> status (command above) changes to <code>CREATE_COMPLETE</code>, get the Kubernetes configuration file (<code>kubeconfig</code>) and configure your environment:</p> <pre><code># Get cluster configuration\nopenstack coe cluster config \"ofocluster\" --force\n\n# Set permissions and move to appropriate location\nchmod 600 config\nmkdir -p ~/.ofocluster\nmv -i config ~/.ofocluster/ofocluster.kubeconfig\n\n# Set KUBECONFIG environment variable\nexport KUBECONFIG=~/.ofocluster/ofocluster.kubeconfig\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#kubernetes-management","title":"Kubernetes management","text":"<p>If you are resuming cluster management after a reboot, you will need to re-set environment variables and source the application credential:</p> <pre><code>source ~/venv/openstack/bin/activate\nexport KUBECONFIG=~/.ofocluster/ofocluster.kubeconfig\nsource ~/.ofocluster/app-cred-ofocluster-openrc.sh\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#view-cluster-nodes","title":"View cluster nodes","text":"<pre><code>kubectl get nodes\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#access-shell-on-cluster-nodes","title":"Access shell on cluster nodes","text":"<p>Run commands on a node with:</p> <pre><code># Start a debug session on a specific node\nkubectl debug node/&lt;node-name&gt; -it --image=ubuntu\n\n# Once inside, you have host access via /host\n# Check kernel modules\nchroot /host modprobe ceph\nchroot /host lsmod | grep ceph\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#check-disk-usage","title":"Check disk usage","text":"<p>Run a one-off command to check disk usage:</p> <pre><code>kubectl debug node/&lt;node-name&gt; -it --image=busybox -- df -h\n</code></pre> <p>Look for the <code>/dev/vda1</code> volume. Then delete the debugging pods:</p> <pre><code>kubectl get pods -o name | grep node-debugger | xargs kubectl delete\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#cluster-resizing","title":"Cluster resizing","text":"<p>These instructions are for managing which nodes are in the cluster, not what software is running on them.</p>"},{"location":"admin/cluster-creation-and-resizing/#resize-the-default-worker-group","title":"Resize the default worker group","text":"<p>Resize the cluster by adding or removing nodes from the original worker group (not a later-added nodegroup). We will likely not do this, relying instead on nodegroups for specific runs.</p> <pre><code>openstack coe cluster resize \"ofocluster\" 4\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#add-a-new-nodegroup","title":"Add a new nodegroup","text":"<p>To add a new nodegroup, first specify its parameters and then use OpenStack to create it:</p> <pre><code># Set nodegroup parameters\nNODEGROUP_NAME=cpu-group  # or gpu-group\nFLAVOR=m3.quad  # or \"g3.medium\" for GPU\nN_WORKER=1\nAUTOSCALE=false\nN_WORKER_MIN=1\nN_WORKER_MAX=5\nBOOT_VOLUME_SIZE_GB=80\n\n# Create the nodegroup\nopenstack coe nodegroup create ofocluster $NODEGROUP_NAME \\\n    --flavor $FLAVOR \\\n    --node-count $N_WORKER \\\n    --labels auto_scaling_enabled=$AUTOSCALE \\\n    --labels min_node_count=$N_WORKER_MIN \\\n    --labels max_node_count=$N_WORKER_MAX \\\n    --labels boot_volume_size=$BOOT_VOLUME_SIZE_GB\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#drain-nodes-before-downsizing-or-deleting","title":"Drain nodes before downsizing or deleting","text":"<p>When decreasing the number of nodes in a nodegroup, it's best practice to drain the Kubernetes pods from them first. Since we don't know which nodes OpenStack will delete when reducing the size, we have to drain the whole nodegroup. This is also what you'd do when deleting a nodegroup entirely.</p> <pre><code>NODEGROUP_NAME=cpu-group\nkubectl get nodes -l capi.stackhpc.com/node-group=$NODEGROUP_NAME -o name | xargs -I {} kubectl drain {} --ignore-daemonsets --delete-emptydir-data\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#resize-a-nodegroup","title":"Resize a nodegroup","text":"<p>Change the number of nodes in an existing nodegroup:</p> <pre><code>N_WORKER=2\nNODEGROUP_NAME=cpu-group\nopenstack coe cluster resize ofocluster --nodegroup $NODEGROUP_NAME $N_WORKER\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#delete-a-nodegroup","title":"Delete a nodegroup","text":"<pre><code>openstack coe nodegroup delete ofocluster $NODEGROUP_NAME\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#delete-the-cluster","title":"Delete the cluster","text":"<pre><code>openstack coe cluster delete \"ofocluster\"\n</code></pre>"},{"location":"admin/cluster-creation-and-resizing/#monitoring-dashboards","title":"Monitoring dashboards","text":"<p>Incomplete notes in development.</p>"},{"location":"admin/cluster-creation-and-resizing/#grafana-dashboard","title":"Grafana dashboard","text":"<pre><code># Port-forward Grafana to your local machine\nkubectl port-forward -n monitoring-system svc/kube-prometheus-stack-grafana 3000:80\n</code></pre> <p>Then open http://localhost:3000 in your browser.</p>"},{"location":"admin/cluster-creation-and-resizing/#kubernetes-dashboard","title":"Kubernetes dashboard","text":"<pre><code># Create service account\nkubectl create serviceaccount dashboard-admin -n kubernetes-dashboard\n\n# Create cluster role binding\nkubectl create clusterrolebinding dashboard-admin \\\n    --clusterrole=cluster-admin \\\n    --serviceaccount=kubernetes-dashboard:dashboard-admin\n\n# Create token (24 hour duration)\nkubectl create token dashboard-admin -n kubernetes-dashboard --duration=24h\n\n# Port-forward (if not already running)\nkubectl port-forward -n kubernetes-dashboard svc/kubernetes-dashboard 8443:443\n</code></pre> <p>Then open https://localhost:8443 in your browser and use the token to log in.</p>"},{"location":"admin/manila-share-mounting/","title":"Manila share mounting","text":"<p>This guide covers mounting a Manila CephFS share to your Kubernetes cluster for persistent storage.</p>"},{"location":"admin/manila-share-mounting/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Kubernetes cluster created with Magnum (see Cluster creation and resizing)</li> <li><code>kubectl</code> configured to connect to the cluster</li> <li>OpenStack application credentials</li> <li>The Manila share already created in OpenStack</li> </ul>"},{"location":"admin/manila-share-mounting/#one-time-local-machine-software-setup","title":"One-time local machine software setup","text":"<p>Ensure your local system has the necessary tools. First, activate the OpenStack virtual environment created in the cluster creation guide.</p> <p>Additionally, we need OpenStack app credentials to look up the parameters of our Manila share based on its name, which in turn requires an additional OpenStack command-line tool for interacting with Manila. An alternative is to look these up from some other existing source (e.g. Horizon UI) and provide them manually, in which case the OpenStack system tools are not necessary.</p> <pre><code>source ~/venv/openstack/bin/activate\nsource ~/.ofocluster/app-cred-ofocluster-openrc.sh\n\n# Install Manila client and jq\npip install -U python-manilaclient\nsudo apt install -y jq\n\n# Install Helm\nsudo snap install helm --classic\n</code></pre>"},{"location":"admin/manila-share-mounting/#install-the-ceph-csi-driver-on-the-cluster","title":"Install the Ceph CSI driver on the cluster","text":"<p>If you are resuming cluster management after a reboot, you will need to re-set environment variables and source the application credential:</p> <pre><code>source ~/venv/openstack/bin/activate\nexport KUBECONFIG=~/.ofocluster/ofocluster.kubeconfig\nsource ~/.ofocluster/app-cred-ofocluster-openrc.sh\n</code></pre> <p>The Ceph CSI (Container Storage Interface) driver enables Kubernetes to mount CephFS shares.</p> <pre><code># Add Helm repository\nhelm repo add ceph-csi https://ceph.github.io/csi-charts\nhelm repo update\n\n# Create namespace for Ceph CSI\nkubectl create namespace ceph-csi-cephfs\n\n# Install Ceph CSI driver\nhelm install --namespace \"ceph-csi-cephfs\" \"ceph-csi-cephfs\" ceph-csi/ceph-csi-cephfs\n\n# Check installation status\nhelm status --namespace \"ceph-csi-cephfs\" \"ceph-csi-cephfs\"\n</code></pre>"},{"location":"admin/manila-share-mounting/#look-up-manila-share-parameters","title":"Look up Manila share parameters","text":"<p>Based on the share name and access rule name, query OpenStack to look up the necessary identifiers. The Kubernetes Manila config requires these values.</p> <pre><code># Set your Manila share and access rule names\nMANILA_SHARE_NAME=dytest3\nexport MANILA_ACCESS_RULE_NAME=dytest3-rw\n\n# Extract Manila monitors (json-formatted list)\nexport MANILA_MONITORS_JSON=$(openstack share export location list \"$MANILA_SHARE_NAME\" -f json | jq -r '.[0].Path | split(\":/\")[0] | split(\",\") | map(\"\\\"\" + . + \"\\\"\") | join(\",\")')\n\n# Extract the root path to the Manila share\nexport MANILA_ROOT_PATH=$(openstack share export location list $MANILA_SHARE_NAME -f json | jq -r '.[0].Path' | awk -F':/' '{print \"/\"$2}')\n\n# Get the access rule ID\nACCESS_RULE_ID=$(openstack share access list \"$MANILA_SHARE_NAME\" -f json | jq -r \".[] | select(.\\\"Access To\\\" == \\\"$MANILA_ACCESS_RULE_NAME\\\") | .ID\")\n\n# Extract the secret key for the access rule\nexport MANILA_ACCESS_KEY=$(openstack share access list \"$MANILA_SHARE_NAME\" -f json | jq -r \".[] | select(.\\\"Access To\\\" == \\\"$MANILA_ACCESS_RULE_NAME\\\") | .\\\"Access Key\\\"\")\n\n# Confirm we extracted the expected attributes\necho $MANILA_MONITORS_JSON\necho $MANILA_ROOT_PATH\necho $MANILA_ACCESS_RULE_NAME\necho $MANILA_ACCESS_KEY\n</code></pre> <p>As an alternative, you can look these parameters up in Horizon.</p>"},{"location":"admin/manila-share-mounting/#clone-or-update-the-ofo-argo-repository","title":"Clone or update the ofo-argo repository","text":"<p>The repository contains Kubernetes configuration templates for Manila share mounting.</p> <pre><code># Clone the repository (first time)\ncd ~/repos\ngit clone https://github.com/open-forest-observatory/ofo-argo\ncd ofo-argo\n</code></pre> <p>Or, if you already have the repository cloned:</p> <pre><code># Update existing repository\ncd ~/repos/ofo-argo\ngit pull\n</code></pre>"},{"location":"admin/manila-share-mounting/#apply-the-share-configuration-to-the-cluster","title":"Apply the share configuration to the cluster","text":"<p>There is a template Kubernetes config file that contains variables such as <code>${MANILA_ACCESS_RULE_NAME}</code>. These variables will be substituted with the environment variables we prepared in the previous step. The following command substitutes the environment variables into the config file and applies it to the cluster. It's done in one step so we don't save this file (which contains secrets) to disk.</p> <p>Note that the namespaces of the various resources are defined within the yaml, so <code>-n</code> does not have to be used here. If namespaces ever need to change, update the config yaml.</p> <pre><code># Create the namespace for the PVC and Argo application\nkubectl create namespace argo\n\n# Substitute variables and apply configuration\nenvsubst &lt; setup/k8s/manila-cephfs-csi-config.yaml | kubectl apply -f -\n\n# Verify resources were created\nkubectl describe secret -n ceph-csi-cephfs manila-share-secret\nkubectl describe persistentvolume ceph-share-rw-pv\nkubectl describe -n argo persistentvolumeclaim ceph-share-rw-pvc\n</code></pre>"},{"location":"admin/manila-share-mounting/#test-the-pvc-mount","title":"Test the PVC mount","text":"<p>Deploy a test pod to verify that the PVC mount works correctly.</p> <p>Note: During development, we sometimes encountered the error <code>MountVolume.MountDevice failed for volume \"ceph-share-rw-pv\" : rpc error: code = Internal desc = rpc error: code = Internal desc = failed to fetch monitor list using clusterID (12345): missing configuration for cluster ID \"12345\"</code>. If this occurs, try deleting all deployed resources except the configmap and re-applying them. It may also be resolved by simply deleting and re-applying the test pod.</p> <pre><code># Deploy test pod\nkubectl apply -f setup/k8s/manila-test-pod.yaml\n\n# Check pod status\nkubectl get pod -n argo manila-test-pod\nkubectl describe pod -n argo manila-test-pod\n\n# Once running, exec into the pod\nkubectl exec -n argo -it manila-test-pod -- /bin/sh\n\n# Inside the pod, check the mount\nls -la /mnt/cephfs\ndf -h /mnt/cephfs\n\n# Test write access\necho \"test\" &gt; /mnt/cephfs/test-file.txt\ncat /mnt/cephfs/test-file.txt\n\n# Exit the pod\nexit\n</code></pre>"},{"location":"admin/manila-share-mounting/#clean-up-test-resources","title":"Clean up test resources","text":"<p>After verifying the mount works, delete the test pod:</p> <pre><code>kubectl delete -n argo pod manila-test-pod\n</code></pre>"},{"location":"admin/manila-share-mounting/#delete-all-resources-if-needed","title":"Delete all resources (if needed)","text":"<p>If you need to completely remove the Manila share mounting configuration:</p> <pre><code>kubectl delete -n argo pod manila-test-pod\nkubectl delete -n argo pvc ceph-share-rw-pvc\nkubectl delete pv ceph-share-rw-pv\nkubectl delete -n ceph-csi-cephfs secret manila-share-secret\nkubectl delete configmap -n ceph-csi-cephfs ceph-csi-config\n</code></pre>"},{"location":"usage/","title":"User guides","text":"<p>This section contains guides for users who need to access and manage the OFO Argo cluster for running workflows. For initial cluster setup, configuration, and maintenance, see the Admin guides section.</p>"},{"location":"usage/argo-usage/","title":"Using Argo on the OFO cluster","text":"<p>This guide describes how to submit workflows to Argo and monitor them.</p>"},{"location":"usage/argo-usage/#prerequisites","title":"Prerequisites","text":"<p>This guide assumes you already:</p> <ul> <li>Have access to the <code>ofocluster</code> kubernetes cluster</li> <li>Have resized the cluster to the apprpriate size for your workflow</li> <li>Have <code>kubectl</code> configured to connect to the cluster</li> </ul> <p>For guidance on all of this setup, see Cluster access and resizing.</p>"},{"location":"usage/argo-usage/#install-the-argo-cli-locally-one-time","title":"Install the Argo CLI locally (one-time)","text":"<p>The Argo CLI is a wrapper around <code>kubectl</code> that simplifies communication with Argo on the cluster. You should install it on your local machine since that is where you will have set up and authenticated <code>kubectl</code> following the Cluster access guide.</p> <pre><code># Specify the CLI version to install\nARGO_WORKFLOWS_VERSION=\"v3.7.2\"\nARGO_OS=\"linux\"\n\n# Download the binary\nwget \"https://github.com/argoproj/argo-workflows/releases/download/${ARGO_WORKFLOWS_VERSION}/argo-${ARGO_OS}-amd64.gz\"\n\n# Unzip\ngunzip \"argo-${ARGO_OS}-amd64.gz\"\n\n# Make binary executable\nchmod +x \"argo-${ARGO_OS}-amd64\"\n\n# Move binary to path\nsudo mv \"./argo-${ARGO_OS}-amd64\" /usr/local/bin/argo\n\n# Test installation\nargo version\n</code></pre>"},{"location":"usage/argo-usage/#authenticate-with-the-cluster","title":"Authenticate with the cluster","text":"<p>Once after every reboot, you will need to re-set your <code>KUBECONFIG</code> environment variable so that <code>argo</code> and <code>kubectl</code> CLI tools can authenticate with the cluster. Simply run <code>export KUBECONFIG=~/.ofocluster/ofocluster.kubeconfig</code>.</p>"},{"location":"usage/argo-usage/#submit-a-workflow","title":"Submit a workflow","text":"<pre><code>source ~/venv/openstack/bin/activate\nexport KUBECONFIG=~/.ofocluster/ofocluster.kubeconfig\nsource ~/.ofocluster/app-cred-ofocluster-openrc.sh\n</code></pre> <p>Simply run <code>argo submit -n argo /path/to/your/workflow.yaml --watch</code>, optionally adding parameters  by appending text in the format <code>-p PARAMETER_NAME=parameter_value</code>. The <code>parameter_value</code> can be  an environment variable. For example:</p> <pre><code>argo submit -n argo workflow.yaml --watch \\\n-p CONFIG_LIST=config_list.txt \\\n-p AGISOFT_FLS=$AGISOFT_FLS \\\n-p RUN_FOLDER=gillan_june27 \\\n-p DB_PASSWORD=&lt;password&gt; \\\n-p DB_HOST=&lt;vm_ip_address&gt; \\\n-p DB_NAME=&lt;db_name&gt; \\\n-p DB_USER=&lt;user_name&gt; \\\n-p S3_BUCKET=ofo-internal \\\n-p S3_PROVIDER=Other \\\n-p S3_ENDPOINT=https://js2.jetstream-cloud.org:8001\n</code></pre>"},{"location":"usage/argo-usage/#observe-and-manage-workflows-through-the-argo-web-ui","title":"Observe and manage workflows through the Argo web UI","text":"<p>Access the Argo UI at argo.focal-lab.org. When prompted to log in, supply the client authentication token. You can find the token string in Vaultwarden under the record \"Argo UI token\".</p>"},{"location":"usage/cluster-access-and-resizing/","title":"Cluster access and resizing","text":"<p>This guide assumes a cluster has already been created, any necessary persistent volumes (e.g. Manila share) have already been configured, and Argo has already been installed. It describes how to access and resize the cluster as a user. For details on initial cluster deployment and setup, including installation of Argo and configuration of persistent volumes, see the Admin guides.</p> <p>The OFO cluster is named <code>ofocluster</code>. The nodes that comprise it can be seen in Exosphere starting with the string <code>ofocluster-</code>. They appear as created by <code>dyoung@access-ci.org</code>. The nodes should not be modified via Exosphere or Horizon, only through the command line tools described below.</p>"},{"location":"usage/cluster-access-and-resizing/#ofo-cluster-management-principles","title":"OFO cluster management principles","text":"<p>When the cluster is not in use, we will downsize it to its minimum size: one m3.small control node and one m3.small worker node. Just before running a compute load (e.g. Argo data processing run) on the cluster, we will manually add one or more 'nodegroups', which contain a specified number of nodes of a specified flavor. We can add CPU and/or GPU nodegroups. Soon after the workflow run is complete, we will manually delete the nodegroup(s) so that the nodes do not consume our compute credits.</p>"},{"location":"usage/cluster-access-and-resizing/#one-time-local-machine-software-setup","title":"One-time local machine software setup","text":"<p>These instructions will set up your local (Linux, Mac, or WSL) machine to control the cluster through the command line.</p>"},{"location":"usage/cluster-access-and-resizing/#install-python-and-create-virtual-environment","title":"Install Python and create virtual environment","text":"<p>Make sure you have a recent Python interpreter and the venv utility, then create a Python virtual environment for OpenStack management. OpenStack is the platform Jetstream2 uses to allow users to create and manage cloud resources.</p> <pre><code>sudo apt update\nsudo apt install -y python3-full python3-venv\npython3 -m venv ~/venv/openstack\nsource ~/venv/openstack/bin/activate\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#install-openstack-command-line-tools","title":"Install OpenStack command line tools","text":"<pre><code>pip install -U python-openstackclient python-magnumclient python-designateclient\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#install-kubectl","title":"Install kubectl","text":"<p>Install the Kubernetes control utility <code>kubectl</code> (from the official Kubernetes documentation):</p> <pre><code># Install prerequisites\nsudo apt install -y apt-transport-https ca-certificates curl gnupg\n\n# Add Kubernetes apt repository\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\nsudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list\nsudo chmod 644 /etc/apt/sources.list.d/kubernetes.list\n\n# Install kubectl\nsudo apt update\nsudo apt install -y kubectl\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#download-openstack-application-credential","title":"Download OpenStack application credential","text":"<p>This is a credential (and associated secret key) that allows you to authenticate with OpenStack in order to manage cloud resources in our project. It will be rotated occasionally, so if yours doesn't appear to work, check whether you have the latest version. In the OFO Vaultwarden, find the entry <code>OpenStack application credential</code>. Download the attached file <code>app-cred-ofocluster-openrc.sh</code> onto your local computer (do not put it on a JS2 machine), ideally into <code>~/.ofocluster/app-cred-ofocluster-openrc.sh</code> (which is where we will assume it is in these docs).</p> <p>Source the application credential (which sets relevant environment variables to authenticate the OpenStack command line tools):</p> <pre><code>source ~/.ofocluster/app-cred-ofocluster-openrc.sh\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#enable-shell-completion","title":"Enable shell completion","text":"<p>This will allow you to use <code>tab</code> to autocomplete OpenStack and Kubectl commands in your shell.</p> <pre><code># Create directory for completion scripts\nmkdir -p ~/.bash_completion.d\n\n# Generate completion scripts\nopenstack complete &gt; ~/.ofocluster/openstack-completion.bash\nkubectl completion bash &gt; ~/.ofocluster/kubectl-completion.bash\n\n# Add to ~/.bashrc\necho 'source ~/.ofocluster/openstack-completion.bash' &gt;&gt; ~/.bashrc\necho 'source ~/.ofocluster/kubectl-completion.bash' &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#set-up-kubectl-to-control-kubernetes","title":"Set up <code>kubectl</code> to control Kubernetes","text":"<p>This is required the first time you interact with Kubernetes on the cluster. <code>kubectl</code> is a tool to control Kubernetes (the cluster's software, not its compute nodes/VMs) from your local command line.</p> <p>Get the Kubernetes configuration file (<code>kubeconfig</code>) and configure your environment:</p> <pre><code># Get cluster configuration\nopenstack coe cluster config \"ofocluster\" --force\n\n# Set permissions and move to appropriate location\nchmod 600 config\nmkdir -p ~/.ofocluster\nmv -i config ~/.ofocluster/ofocluster.kubeconfig\n\n# Set KUBECONFIG environment variable\nexport KUBECONFIG=~/.ofocluster/ofocluster.kubeconfig\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#cluster-resizing","title":"Cluster resizing","text":"<p>These instructions are for managing which nodes are in the cluster, not what software is running on them.</p> <p>If you are resuming cluster management after a reboot, you will need to re-set environment variables and source the application credential:</p> <pre><code>source ~/venv/openstack/bin/activate\nexport KUBECONFIG=~/.ofocluster/ofocluster.kubeconfig\nsource ~/.ofocluster/app-cred-ofocluster-openrc.sh\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#add-a-new-nodegroup","title":"Add a new nodegroup","text":"<p>To add a new nodegroup, first specify its parameters and then use OpenStack to create it:</p> <pre><code># Set nodegroup parameters\nNODEGROUP_NAME=cpu-group  # or gpu-group, or whatever is meaningful to you\nFLAVOR=m3.quad  # or \"g3.medium\" etc for GPU\nN_WORKER=1\nAUTOSCALE=false\nN_WORKER_MIN=1 # Only relevant for autoscale\nN_WORKER_MAX=5 # Only relevant for autoscale\nBOOT_VOLUME_SIZE_GB=80\n\n# Create the nodegroup\nopenstack coe nodegroup create ofocluster $NODEGROUP_NAME \\\n    --flavor $FLAVOR \\\n    --node-count $N_WORKER \\\n    --labels auto_scaling_enabled=$AUTOSCALE \\\n    --labels min_node_count=$N_WORKER_MIN \\\n    --labels max_node_count=$N_WORKER_MAX \\\n    --labels boot_volume_size=$BOOT_VOLUME_SIZE_GB\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#drain-nodes-before-downsizing-or-deleting","title":"Drain nodes before downsizing or deleting","text":"<p>When decreasing the number of nodes in a nodegroup, it is best practice to drain the Kubernetes pods from them first. Given that we don't know which nodes OpenStack will delete when reducing the size, we have to drain the whole nodegroup. This is also what you'd do when deleting a nodegroup entirely.</p> <pre><code>NODEGROUP_NAME=cpu-group\nkubectl get nodes -l capi.stackhpc.com/node-group=$NODEGROUP_NAME -o name | xargs -I {} kubectl drain {} --ignore-daemonsets --delete-emptydir-data\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#resize-a-nodegroup","title":"Resize a nodegroup","text":"<p>Change the number of nodes in an existing nodegroup:</p> <pre><code>NODEGROUP_NAME=cpu-group\nN_WORKER=2\nopenstack coe cluster resize ofocluster --nodegroup $NODEGROUP_NAME $N_WORKER\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#delete-a-nodegroup","title":"Delete a nodegroup","text":"<pre><code>openstack coe nodegroup delete ofocluster $NODEGROUP_NAME\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#cluster-inspection-commands","title":"Cluster inspection commands","text":"<p>If you are resuming cluster management after a reboot, you will need to re-set environment variables and source the application credential:</p> <pre><code>source ~/venv/openstack/bin/activate\nexport KUBECONFIG=~/.ofocluster/ofocluster.kubeconfig\nsource ~/.ofocluster/app-cred-ofocluster-openrc.sh\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#check-cluster-vm-status","title":"Check cluster VM status","text":"<p>This looks at the status of the nodes (e.g. size and quantity) from the perspective of OpenStack. It does not examine the state of the software on the nodes (e.g. Kubernetes, Argo).</p> <pre><code>openstack coe cluster list\nopenstack coe cluster show ofocluster\nopenstack coe nodegroup list ofocluster\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#view-kubernetes-nodes","title":"View Kubernetes nodes","text":"<p>Display Kubernetes node names, including which 'nodegroup' each belongs to.</p> <pre><code>kubectl get nodes\n</code></pre> <p>Display the utilization of CPU and memory on the nodes.</p> <pre><code>kubectl top nodes\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#access-the-shell-on-cluster-nodes","title":"Access the shell on cluster nodes","text":"<p>This can be useful for debugging. Run commands on a node with:</p> <pre><code>kubectl debug node/&lt;node-name&gt; -it --image=ubuntu\n</code></pre> <p>Once inside, you have access to the host VM's filesystem via /host. You could use this, for example, to check kernel modules:</p> <pre><code>chroot /host modprobe ceph\nchroot /host lsmod | grep ceph\n</code></pre> <p>Or to check disk usage:</p> <pre><code>kubectl debug node/&lt;node-name&gt; -it --image=busybox -- df -h\n</code></pre> <p>... then look for the <code>/dev/vda1</code> volume.</p> <p>When done, delete the debugging pods:</p> <pre><code>kubectl get pods -o name | grep node-debugger | xargs kubectl delete\n</code></pre>"},{"location":"usage/cluster-access-and-resizing/#kubernetes-monitoring-dashboards","title":"Kubernetes monitoring dashboards","text":"<p>Incomplete notes in development.</p>"},{"location":"usage/cluster-access-and-resizing/#grafana-dashboard","title":"Grafana dashboard","text":"<pre><code># Port-forward Grafana to your local machine\nkubectl port-forward -n monitoring-system svc/kube-prometheus-stack-grafana 3000:80\n</code></pre> <p>Then open http://localhost:3000 in your browser.</p>"},{"location":"usage/cluster-access-and-resizing/#kubernetes-dashboard","title":"Kubernetes dashboard","text":"<pre><code># Create service account\nkubectl create serviceaccount dashboard-admin -n kubernetes-dashboard\n\n# Create cluster role binding\nkubectl create clusterrolebinding dashboard-admin \\\n    --clusterrole=cluster-admin \\\n    --serviceaccount=kubernetes-dashboard:dashboard-admin\n\n# Create token (24 hour duration)\nkubectl create token dashboard-admin -n kubernetes-dashboard --duration=24h\n\n# Port-forward (if not already running)\nkubectl port-forward -n kubernetes-dashboard svc/kubernetes-dashboard 8443:443\n</code></pre> <p>Then open https://localhost:8443 in your browser and use the token to log in.</p>"}]}